{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, TFT5Model, TFT5ForConditionalGeneration, pipeline\n",
    "import tensorflow as tf\n",
    "from tensorflow import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7016a63c136948ecb5bc42bfc64a82b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1199.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7b1dfad5e94a8fa3956d19102795cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=892146080.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "model = TFT5Model.from_pretrained('t5-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.t5.tokenization_t5.T5Tokenizer"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 4, 32128), dtype=float32, numpy=\n",
       "array([[[-66.46409 , -19.94818 , -27.964075, ..., -82.65033 ,\n",
       "         -82.96196 , -82.93362 ],\n",
       "        [-70.99568 , -20.874355, -28.396765, ..., -81.29921 ,\n",
       "         -81.64886 , -81.510704],\n",
       "        [-58.566452, -18.780762, -25.751802, ..., -77.19243 ,\n",
       "         -77.45787 , -77.257515],\n",
       "        [-49.04378 , -10.060839, -18.349121, ..., -61.042744,\n",
       "         -61.258778, -61.13417 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Studies have been shown that owning a dog is good for you\",return_tensors=\"tf\").input_ids  # Batch size 1\n",
    "decoder_input_ids = tokenizer(\"Studies show that\", return_tensors=\"tf\").input_ids  # Batch size 1\n",
    "outputs = model(input_ids, decoder_input_ids=decoder_input_ids)\n",
    "outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.modeling_tf_outputs.TFSeq2SeqModelOutput'>\n",
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(outputs))\n",
    "print(type(outputs[0]))\n",
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
      "C:\\Users\\13109\\Anaconda3\\lib\\site-packages\\transformers\\models\\t5\\tokenization_t5.py:186: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
     ]
    }
   ],
   "source": [
    "model = TFT5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "inputs = tokenizer('The <extra_id_0> walks in <extra_id_1> park', return_tensors='tf').input_ids\n",
    "labels = tokenizer('<extra_id_0> cute dog <extra_id_1> the <extra_id_2> </s>', return_tensors='tf').input_ids\n",
    "outputs = model(inputs, labels=labels)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "\n",
    "inputs = tokenizer(\"summarize: studies have shown that owning a dog is good for you \",\n",
    "                   return_tensors=\"tf\").input_ids  # Batch size 1\n",
    "\n",
    "result = model.generate(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 17])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ORME, Tennessee (CNN) -- The drought in the Southeastern United States means more than just brown lawns to the folks in Orme, Tennessee. Water flows from their taps for just three hours each evening.\\n\\nA 1961 firetruck loads up with water from a hydrant in Alabama to haul back to Orme, Tennessee.\\n\\nThe mountain spring that supplies water to the town usually dries up at the end of summer, but just for a few days. This year it dried up early, on August 1, and hasn\\'t revived, leaving the town\\'s 145 residents high and dry and relying on water trucked in from the next state.\\n\\nEvery day at 6 p.m., Orme Mayor Tony Reames turns a big valve to release water from the town\\'s tank. When he turns the crank again at 9 p.m., taps in the town run dry.\\n\\n\"When they cut it back on we jump for joy,\" Orme resident Debbie Cash said. \"And then you only have it for three hours.\"\\n\\nThree hours to do all the laundry, bathing, dishwashing and animal watering that has to be done.  Watch how Cash copes Â»\\n\\nThe old mining town could be the canary in the coal mine for the rest of the region. Just 150 miles to the southeast, the 4.5 million people who live in and around Atlanta, Georgia, are nervously watching water levels go down at their major reservoir. The drought has highlighted an ongoing struggle between Georgia, Alabama and Florida over rights to water from the Chattahoochee River.\\n\\n\"All of these people that are on the river systems better take note, because once your streams and tributaries to the river start drying up, the river isn\\'t far behind,\" Reames said.  See photos of a Atlanta\\'s shrinking Lake Lanier reservoir Â»\\n\\nVolunteers take turns three days a week driving a tanker truck or Orme\\'s diesel-belching 1961 fire truck to a hydrant near Bridgeport, Alabama, 2Â½ miles down the road. Making several round trips, they haul about 25,000 gallons of water back to Orme each day.\\n\\nBridgeport, which gets its water from the Tennessee River, doesn\\'t charge its neighbor. Stevenson and New Hope, Alabama, also help out, occasionally bringing trucks full of water to the hydrant, where it\\'s transferred to the Orme trucks.  See where Orme has to go for water Â»\\n\\nBut things are looking up in Orme. A pipeline from Bridgeport is nearly complete, built with the help of a $378,000 grant from the federal government.\\n\\n\"With this new water coming in, then we\\'ll have it made,\" Cash said. \"Now we won\\'t have to worry about it no more.\"\\n\\nIn addition, an Austin, Texas, company called H2O Guard is planning to donate water-saving sink aerators, shower heads and toilet valves to everyone in Orme  on November 17, company spokesman Robert Easter said.\\n\\n\"We think we\\'re going to get another 90 gallons\\' savings per day per household,\" Easter said. \"That\\'ll make that little water tower in that town go from three hours to four hours without any change in anyone\\'s habits.\"\\n\\nReames said residents have found creative ways to conserve, flushing toilets with condensation water from air conditioners and undrinkable water from swimming pools that were filled early in the summer.\\n\\nIt\\'s a lesson for everyone.\\n\\n\"Cherish the water you got and be kind of careful with it,\" Cash said, \"because you never know if you will be out of water.\" E-mail to a friend\\n\\nCNN\\'s Jim Kavanagh contributed to this report.\\n\\n@highlight\\n\\nOrme, Tennessee, has running water from 6 p.m. to 9 p.m.\\n\\n@highlight\\n\\nTown\\'s spring ran dry in midst of Southeast drought\\n\\n@highlight\\n\\nDonated water is trucked in from Alabama\\n\\n@highlight\\n\\nCompletion of pipeline will solve problem for good'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open('example.story','r')\n",
    "story = f.read()\n",
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_story = story.split('\\n\\n@highlight\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 20), dtype=int32, numpy=\n",
       "array([[    0, 32099,     6, 32098,     6, 32097,     6, 32096,     6,\n",
       "        32095,     6, 32094,     6, 32093,     6, 32092,     6,   955,\n",
       "          526,     6],\n",
       "       [    0,   955,   526,     6, 12976,     6,     3,   547,  3999,\n",
       "         7832,   193,   507,     3,    18,  1401,     3,   102,     5,\n",
       "           51,     5],\n",
       "       [    0, 32099,    31,     7,  2141,  4037,  2192,    16,     8,\n",
       "         2214,    13, 18862, 19611,    16,     8,  2214,    13, 18862,\n",
       "        19611,     5],\n",
       "       [    0,  3999,   278,   920,    19,  4072,    15,    26,    16,\n",
       "           45, 13050,     1,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0],\n",
       "       [    0,     3,     2,     3,     2,     3,     2,     3,     2,\n",
       "            3,     2,     3,     2,     3,     2,     3,     2,     3,\n",
       "            2,     3]])>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(split_story, return_tensors=\"tf\", padding = True).input_ids\n",
    "\n",
    "result = model.generate(inputs)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5Model.\n",
      "\n",
      "All the layers of TFT5Model were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5Model for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d55b7fa23c0b4a3faeb6ac889867d908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.pipelines.text2text_generation.SummarizationPipeline"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(summarizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (814 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': \"a 1961 firetruck loads up with water from a hydrant to haul back to Orme, Tennessee . the town's residents are nervously watching water levels go down at their major reservoir . a pipeline from Bridgeport is nearly complete, built with the help of a $378,000 grant from the federal government .\"}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer(split_story[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
