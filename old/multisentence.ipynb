{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install rouge\n",
    "# !pip install sentencepiece\n",
    "# !pip install nomkl\n",
    "# !pip install datasets\n",
    "# !pip install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/futureperfect6/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, T5ForConditionalGeneration, TFT5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasets import Dataset\n",
    "from rouge import Rouge\n",
    "import nltk.data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some functions for convenience later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_story(filename):\n",
    "    \"\"\"Given the CNN data file, reformats to separate the story from the highlights.\n",
    "    Highlights are returned as a single string\"\"\"\n",
    "    \n",
    "    file = open(filename,'r')\n",
    "    text = file.read()\n",
    "    \n",
    "    # split the story and highlights\n",
    "    split_text = text.split('\\n\\n@highlight\\n\\n')\n",
    "    story = split_text[0]\n",
    "    highlights = split_text[1:]\n",
    "    \n",
    "    # return both, rejoining highlights as a single string\n",
    "    return story, highlights#'. '.join(highlights)+'.'\n",
    "\n",
    "def cos_sims(out_sent, ref_sents):\n",
    "    \"gets cosine similarities for an output sentence with respect to the highlight sentences. Returns the sum of values.\"\n",
    "    \n",
    "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
    "    \n",
    "    # get sentence level vectors with tf-idf\n",
    "    tfidf = vect.fit_transform([out_sent] + ref_sents)\n",
    "    \n",
    "    # get similarity matrix\n",
    "    similarity_mat = tfidf * tfidf.T\n",
    "    \n",
    "    # only values comparing \"out_sent\" with each sent in \"ref_sents\"\n",
    "    return similarity_mat.toarray()[:1,1:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a T5 Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t5 model\n",
    "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "\n",
    "# t5 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('t5-base')\n",
    "\n",
    "def tokenize(batch):\n",
    "    \"\"\"Applies tokenizer to a whole dataset at once. Input is a dataset with raw text data, \n",
    "    and output is a dataset with tokenized data\"\"\"\n",
    "    \n",
    "    tokenized_input = tokenizer(batch['source'], padding='max_length', truncation=True)\n",
    "    tokenized_label = tokenizer(batch['target'], padding='max_length', truncation=True)\n",
    "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92579 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>story</th>\n",
       "      <th>train</th>\n",
       "      <th>validation</th>\n",
       "      <th>test</th>\n",
       "      <th>duplicate</th>\n",
       "      <th>source</th>\n",
       "      <th>highlights</th>\n",
       "      <th>broken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01.story</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef.story</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>00027e965c8264c35cc1bc55556db388da82b07f.story</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0002c17436637c4fe1837c935c04de47adb18e9a.story</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b.story</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           story  train  \\\n",
       "0           0  0001d1afc246a7964130f43ae940af6bc6c57f01.story      1   \n",
       "1           1  0002095e55fcbd3a2f366d9bf92a95433dc305ef.story      1   \n",
       "2           2  00027e965c8264c35cc1bc55556db388da82b07f.story      1   \n",
       "3           3  0002c17436637c4fe1837c935c04de47adb18e9a.story      1   \n",
       "4           4  0003ad6ef0c37534f80b55b4235108024b407f0b.story      1   \n",
       "\n",
       "   validation  test  duplicate source  highlights  broken  \n",
       "0           0     0          0      0           4       0  \n",
       "1           0     0          0      0           4       0  \n",
       "2           0     0          0      0           3       0  \n",
       "3           0     0          0      0           4       0  \n",
       "4           0     0          0      0           3       0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load metadata\n",
    "cnn_meta = pd.read_csv('cnn_meta.csv')\n",
    "print(len(cnn_meta.index),'rows')\n",
    "cnn_meta.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Formatting\n",
    "\n",
    "Right now, we have a text file for each story. T5 requires a single matrix (a dataset object is perfect) with source/target columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 passed, \n",
      "\n",
      "time: 0.2742847124735514 minutes\n",
      "\n",
      "Dataset({\n",
      "    features: ['source', 'target'],\n",
      "    num_rows: 100\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "source_text_train = []\n",
    "target_text_train = []\n",
    "\n",
    "# get list of training files\n",
    "train_files = cnn_meta[cnn_meta['train']==1].reset_index()['story']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# for i in range(len(train_files)):\n",
    "for i in range(100):\n",
    "    \n",
    "    # get formatted input and target\n",
    "    story, highlights = format_story('ernesto/cnn_stories_tokenized/'+ train_files[i])\n",
    "    \n",
    "    # format data as story/joined highlights pairs\n",
    "    source_text_train.append(story)\n",
    "    target_text_train.append('. '.join(highlights)+'.')\n",
    "    \n",
    "#     source_text.append(tokenizer('summarize: ' + story, return_tensors='tf').input_ids)\n",
    "#     target_text.append(tokenizer(highlights, return_tensors='tf').input_ids)\n",
    "    \n",
    "    if (i+1)%100 == 0:\n",
    "        print(i+1, \"passed\", end = ', ')\n",
    "        \n",
    "\n",
    "# print the time this took in minutes\n",
    "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
    "print('')\n",
    "\n",
    "train_df = pd.DataFrame(list(zip(source_text_train, target_text_train)),columns =['source', 'target'])\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "time: 0.0519867738087972 minutes\n",
      "Dataset({\n",
      "    features: ['source', 'target'],\n",
      "    num_rows: 20\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "source_text_val = []\n",
    "target_text_val = []\n",
    "\n",
    "# get list of validation files\n",
    "val_files = cnn_meta[cnn_meta['validation']==1].reset_index()['story']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# for i in range(len(val_files)):\n",
    "for i in range(20):\n",
    "    \n",
    "    # get formatted input and target\n",
    "    story, highlights = format_story('ernesto/cnn_stories_tokenized/'+ val_files[i])\n",
    "    \n",
    "    # format data as story/joined highlights pairs\n",
    "    source_text_val.append(story)\n",
    "    target_text_val.append('. '.join(highlights)+'.')\n",
    "    \n",
    "#     source_text.append(tokenizer('summarize: ' + story, return_tensors='tf').input_ids)\n",
    "#     target_text.append(tokenizer(highlights, return_tensors='tf').input_ids)\n",
    "    \n",
    "    if (i+1)%100 == 0:\n",
    "        print(i+1, \"passed\", end = ', ')\n",
    "\n",
    "# print the time this took in minutes\n",
    "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
    "\n",
    "val_df = pd.DataFrame(list(zip(source_text_val, target_text_val)),columns =['source', 'target'])\n",
    "val_dataset = Dataset.from_pandas(val_df)\n",
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's get some baseline loss values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "tensor(3.4585, grad_fn=<NllLossBackward>)\n",
      "tensor(3.2363, grad_fn=<NllLossBackward>)\n",
      "tensor(5.1195, grad_fn=<NllLossBackward>)\n",
      "tensor(2.4872, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# check some stories for pre-training loss\n",
    "for i in range(5):\n",
    "    \n",
    "    # get formatted input and target\n",
    "    story, highlights = format_story('ernesto/cnn_stories_tokenized/'+val_files[i])\n",
    "    \n",
    "    #train the model\n",
    "    input_ids = tokenizer('summarize: ' + story, return_tensors='pt').input_ids\n",
    "    labels = tokenizer('. '.join(highlights)+'.', return_tensors='pt').input_ids\n",
    "    \n",
    "    # compute loss (this returns an array of things)\n",
    "    loss = model(input_ids=input_ids, labels=labels).loss\n",
    "    \n",
    "    # print loss (sum of array values above)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db577dbae4cb423dac7297a93fddff28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e94963ea27bc494ca866af276df61216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_tokenized = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
    "val_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'labels', 'source', 'target'],\n",
       "    num_rows: 20\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 02:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "time: 2.7486213127772015 minutes\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "output_dir = 'multisentence_model'\n",
    "\n",
    "# training arguments to feed to Trainer object\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = 'baseline_model', # trained model will be saved here\n",
    "    num_train_epochs = 2, # number of times each story will be touched\n",
    "    per_device_train_batch_size = 8, # number of examples per batch\n",
    "    per_device_eval_batch_size = 8, # number of examples per batch\n",
    "    eval_accumulation_steps = 1,\n",
    "    prediction_loss_only = True,\n",
    "    learning_rate = 0.001,\n",
    "    evaluation_strategy = 'steps',\n",
    "    save_steps = 10,\n",
    "    save_total_limit = 1,\n",
    "    remove_unused_columns = True,\n",
    "    run_name = 'run_name',\n",
    "    logging_steps = 500, # print loss after this many steps\n",
    "    eval_steps = 500, # calculate loss after this many steps\n",
    "    logging_first_step = False,\n",
    "    load_best_model_at_end = True,\n",
    "    metric_for_best_model = \"loss\", \n",
    "    greater_is_better = False\n",
    ")\n",
    "\n",
    "# create Trainer to feed the train/dev data\n",
    "trainer = Trainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = train_tokenized,\n",
    "    eval_dataset = val_tokenized\n",
    ")\n",
    "\n",
    "# train the model and save it to our directory\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir + '/model')\n",
    "\n",
    "# print the time this took in minutes\n",
    "print('\\n\\ntime:', (time.time()-start)/60, 'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How did Training affect the loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1109, grad_fn=<NllLossBackward>)\n",
      "tensor(2.6054, grad_fn=<NllLossBackward>)\n",
      "tensor(3.3036, grad_fn=<NllLossBackward>)\n",
      "tensor(3.9340, grad_fn=<NllLossBackward>)\n",
      "tensor(2.2287, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "### Check the same stories as before and pray the loss has decreased ###\n",
    "\n",
    "# load our model\n",
    "baseline_model = T5ForConditionalGeneration.from_pretrained('multisentence_model/model')\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    # get formatted input and target\n",
    "    story, highlights = format_story('ernesto/cnn_stories_tokenized/'+val_files[i])\n",
    "    \n",
    "    #train the model\n",
    "    input_ids = tokenizer('summarize: ' + story, return_tensors='pt').input_ids\n",
    "    labels = tokenizer('. '.join(highlights)+'.', return_tensors='pt').input_ids\n",
    "    \n",
    "    # compute loss (this returns an array of things)\n",
    "    loss = baseline_model(input_ids=input_ids, labels=labels).loss\n",
    "    \n",
    "    # print loss (sum of array values above)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <unk> The Dukes of Hazzard '' ran until 1985 and spawned television movies, an animated series and video games. In the 1950s and 1960s, Best accumulated scores of credits ; he played a variety of supporting characters in such TV shows as The Twilight Zone, Bonanza and Gunsmoke.</s>\n",
      "\n",
      "<pad><pad> The attorney for a suburban New York cardiologist is calling the allegations against his client unsubstantiated. It doesn't matter what anyone says, he is presumed to be innocent. He's got patients to see him.</s>\n",
      "\n",
      "<pad><pad> <unk> The President enrolled at Occidental College in Los Angeles in 1979 - LRB. He credits the Clean Air Act with making Americans a lot healthier, in addition to being able to see the mountains in the background because they aren't covered in smog.</s>\n",
      "\n",
      "<pad><pad> A Russian TV channel aired Hillary Clinton's first campaign video with a rating stamp. It features about five seconds of two men holding hands. The video was released over the weekend to announce the start of her 2016 U.S. presidential campaign.</s>\n",
      "\n",
      "<pad><pad> In the 2016 race, Marco Rubio is seeking the Republican presidential nomination. He is a fierce opponent of <unk> Obamacare's executive action on immigration.</s>\n",
      "\n",
      "ROUGE F1: 0.22621460302185561\n"
     ]
    }
   ],
   "source": [
    "# for scoring outputs\n",
    "rouge = Rouge()\n",
    "\n",
    "# get list of training files\n",
    "test_files = cnn_meta[cnn_meta['test']==1].reset_index()['story']\n",
    "\n",
    "scores = []\n",
    "\n",
    "# for i in range(len(test_files)):\n",
    "for i in range(5):\n",
    "    # format the text to input/target format\n",
    "    story, highlights = format_story('ernesto/cnn_stories_tokenized/'+test_files[i])\n",
    "\n",
    "    # encode the input\n",
    "    encoded = tokenizer.encode('summarize: ' + story.replace('\\n',' '), return_tensors='pt')\n",
    "\n",
    "    # generate the output\n",
    "    output = baseline_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
    "                             min_length=30, max_length=300, early_stopping=True)\n",
    "    summary = tokenizer.decode(output[0])\n",
    "    print(summary)\n",
    "    print('')\n",
    "    \n",
    "    # get ROUGE scores between thoutput and highlights\n",
    "#     scores = [rouge.get_scores(summary,highlight)[0]['rouge-1']['f'] for highlight in highlights]\n",
    "    score = rouge.get_scores(summary,'. '.join(highlights)+'.')[0]['rouge-1']['f']\n",
    "    \n",
    "    scores.append(score)\n",
    "\n",
    "print('ROUGE F1:',np.mean(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
