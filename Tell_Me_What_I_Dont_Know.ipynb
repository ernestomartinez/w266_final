{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tell Me What I Dont Know.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1b8bdb3da74f498aa3d75a07b6b5ed35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6ce74e6c51034f2bbc96d064af49b2f9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78f9ec46abc14cd8bc5a7bd9bfe44662",
              "IPY_MODEL_4512c744eecd4f65bd34660b4f7018fc"
            ]
          }
        },
        "6ce74e6c51034f2bbc96d064af49b2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78f9ec46abc14cd8bc5a7bd9bfe44662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b8e1e2bd34764eb08508221925de1165",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1197,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1197,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ce2b37fe08bf42f6b7e2139063d8589b"
          }
        },
        "4512c744eecd4f65bd34660b4f7018fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ba3551d4da8c4e12950bd323e612f762",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.20k/1.20k [00:00&lt;00:00, 3.20kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd2280f5dabb4490bc6736309dc55a9b"
          }
        },
        "b8e1e2bd34764eb08508221925de1165": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ce2b37fe08bf42f6b7e2139063d8589b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ba3551d4da8c4e12950bd323e612f762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd2280f5dabb4490bc6736309dc55a9b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0f5fb7672bf448d6a59b370e7ccb09a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_eb6612edd211431db061341d320f8a09",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_4b6649230f8d4a4796120bf7f88e9cde",
              "IPY_MODEL_01a6350104b94e67b4929d4ec6e8aa39"
            ]
          }
        },
        "eb6612edd211431db061341d320f8a09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b6649230f8d4a4796120bf7f88e9cde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a0ba1b222a314d2eaa8f27e04a6695da",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 242065649,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 242065649,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aaa3dbbaac1649e4983b0c3c1435eb9f"
          }
        },
        "01a6350104b94e67b4929d4ec6e8aa39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e512f16f86684ac3ad0432cf2c0ed265",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 242M/242M [00:08&lt;00:00, 28.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ef076476e9d742908f9193ee0addc3ec"
          }
        },
        "a0ba1b222a314d2eaa8f27e04a6695da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aaa3dbbaac1649e4983b0c3c1435eb9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e512f16f86684ac3ad0432cf2c0ed265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ef076476e9d742908f9193ee0addc3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8df7bf0ba3b548868d635360ae00cfe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_b8c136b620974a6dae0fcbb7fe7b1a3c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f762948f2928495db2cb282a913bf1cc",
              "IPY_MODEL_fec4e736c1ad4bf0b40f558069f01f08"
            ]
          }
        },
        "b8c136b620974a6dae0fcbb7fe7b1a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f762948f2928495db2cb282a913bf1cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_738c13e75d7a46db883dd1df532412a5",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 791656,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 791656,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c80bfe584564b639739a61f19b0dcca"
          }
        },
        "fec4e736c1ad4bf0b40f558069f01f08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_03c7b733910d4d00bad88120fcbe45ec",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 792k/792k [04:50&lt;00:00, 2.72kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4c40ee9549cb41bba5c7236499d02fae"
          }
        },
        "738c13e75d7a46db883dd1df532412a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c80bfe584564b639739a61f19b0dcca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "03c7b733910d4d00bad88120fcbe45ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4c40ee9549cb41bba5c7236499d02fae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1aa1fec87b9f420bbbb795d9e2b19008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8e3fabed2b64ce7abc48ebacb817328",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c2d3b96c74242fea9258ea95e02e4a3",
              "IPY_MODEL_9b4a3f2b85a1493da081136d4083457e"
            ]
          }
        },
        "d8e3fabed2b64ce7abc48ebacb817328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c2d3b96c74242fea9258ea95e02e4a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_5c8823c249b44b898d4431069af0d364",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1389353,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1389353,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d9dd2214a0464d249f73b6b2abf84f6c"
          }
        },
        "9b4a3f2b85a1493da081136d4083457e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_676b238e9c304c73bfe01bbb532aade4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.39M/1.39M [04:50&lt;00:00, 4.78kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8eb838639c0f4e39bc6829f55b7181d2"
          }
        },
        "5c8823c249b44b898d4431069af0d364": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d9dd2214a0464d249f73b6b2abf84f6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "676b238e9c304c73bfe01bbb532aade4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8eb838639c0f4e39bc6829f55b7181d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y11ViCbDaTop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxOt0aMiDd3s"
      },
      "source": [
        "# <center>Tell Me What I Don't Know</center>\n",
        "#### <center>An exploration of selective summarization using the CNN Dataset</center>\n",
        "<center>by Dylan Mair and Ernesto Martinez<center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLsDS7jGy87h"
      },
      "source": [
        "## Premise\n",
        "\n",
        "Field Service Activity reports produced by customer support combine a natural language description with precise attributes that categorize the nature of the engagement - such as ‘who’ carried out ‘what’ with ‘whom’. The description can be very wordy, such as an attached email trail. We seek to summarize these, capturing highlights beyond our known attributes. Using T5 summarization (Raffel et al 2020) of the non-anonymized CNN-DailyMail dataset (See et al 2017) as a baseline, we seek a methodology that significantly de-emphasises what we already know to summarize the next most salient information. T5 summarization is a Transformer architecture that uses self-attention mechanisms to solve multiple text processing problems. \n",
        "\n",
        "We will assign one @highlight of each article as our prior knowledge. We will use ROUGE success metrics (Ganesan 2018) to determine whether our summaries resemble our labels, and cross examine labels with prior knowledge to identify and possibly exclude existing dependencies. We could try excluding prior knowledge from model training if it had consistent characteristics, however we expect to show this is not true of our analogous public dataset.\n",
        "\n",
        "A simple solution might be to generate two summary sentences and eliminate the sentence that more closely resembles prior knowledge. The result may still contain significant prior knowledge. We will also explore the potential of using Maximal Marginal Relevance (Rachman et al 2019; Carbinell et al 2016) and Integer Linear Programming (Roth & Srikumar 2016) to select summary sentences that avoid redundant facts.\n",
        "\n",
        "An efficient methodology may involve adjusting the model for each article to de-emphasise prior knowledge. This may resemble debiasing, however it will change for every article. Rebuilding the model each time is unlikely to be practical. We will confirm this with timing tests and compare speed and accuracy of our novel (yet to be determined) approach.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwgZcVCqXq8T"
      },
      "source": [
        "#### Sample FSA\n",
        "\n",
        "| Example of an FSA ||\n",
        "|:-----|:-------------|\n",
        "| `CSA`: Adam |  `Customer`: Shell |\n",
        "| `Activity`: Training | `Products`: GEPS |\n",
        "| `Attendees`: Coen, Anouk, Mae, Lee, Lars, Elke** |\n",
        "| `Description`: Second GEPS training for Shell, 6 attendees this time and interacting during the call with questions, one user commented that the GEPS product would be excellent for her day to day workflow when looking at competitor monitoring. I showcased the Upstream Intelligence Beta and the global impact special interest tags and alluded to the API but there was no direct interest from users. |\n",
        "| `Summary for Sales`: Adam trained 6 users at Shell on GEPS. GEPS seen as excellent for competitor monitoring. Users not interested in APIs.|\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8QXGTDPnVGd"
      },
      "source": [
        "\n",
        "1. Train the model with all highlights\n",
        "2. Train the model with only the desirable highlights (exclude first highlight in each case)\n",
        "3. Train the model with only the desirable highlights, while somehow 'discouraging' the set of undesirable (first) highlights.\n",
        "\n",
        "Ultimately we seek methodologies to eliminate the first highlight in our results.\n",
        "\n",
        "Scenario 2 is not expected to find anything systematic about the \"first highlights\", so the result should just be noisier (result of using a smaller training set). It will be interesting to see if there is more to this.\n",
        "\n",
        "Scenario 3 may suffer the same problem as Scenario 2 - discouraging a set of solutions while training the whole training set is not very targeted. We might expect to find a more effective modification that is applied during prediction rather than modeling.\n",
        "\n",
        "This is not a perfect analogue for our customer care cases, which have a very clear and systematic structure to the undesirable \"highlights\", such as: \\<Person> \\<verbed> \\<X> users at \\<company> using \\<Product>. Unfortunately we cannot use company data for this project, so instead we're considering a case more akin to resolving \"updates\" to a story that go beyond the original gist extracted when the story was first published (considering perhaps that the updates are throughout the story, not simply concatenated).\n",
        "\n",
        "But first, let's see how many highlights our stories have."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eAq-fUHL1gl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbhXfCwV0TAh"
      },
      "source": [
        "### Data Sourcing\n",
        "\n",
        "CNN stands for Cable News Network in this case, not C Neural Network.\n",
        "\n",
        "The CNN dataset and the DM (Daily Mail) dataset was originally created by [Hermann et al (2015)](https://arxiv.org/abs/1506.03340) by crawling the CNN website. The process was recreated by [Ramesh Nallapati et al (2016)](https://arxiv.org/abs/1602.06023) and then [See et al (2017)](https://arxiv.org/abs/1704.04368) using links to a web archive. See provided the [links to the articles in a github](https://github.com/abisee/cnn-dailymail) (in the directory `url_list`. However the data was not included subsequent to legal advice. The data was later prepared and uploaded to  [github by Jaffer Wilson](https://github.com/JafferWilson/Process-Data-of-CNN-DailyMail), tokenized but not embedded. They are provided as `CNN_STORIES_TOKENIZED.zip` and `FINISHED_FILES.zip`. However:\n",
        "* `CNN_STORIES_TOKENIZED` does not identify which stories were used for training, validation or testing.\n",
        "* `FINISHED_FILES.zip` (a binary repository) provides story-highlight pairs, ie. each story reproduced to pair with each of its highlights. Our ultimate workflow requires multiple highlights linked to each story.\n",
        "\n",
        "Either dataset could be recovered. Ultimately we took the long road, chasing each URL and finding a `CCN_STORIES_TOKENIZED` story to match. This was labour intensive however the discovery of duplicates might warrant our approach. Further work might determine whether duplicates were resolved when generating `FINISHED_FILES.zip`.\n",
        "\n",
        "Searching the `CNN_STORIES_TOKENIZED` required all stories to be loaded into memory. This was done on a local laptop while we were still resolving the Colab issue with too many files in a single directory. This search step is shown in Section 1.3 below for reference, using stories already split into train/valid/test subdirectories.\n",
        "\n",
        "The results of Section 1.3 were then uploaded to Google Drive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN01FtIZBieV"
      },
      "source": [
        "## Loading Data and Metadata"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWKWdV7eKae0",
        "outputId": "865d7f17-ca9c-493e-b73c-899cde0cfb10"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install rouge\n",
        "!pip install sentencepiece\n",
        "!pip install datasets\n",
        "!pip install pytorch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/81/91/61d69d58a1af1bd81d9ca9d62c90a6de3ab80d77f27c5df65d9a2c1f5626/transformers-4.5.0-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 11.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/cd/342e584ee544d044fb573ae697404ce22ede086c9e87ce5960772084cad0/sacremoses-0.0.44.tar.gz (862kB)\n",
            "\u001b[K     |████████████████████████████████| 870kB 54.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 55.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.44-cp37-none-any.whl size=886084 sha256=3755a918e0d900c02852b6f3a08c900ef58955af995f1b597b718078cd821fc4\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/fb/c0/13ab4d63d537658f448366744654323077c4d90069b6512f3c\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.44 tokenizers-0.10.2 transformers-4.5.0\n",
            "Collecting rouge\n",
            "  Downloading https://files.pythonhosted.org/packages/43/cc/e18e33be20971ff73a056ebdb023476b5a545e744e3fc22acd8c758f1e0d/rouge-1.0.0-py3-none-any.whl\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.0\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 12.4MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/90/43b396481a8298c6010afb93b3c1e71d4ba6f8c10797a7da8eb005e45081/datasets-1.5.0-py3-none-any.whl (192kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/11/f7689b996f85e45f718745c899f6747ee5edb4878cadac0a41ab146828fa/fsspec-0.9.0-py3-none-any.whl (107kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 17.0MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e7/27/1c0b37c53a7852f1c190ba5039404d27b3ae96a55f48203a74259f8213c9/xxhash-2.0.0-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n",
            "Successfully installed datasets-1.5.0 fsspec-0.9.0 huggingface-hub-0.0.8 xxhash-2.0.0\n",
            "Collecting pytorch\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/67/f403d4ae6e9cd74b546ee88cccdb29b8415a9c1b3d80aebeb20c9ea91d96/pytorch-1.0.2.tar.gz\n",
            "Building wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pytorch\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n",
            "Installing collected packages: pytorch\n",
            "    Running setup.py install for pytorch ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-mwbzut2_/pytorch/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-mwbzut2_/pytorch/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-nrv_46ce/install-record.txt --single-version-externally-managed --compile Check the logs for full command output.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iC-7Z3jiYBo",
        "outputId": "6c1be1bc-7254-4060-9064-534e2a487902"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# sign in to mount your google drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9JQI5V-C6js",
        "outputId": "fc2a6b2d-3827-43f9-a014-0468781435d2"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "project_id = 'w266-301118'\n",
        "!gcloud config set project {project_id}\n",
        "!gsutil ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Updated property [core/project].\n",
            "gs://cnn_stories/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h7ypwhqC7g-",
        "outputId": "ff2fd7bb-31ae-4dde-ea0c-0b65fdddd1e1"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "import torch\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, TFT5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from datasets import Dataset\n",
        "from rouge import Rouge\n",
        "import nltk.data\n",
        "nltk.download('punkt')\n",
        "\n",
        "DATADIR = '/content/drive/MyDrive/cnn_data/cnn_stories_tokenized'\n",
        "TOPDIR = '/content/drive/MyDrive/cnn_data'\n",
        "\n",
        "os.chdir(TOPDIR)\n",
        "print(\"Contents of the Top Directory:\")\n",
        "print(os.listdir())\n",
        "\n",
        "# The data has already been sorted into smaller subdirectories so we can\n",
        "# safely list the contens of the data directory.\n",
        "# See 'Multiple Data Directories' section for details below.\n",
        "os.chdir(DATADIR)\n",
        "print(\"Contents of the Data Directory:\")\n",
        "print(os.listdir())\n",
        "# !unzip cnn_stories.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Contents of the Top Directory:\n",
            "['naive_exclusion_model', 'example.story', 'finished_files', 'story_list.txt', 'cnn_stories_tokenized', 'cnn_meta.csv', 'all_stories.pkl', 'baseline_model', 'output', 'runs', 'exclude_highlights_model']\n",
            "Contents of the Data Directory:\n",
            "['train1', 'train2', 'train3', 'valid', 'train4', 'test']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3nDaGpWo0Ht"
      },
      "source": [
        "Unzipping file uploaded to My Drive. This took 21.8 minutes to complete. Files had previously been tagged as train, validation or test articles in `cnn_meta.csv` prepared on a local computer to avoid Colab issues with too many files in one directory.\n",
        "\n",
        "Using our metadata tags, we create three lists of files, for training, validation and testing. We do some basic filtering along the way (meta tags explained in Sidebar below)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzJRkn20ibIE",
        "outputId": "746dfe51-ba8f-4938-e17e-2b1d6b5a0f0b"
      },
      "source": [
        "cnn_meta = pd.read_csv(TOPDIR + '/cnn_meta.csv')\n",
        "\n",
        "print(list(cnn_meta.columns))\n",
        "print(\"(Some) duplicate stories:\", np.sum(list(cnn_meta['duplicate'])))\n",
        "print(\"Stories with only 1 highlight:\",\n",
        "      np.unique(list(cnn_meta['highlights']), return_counts=True)[1][0])\n",
        "print(\"Broken not-highlights:\", np.sum(list(cnn_meta['broken'])))\n",
        "print(\"Empty stories:\", np.sum(list(cnn_meta['nostory'])))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['filenum', 'story', 'dirstory', 'train', 'validation', 'test', 'duplicate', 'source', 'highlights', 'broken', 'nostory']\n",
            "(Some) duplicate stories: 14\n",
            "Stories with only 1 highlight: 59\n",
            "Broken not-highlights: 221\n",
            "Empty stories: 119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyHuJEvS8coH"
      },
      "source": [
        "The 4 file lists are filtered to remove duplicates, solo-highlights and non-stories (broken) and missing stories (nostory)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X56PlhoDilCA",
        "outputId": "a5c07e2b-3d5c-4a8a-e7d2-257d5561a555"
      },
      "source": [
        "train_meta = cnn_meta[ (cnn_meta['train'] == 1) &\n",
        "                       (cnn_meta['duplicate'] == 0) &\n",
        "                       (cnn_meta['highlights'] > 1) &\n",
        "                       (cnn_meta['broken'] == 0) &\n",
        "                       (cnn_meta['nostory'] == 0) ]\n",
        "train_files = list(train_meta['dirstory'])\n",
        "valid_meta = cnn_meta[ (cnn_meta['validation'] == 1) &\n",
        "                       (cnn_meta['duplicate'] == 0) &\n",
        "                       (cnn_meta['highlights'] > 1) &\n",
        "                       (cnn_meta['broken'] == 0) &\n",
        "                       (cnn_meta['nostory'] == 0) ]\n",
        "valid_files = list(valid_meta['dirstory'])\n",
        "test_meta = cnn_meta[ (cnn_meta['test'] == 1) &\n",
        "                      (cnn_meta['duplicate'] == 0) &\n",
        "                      (cnn_meta['highlights'] > 1) &\n",
        "                       (cnn_meta['broken'] == 0) &\n",
        "                       (cnn_meta['nostory'] == 0) ]\n",
        "test_files = list(test_meta['dirstory'])\n",
        "print(\"Train files:\", len(train_files), \"Validation files:\",\n",
        "      len(valid_files), \"Test files:\", len(test_files))\n",
        "\n",
        "stories = list(cnn_meta['story'])\n",
        "file_list = list(cnn_meta['dirstory'])\n",
        "print(\"All stories\", len(stories), \"Files\", len(file_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train files: 89904 Validation files: 1182 Test files: 1081\n",
            "All stories 92579 Files 92579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4YA9vqf95_h"
      },
      "source": [
        "def format_story(text):\n",
        "    \"\"\"Given the CNN data file, reformats to separate the story from the\n",
        "    highlights. Highlights are returned as a single string\"\"\"\n",
        "    \n",
        "    # split the story and highlights\n",
        "    split_text = text.split('\\n\\n@highlight\\n\\n')\n",
        "    story = split_text[0]\n",
        "    highlights = split_text[1:]\n",
        "    \n",
        "    # return both\n",
        "    return story, highlights#'. '.join(highlights)+'.'\n",
        "\n",
        "def cos_sims(out_sent, ref_sents):\n",
        "    \"\"\"gets cosine similarities for an output sentence with respect to the\n",
        "    highlight sentences. Returns the sum of values.\"\"\"\n",
        "    \n",
        "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
        "    \n",
        "    # get sentence level vectors with tf-idf\n",
        "    tfidf = vect.fit_transform([out_sent] + ref_sents)\n",
        "    \n",
        "    # get similarity matrix\n",
        "    similarity_mat = tfidf * tfidf.T\n",
        "    \n",
        "    # only values comparing \"out_sent\" with each sent in \"ref_sents\"\n",
        "    return similarity_mat.toarray()[:1,1:][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18iFEqUl2sBF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ee871e-3eb3-48b0-a54a-a1090921400e"
      },
      "source": [
        "story_list = []\n",
        "highlights_list = []\n",
        "\n",
        "os.chdir(DATADIR)\n",
        "\n",
        "RELOAD_DATA = False\n",
        "\n",
        "if RELOAD_DATA:\n",
        "    start = time.time()\n",
        "    for i in range(len(file_list)):\n",
        "    # for i in range(1000):\n",
        "      file = open(file_list[i], 'r', errors='ignore')\n",
        "\n",
        "      full_text = file.read()\n",
        "      file.close()\n",
        "      story, highlights = format_story(full_text)\n",
        "      story_list.append(story)\n",
        "      highlights_list.append(highlights)\n",
        "\n",
        "      if (i + 1) % 500 == 0:\n",
        "        print(i+1, \"read\")\n",
        "    print('\\n\\ntime:', (time.time() - start) / 60, 'minutes')\n",
        "\n",
        "    all_stories = pd.DataFrame(list(zip(file_list, story_list, highlights_list)),\n",
        "                              columns =['filename', 'story', 'highlights'])\n",
        "    print(all_stories.shape)\n",
        "    all_stories = all_stories.set_index('filename')\n",
        "    print(all_stories.shape)\n",
        "    all_stories.head()\n",
        "\n",
        "    all_stories.to_pickle(TOPDIR + '/all_stories.pkl')\n",
        "\n",
        "if not RELOAD_DATA:\n",
        "    start = time.time()\n",
        "    all_stories = pd.read_pickle(TOPDIR + '/all_stories.pkl')\n",
        "    print('\\n\\Loaded in:', time.time() - start, 'seconds')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\\Loaded in: 6.025774717330933 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53MXFVfLLCo8"
      },
      "source": [
        "For reference, the data loaded in 273 minutes = 5 &half; hours using Colab not-Pro (Pro not available to users in Singapore)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ID-Bv0TUpe7t",
        "outputId": "8b9c1b4f-2d50-4b9e-d1f9-d1887fbc22f0"
      },
      "source": [
        "all_stories.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>story</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>filename</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>train1/0001d1afc246a7964130f43ae940af6bc6c57f01.story</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>[Syrian official : Obama climbed to the top of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>train1/0002095e55fcbd3a2f366d9bf92a95433dc305ef.story</th>\n",
              "      <td>-LRB- CNN -RRB- -- Usain Bolt rounded off the ...</td>\n",
              "      <td>[Usain Bolt wins third gold of world champions...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>train1/00027e965c8264c35cc1bc55556db388da82b07f.story</th>\n",
              "      <td>Kansas City , Missouri -LRB- CNN -RRB- -- The ...</td>\n",
              "      <td>[The employee in agency 's Kansas City office ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>train1/0002c17436637c4fe1837c935c04de47adb18e9a.story</th>\n",
              "      <td>Los Angeles -LRB- CNN -RRB- -- A medical docto...</td>\n",
              "      <td>[NEW : A Canadian doctor says she was part of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>train1/0003ad6ef0c37534f80b55b4235108024b407f0b.story</th>\n",
              "      <td>-LRB- CNN -RRB- -- Police arrested another tee...</td>\n",
              "      <td>[Another arrest made in gang rape outside Cali...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                story                                         highlights\n",
              "filename                                                                                                                                                \n",
              "train1/0001d1afc246a7964130f43ae940af6bc6c57f01...  It 's official : U.S. President Barack Obama w...  [Syrian official : Obama climbed to the top of...\n",
              "train1/0002095e55fcbd3a2f366d9bf92a95433dc305ef...  -LRB- CNN -RRB- -- Usain Bolt rounded off the ...  [Usain Bolt wins third gold of world champions...\n",
              "train1/00027e965c8264c35cc1bc55556db388da82b07f...  Kansas City , Missouri -LRB- CNN -RRB- -- The ...  [The employee in agency 's Kansas City office ...\n",
              "train1/0002c17436637c4fe1837c935c04de47adb18e9a...  Los Angeles -LRB- CNN -RRB- -- A medical docto...  [NEW : A Canadian doctor says she was part of ...\n",
              "train1/0003ad6ef0c37534f80b55b4235108024b407f0b...  -LRB- CNN -RRB- -- Police arrested another tee...  [Another arrest made in gang rape outside Cali..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imtSYmDaIonj"
      },
      "source": [
        "### Sidebar: Making Metadata\n",
        "\n",
        "Initially we were not aware that Colab limits files-per-directory so initial data exploration was done on a laptop * with results collated in Excel. Working backwards from the resulting metadata (sample shown below):\n",
        "\n",
        "* *Dell Latitude 7280 laptop, 16GB RAM, Intel&reg; Core&trade; i7-6600U CPUs, 4 @ 2.60GHz, Intel&reg; HD Graphics 520, Windows 10 Enterprise.* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "nyj1E2vXqNdj",
        "outputId": "dba905fd-eacf-4265-e54a-c6225d8fdb9b"
      },
      "source": [
        "cnn_meta[79:89:3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filenum</th>\n",
              "      <th>story</th>\n",
              "      <th>dirstory</th>\n",
              "      <th>train</th>\n",
              "      <th>validation</th>\n",
              "      <th>test</th>\n",
              "      <th>duplicate</th>\n",
              "      <th>source</th>\n",
              "      <th>highlights</th>\n",
              "      <th>broken</th>\n",
              "      <th>nostory</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>79</td>\n",
              "      <td>003d23a256ce34d73e05968a04727c0ed4a2a456.story</td>\n",
              "      <td>train1/003d23a256ce34d73e05968a04727c0ed4a2a45...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>82</td>\n",
              "      <td>003f8c8953025e086fa773c9b40d9b8cd6d9754c.story</td>\n",
              "      <td>train1/003f8c8953025e086fa773c9b40d9b8cd6d9754...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>85</td>\n",
              "      <td>0041698b4463a633f912681b96f73648cb012e33.story</td>\n",
              "      <td>test/0041698b4463a633f912681b96f73648cb012e33....</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>http://web.archive.org/web/20150522032639id_/h...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>88</td>\n",
              "      <td>0044e296ecfe3ba57a351ad2a36d034491e878ce.story</td>\n",
              "      <td>valid/0044e296ecfe3ba57a351ad2a36d034491e878ce...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>http://web.archive.org/web/20150324215354id_/h...</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    filenum                                           story  ... broken  nostory\n",
              "79       79  003d23a256ce34d73e05968a04727c0ed4a2a456.story  ...      0        0\n",
              "82       82  003f8c8953025e086fa773c9b40d9b8cd6d9754c.story  ...      0        0\n",
              "85       85  0041698b4463a633f912681b96f73648cb012e33.story  ...      0        0\n",
              "88       88  0044e296ecfe3ba57a351ad2a36d034491e878ce.story  ...      0        0\n",
              "\n",
              "[4 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InzgwXSEOseE"
      },
      "source": [
        "#### Identifying Test and Validation Data\n",
        "\n",
        "We began with the `CNN_STORIES_TOKENIZED` data which links each article to multiple highlights. In order to make any direct comparisons with prior work we sought to tag training, validation and testing articles in exactly the same way. Test articles come from April 2015; validation articles from March 2015; and training articles were published from April 2007 to February 2015. Unfortunately the publication dates were not saved with the articles so there was no direct link between these articles and their role in the modeling.\n",
        "\n",
        "With URLs provided for 1,000+ test articles, 1,000+ validation articles, and 90,000+ training articles, we chose to manually identify just the test and validation articles, and assume everything else was training articles. Each URL was clicked on, and a piece of the resulting CNN article was pasted into a search loop to compare with the `CNN_STORIES_TOKENIZED` corpus stored in memory.\n",
        "\n",
        "Reading all articles into memory:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqM5wgYhrDKO",
        "outputId": "1674f43f-57d2-4dcd-b0e2-82a5a0530c59"
      },
      "source": [
        "# This code tests a string against all of the stories, and lists any stories\n",
        "# (filenames) that match. Again, those subdirectories weren't there during the\n",
        "# original search.\n",
        "\n",
        "text = 'A 16-year-old teenager has been charged for'\n",
        "\n",
        "matches = all_stories[ all_stories['story'].str.contains(text) ]\n",
        "print(*list(matches.index), sep='\\n')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valid/a3a67e56b951f6dd6e53e6299e252454b5ccac8b.story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnT6PzK2XOcj"
      },
      "source": [
        "That step was done for EVERY. SINGLE. TEST and VALIDATION URL. If the string was not unique, then another string from the article was chosen.  Apostrophes and periods were modified by tokenization and so the search string would need to be adjusted accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AojDccvlz7h"
      },
      "source": [
        "#### Identifying Duplicates\n",
        "A few duplicates were identified in this process, such as this well-worn story about the pronunciation of `GIF`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZGUlR0GcXDp",
        "outputId": "c4833250-35dc-4e37-98f9-e61c9c6d913e"
      },
      "source": [
        "text = 'on the big screens at the Cipriani Wall'\n",
        "\n",
        "matches = all_stories[ all_stories['story'].str.contains(text) ]\n",
        "print(*list(matches.index), sep='\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "valid/e53d78944b7c417dbf36fa1430380c4ef052c95b.story\n",
            "train4/ee36e99232ee871002ce2dfbd803ae133b2b5b50.story\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIyHLrAOXIQA",
        "outputId": "93bfd70e-962e-451e-ebde-20a502211a27"
      },
      "source": [
        "story1 = all_stories.loc[all_stories.index ==\n",
        "             'valid/e53d78944b7c417dbf36fa1430380c4ef052c95b.story',\n",
        "             'story'].iloc[0]\n",
        "story2 = all_stories.loc[all_stories.index ==\n",
        "             'train4/ee36e99232ee871002ce2dfbd803ae133b2b5b50.story',\n",
        "             'story'].iloc[0]\n",
        "\n",
        "print(story1.replace('\\n', '').replace(' .', '.\\n').replace('@', '\\n@'))\n",
        "print('------------------------------------------------------------')\n",
        "print(story2.replace('\\n', '').replace(' .', '.\\n').replace('@', '\\n@'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We ca n't settle iPhone vs. Android or `` Star Wars '' vs. `` Star Trek '' for you.\n",
            " But another long-running geek debate was put to rest Tuesday night.\n",
            "Those short , animated loops that have captivated the Web for decades ? They 're pronounced like a brand of peanut butter.\n",
            "Steve Wilhite created the Graphics Interchange Format , or GIF , while working for Compuserve in 1987.\n",
            " On Tuesday , he received a Webby Award for it and delivered his five-word acceptance speech -LRB- that 's all the Webbys allow -RRB- by flashing a GIF on the big screens at the Cipriani Wall Street in New York.\n",
            "And , in a flash , it all became clear :`` It 's pronounced JIF , not GIF.\n",
            " ''Of course , in the grand tradition of heated debate , a flat statement of fact by the creator was n't enough to sway some partisans.\n",
            " On Twitter , `` GIF '' became a trending topic as some folks pushed back.\n",
            "`` Graphics Interchange Format.\n",
            " Graphics.\n",
            " Not Jraphics.\n",
            " #GIF #hardg , '' wrote Web designer Dan Cederholm.\n",
            "`` So instead of GIF , we 've got to say JIF ? YEAH RIGHT , '' chimed in October Jones , creator of the `` Texts From Dog '' Tumblr and book.\n",
            " `` And I suppose those animals with long necks are called ` JIRAFFES.\n",
            " ' ''And , of course , the peanut butter brand was getting lots of free publicity along the way.\n",
            " The always amusing HAL 9000 account -LRB- yes , somebody tweets as the robot from `` 2001 '' -RRB- posted an `` animated JIF '' -- which is to say , a swirling , animated jar of the tasty , high-protein spread.\n",
            "So , it 's perhaps no surprise that the company got into the act itself.\n",
            " Wednesday afternoon , the company took to Twitter with a post reading , `` It 's pronounced Jif ®.\n",
            " '' The tweet linked to , what else , a multi-colored GIF flashing the same phrase.\n",
            "Animated GIFs were a staple of the early Internet.\n",
            " Remember The Dancing Baby ? That 's a GIF.\n",
            "They fell out of favor as more advanced graphics technology emerged.\n",
            " But in the past couple of years , the Web has remembered how much fun it is to watch ridiculous things happen over and over again.\n",
            "Appropriately , Wilhite received his Lifetime Achievement Award from David Karp , the founder of Tumblr , one prominent place where GIFs found a new fanbase.\n",
            "In less publicized interviews , Wilhite had argued for the soft-G pronunciation for years.\n",
            " So , will a widely covered `` speech '' in front of some of the Web 's most influential folks finally be the turning point ?Maybe not.\n",
            "Last month , no less an authority than the White House posted an image on its new Tumblr feed advocating for the hard-G.\n",
            " And the Oxford English Dictionary says both pronunciations are acceptable.\n",
            "So , here 's wishing Mr. Wilhite `` Jood Luck.\n",
            " ''\n",
            "------------------------------------------------------------\n",
            "-LRB- CNN -RRB- We ca n't settle iPhone vs. Android or `` Star Wars '' vs. `` Star Trek '' for you.\n",
            " But we can settle another long-running geek debate :Those short , animated loops that have captivated the Web for decades ? They 're pronounced like a brand of peanut butter.\n",
            "Steve Wilhite created the Graphics Interchange Format , or GIF , while working for Compuserve in 1987.\n",
            " When he received a Webby Award in 2013 for it , and delivered his five-word acceptance speech -LRB- that 's all the Webbys allow -RRB- , he flashed a GIF on the big screens at the Cipriani Wall Street in New York.\n",
            "And , in a flash , it all became clear :`` It 's pronounced JIF , not GIF.\n",
            " ''Of course , in the grand tradition of heated debate , a flat statement of fact by the creator was n't enough to sway some partisans.\n",
            " On Twitter , `` GIF '' became a trending topic as some folks pushed back.\n",
            "`` Graphics Interchange Format.\n",
            " Graphics.\n",
            " Not Jraphics.\n",
            " #GIF #hardg , '' wrote Web designer Dan Cederholm.\n",
            "`` So instead of GIF , we 've got to say JIF ? YEAH RIGHT , '' chimed in October Jones , creator of the `` Texts From Dog '' Tumblr and book.\n",
            " `` And I suppose those animals with long necks are called ` JIRAFFES.\n",
            " ' ''And , of course , the peanut butter brand was getting lots of free publicity along the way.\n",
            " The always amusing HAL 9000 account -LRB- yes , somebody tweets as the robot from `` 2001 '' -RRB- posted an `` animated JIF '' -- which is to say , a swirling , animated jar of the tasty , high-protein spread.\n",
            "Animated GIFs were a staple of the early Internet.\n",
            " Remember The Dancing Baby ? That 's a GIF.\n",
            "They fell out of favor as more advanced graphics technology emerged.\n",
            " But in the past couple of years , the Web has remembered how much fun it is to watch ridiculous things happen over and over again.\n",
            "Wilhite has argued for the soft-G pronunciation for years.\n",
            " Yet no less an authority than the White House has posted an image on its Tumblr feed advocating for the hard-G.\n",
            " And the Oxford English Dictionary says both pronunciations are acceptable.\n",
            "So , here 's wishing Mr. Wilhite `` Jood Luck.\n",
            " ''\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQMTo7spkJ26"
      },
      "source": [
        "They're not identical (\"in 2013\" added to the third highlight, perhaps because the original article was published 2 years earlier) but they're almost entirely the same. Since we did not go manually through all the train story links we likely missed other duplicates but we flagged the ones we found anyway in the meta information table."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaBYeaLhmg-Q"
      },
      "source": [
        "#### Minimum 2 Highlights\n",
        "\n",
        "Our Premise, that we can downplay one summary (highlight) in favour of others, requires that we have at least two highlights. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2NC4UmQcm7Y",
        "outputId": "55bc27e8-f616-4230-cf00-91c49332f708"
      },
      "source": [
        "all_stories['hlcount'] = all_stories['highlights'].str.len()\n",
        "\n",
        "print(\"Total number of highlights:\", all_stories['hlcount'].sum())\n",
        "print(\"Average highlights per article:\",round(all_stories['hlcount'].mean(),3))\n",
        "print(\"Count of number of highlights per article:\\n\",\n",
        "      all_stories.groupby('hlcount')['hlcount'].count())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of highlights: 329439\n",
            "Average highlights per article: 3.558\n",
            "Count of number of highlights per article:\n",
            " hlcount\n",
            "1       59\n",
            "2     3028\n",
            "3    34691\n",
            "4    54754\n",
            "5       47\n",
            "Name: hlcount, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LV2j9T1DreEJ"
      },
      "source": [
        "We have just 59 articles that have only 1 highlight. They represent a tiny proportion of the articles. We tag them in the meta data and we will likely eliminate those entirely.\n",
        "\n",
        "We compare this with the number of pairs that See et al (2017, \"Get to the Point\") used in their dataset - which had been carefully aligned with prior work by using the same cleanup script that prior authors (Nallapati et al., 2016, 2017) had used. Unfortunately this script is not attached to any of the relevant papers. As we go along we might find further reasons for cleanup affecting 17K highlights (such as the length of such highlights).\n",
        "\n",
        "That's 17,256 pairs more in our data, than in the data used by other authors. We can definitely cut more aggressively here. Let's explore the size of train and test used by See et al:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-f-jc1oBucO8",
        "outputId": "58b8a2ea-13f5-4622-c659-578c3efa17f1"
      },
      "source": [
        "# 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.\n",
        "\n",
        "x = 3028 * 2 + 34691 * 3 + 54754 * 4 + 47 * 5\n",
        "print(\"Number of pairs in our dataset:\", x)\n",
        "print(\"Difference with See:\", x - 287266 - 13368 - 11490)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of pairs in our dataset: 329380\n",
            "Difference with See: 17256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqop6N4tu_SR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa27186d-be95-4d6b-d88d-8bae792313bd"
      },
      "source": [
        "# 287,226 training pairs, 13,368 validation pairs and 11,490 test pairs.\n",
        "\n",
        "see_pairs = 287226 + 13368 + 11490\n",
        "print(\"See training %\", round(100*287226/see_pairs, 2))\n",
        "print(\"See validation (dev) %\", round(100*13368/see_pairs, 2))\n",
        "print(\"See test %\", round(100*11490/see_pairs, 2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "See training % 92.03\n",
            "See validation (dev) % 4.28\n",
            "See test % 3.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOnp1Gw9viGa"
      },
      "source": [
        "Interesting facts from Hermann, 2015, regarding creation of this dataset:\n",
        "\n",
        "Table 1: Corpus statistics. Articles were collected starting in April 2007 for CNN and June 2010 for the Daily Mail, both until the end of April 2015. Validation data is from March, test data from April 2015. Articles of over 2000 tokens and queries whose answer entity did not appear in the context were filtered out.\n",
        "\n",
        "At this point we realize the Daily Mail articles are definitely missing from our dataset. Do we want to bring them in later? Do we ignore them for the homework?\n",
        "\n",
        "Hermann paper: https://arxiv.org/pdf/1506.03340.pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWjevmpIt1Aw"
      },
      "source": [
        "#### Broken highlights\n",
        "\n",
        "Along the way we explored the length of the highlights, in words. They had a strange distribution:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRhK0Y61z2Gf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "529f7938-3ec4-4c63-d5bb-844c2d3d67b8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "all_stories = all_stories.assign(hl_lens=[[len(k.split()) for k in row] for\n",
        "                                          row in all_stories.highlights])\n",
        "\n",
        "flat_hl_words = [ x for nums in list(all_stories['hl_lens']) for x in nums ]\n",
        "\n",
        "print(\"Number of @highlights:\", len(flat_hl_words))\n",
        "\n",
        "plt.hist(flat_hl_words, density=False, bins=44)  # density=False gives counts\n",
        "plt.ylabel('Count (log display)')\n",
        "plt.yscale('log')\n",
        "plt.xlabel('Words in the @highlight');"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of @highlights: 329439\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS5BrGvQ1-Nh"
      },
      "source": [
        "There are definitely some peculiar results happening for highlights over 40 words long. Let's take a look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL6uAolpzvoh",
        "outputId": "4c929fc0-2d42-44ea-f285-7b96eabbcc62"
      },
      "source": [
        "for index, row in all_stories.iterrows():\n",
        "    for hl in range(len(row['hl_lens'])):\n",
        "        if row['hl_lens'][hl] > 42:\n",
        "            print(index, 'highlight', hl, 'has', row['hl_lens'][hl], 'words: ',\n",
        "                  row['highlights'][hl])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train1/00f222d27b211c4d83e77f297ccfebe6a652f900.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/027388f2c138301038249446d8c71170f470e83a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/034c096518e949eb207d14af18dffd39549eb084.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/03ea986d6ea03bbbf7851bbc4e0b92bfbb0c4e68.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/061efe1739b2f6a6e5e9192c68b5e8347841567f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/070af64d0540469ac16304a4919149257361ef01.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/0a23ba95558bc16e21815b6d1297b60c0e78caaa.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/0bcc5c7a3e66e114aaa39dcca83d4656d5978022.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/0f359f891df28b1c4eb2243d327de77468ec96f1.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/102e5dba7396a186fc5427a150bd512a8440ae11.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/13c640e0d9272486c48e01fa3ee623be492b6f18.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/1647f34613dfa54ca72bba2e4258ecf285b0c98d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/176e18fb6091c162b0b28dec636f73fbb9a9590f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/17c38a9baea749e9f3a965f56b40a6a5362e92a3.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/19bd07aceab873b1acc66cc84cfc949bb5b4f422.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/1dfd6f356085157fdc79cbb2834225323b11323e.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/1e04b82913430f6a5026d7c482b4cca7cdee8005.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/2046467e1e3b3870c3e8640e5422cec3985b027a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/206de4767426d2f1180853f3b331c5999b902c26.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/2140406283d5aee49a5916dca537206796e7f22e.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/21a46cd422844dd137452e452a105b9ee3107261.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/225441bb4dd046d67466717672f82e913ceb3ae5.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "test/225f2330a6477e05fd3cebb31d3c87243cd40eb6.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/231b652e51aa4c9c57e1f82d440bea8f8850c5df.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "test/23d1195f78742c6abe27bb5447ac993a78fcedd1.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/24bff63a7272bc1e90960d71b17a2f58fe185b19.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/24f9a20d17b4a7612906956d2d5554404ca147cf.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/2a62c8786848758cb2ac3a25b2d2981ea02bd448.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "test/2ca115f57e765b2ce61e8e0adeca3cbabd3dd4a1.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/2d9a78313a94d6c040d339088431db3fe662773e.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call !\n",
            "train1/2de3f4f94969ca49c3e922d8798f58462e6c237f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/2e10981191fc25d0ae04a8dfe82f7ea575298e1b.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/300455a5f9aa0f1d65078af0abf0cbeaed426247.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/3150391d39c32680a9fe5eb36ba21ddd2754957a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/3668789e5d82357aeeef2454d263336b0e6ddd1a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/39e8aee64512607909625e21776dc38a862a2bb8.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/3b4c3296e4dafabda9bb30f4717f4177ecbab2fb.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/3ea97b839102e7c13e26cc2a6fc9203e71843275.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train1/44c9e72ae89bc43d207fb72342555cc59f8fad25.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/45a4af2a7b8fdcec01db4a46ba302e6ddc16a491.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/46916855a5ebac16718309d8aae85af18b9b8e7d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/4b6cc024477ae709783fbed12df3eec3dd52aa41.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/4b96b47548ff22032385a979ff9b9c726ebc1ee0.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/4c3e3e7348884e98690875c335627b8b62d00b5f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/4e9b292ad8225ab4fccb89cfcfb451e892365918.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/5569c84dab9169907197143f8b0263f23d29c08a.story highlight 1 has 44 words:  NEW : Former Air Force fighter pilot has worked with NASA as safety consultant NYC mayor says pilot checked plane twice for passengers before leaving `` I 've flown in a lot of planes and that was a phenomenal landing , '' passenger said\n",
            "train2/57163032076b6fbfb6faac3d5c9e8a05ab7f526d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/57a17adf02dd053009868eb93e958d912dfd0c2c.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/597701c1f566ba2ecce7e57f45a2cbc529bee48f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/5ea176e7d6ae32a7a1aeaa6d4427aef9b0df4949.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/5f65f0cfa18ab44d0c2bfc52b99e1b287b8fd3f5.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/61e60bae5722d4495477f2093c1d47a7e0f57cb0.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/6228621b24dda1c4707b60a20c3bc3df74ae6160.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/62968eb515e3b44d1af34c975470afb808efc989.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/62e6def47b76c2c39cbaa280314e22581305bea4.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/6427e4d3b2762397218f6298e00e7f48952d0a56.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/6529ca4ce3845f571121a0c7ac28f1b7fe4cbacd.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/65a5494c97a417daeaf5f74fedae54816c95c9ae.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call !\n",
            "train2/6671aa5bde79419f7d4964ee4ea9518eb18e47df.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/66e219eb4819191c447c829bd89cff1693eeb6ed.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/6778ca292cbd1d58d9da8efc56545e0cbfecc7d2.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/693c4e57d80ce32a5ac965f54f9e3dc475840967.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/69c8c00b7121be845a3c9395d818f5686fdd57a1.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/6bd75c94c7432c18face90c957d000780db4951f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/72b85a8e325ab55dac66b0e59c00e9aeb7a71428.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7394cbd65b7068d212d1d91b392fdc6f94817bf5.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/744dd18b6aac9eb06446491592818c17b29df2ca.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/74d3a6aaf83868730f84a829a1f8898784befbea.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7672665910a1d44c77c700503a120d3a886ca083.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/77333eba545b15308570fea217e42fb45bebaf49.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/78786ac33958e82f815013d3400e900419c8d9ca.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/79941d61c117f204e1069ca0db952a664898e82c.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7998179f10150f26cfba61b5217cb31b9a4b88c6.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/7a35a4889e481b2dd97ffb62ddfb1d33e3a79b9a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/7af6d7b1d9dc6fd6408089acdfd36d19d3fdc271.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7b5b173f5294a8065a418fe44da8f86c1d965174.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7bd934765033f928fe875ab77ae7c4bee1ac5ea5.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7c087f2e2566916c864f6b805d322278717cc9ee.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7c2459886d6711b03b4aa1cbfbb59e2a7a61e44a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7de723ee27aeb659083f60b9dbdc0b4531ca5a23.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7ef15bbbdc67d13ea6fc23335a35db928ce9bd4c.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/7f7a5f91f03434663399584e9d87dda278bd16b2.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/80e505bbd36cd74dc3dd8022c07813deee69fbf5.story highlight 2 has 44 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News .\n",
            "\n",
            "- You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call !\n",
            "train2/80fb4a7f593611ee786f64f896d45c33ff7eaffb.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/838ff4015bc974b8a930b57b0372774d276cd36b.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train2/8827ede8c129346939edb38f133e8a4583432fba.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/89b409451c271eb34923a8bdd8584ede18893365.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/8abad9e7450166a0c9a5141c935c825b24c7577b.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/8c331ad26e01b7888d4703bd4d3dae44173d247a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/8d09c6fc1bf582bc42d10a9a04e1f6f702b882f2.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/905ff0c7afa46b3d18bbea62b5af44eef8a87fcd.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/90824bd5199ac8df2eba624a24634e02a3d7e3de.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/9095a4574a34ba86bb68d90e3a40bf506960ccab.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/9109d261260fa40104d9a7b7abc4bbe212ad6ec4.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/996f35f29005edfa930c5ce460f25755c62f7329.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/9b64215d7aeae0f8db246d5eba08657a73ab09aa.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/9cc6785428bbe5c2036edc36fb8dfec962dd4c68.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/a146256fa6faffc2216edf682719228ee6fbe413.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/a274d9a109037c74fde723d97fadd10b85ee3f25.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/a2bf7ff4dd019c1e765c3016d8548011eac87b1d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/a5afac5daa574c1c005a047a17e81da10999aded.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/a6086f48393a337c6532b5481a980360a6d3cc42.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/a8369c2986b97d6599e2d001e490393606a2b137.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/ac0f85dfb394f1ae26f283eca756a02ed8e3e563.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/ac3c0521a93891dfac7b6dafc9601e5618550573.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/af11f7039f4a0a04058abf962181ce0e4c86caac.story highlight 0 has 43 words:  Unmanned Indian probe lands succesfully on the moon Friday Probe hit the moon at a speed of 5760 kilometers per hour Two-year mission seeks high-resolution , 3-D imaging of the moon 's surface Probe will also search for evidence of water or ice\n",
            "train3/afc4b37ea2ca9e62620699c4f5c05b2135b0dbbd.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/b028d3d76271da75f5f23fda303cef877dced73a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/b3d6d845979f1ff2d72fac077a05828170440f72.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/b629751290d4693636d16880b5b98b4dcfbc7f43.story highlight 1 has 44 words:  NEW : Former Air Force fighter pilot has worked with NASA as safety consultant NYC mayor says pilot checked plane twice for passengers before leaving `` I 've flown in a lot of planes and that was a phenomenal landing , '' passenger said\n",
            "train3/b6f8772a58838497e57ff780fbb3b220d6911de6.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/ba6091a972fdca03583c5af6845af68c0e755240.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/bbbd3212034ac1efc2d5f3a5e8da07b8cf303195.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/bdf542b05c1717ae9ef064e5fafa4a2946785d09.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/be754a20d8cc7f5569888cd8aedce73bbff3c35c.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/bf3d4723b9093ba7bec31f37cbccf691575b6ea9.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/c1626015c9f64d49451b1d4cc7a1039d66e944a6.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/c17d7b362ef2571c2b32209b4096e9bd200bff08.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/c306266a9ef2e6c3c89d042600ec2db1f51b52d9.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/c61800416bbfb10a1aa1c3d8f8b77f388155afea.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/c6d2512aaed885690728209997b8272faa3d0a8d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/c88b9f2d5dcb291ef3f68b95d527703a9ba9fb35.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train3/c8c605902f07ae5805c3d50a826eae6e870b1e29.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/d15c080bdea259cdb219cefde90b4b4e38c595be.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/d2b34274b0c107110f0e11e50ef8a1446bbfae8d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/d641c661cbf1a8763174a5c838b9692106879372.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/d80ce6113483fa636f06bae62ae9e697f02ae113.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/dad0fac64e4101a506df45574b2f38c654beff5f.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/dad1b88de084e46c22459d659f05ef762191c46d.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/de6158159fa9febf2a233990c65396c678703eaf.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/e466675b2acd67615e33a769739912083299ee5c.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "test/e6d598193acfbce5fbbab20d7051caff5c8f2e59.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/e8dfb2210afc3eaf01b2b4cb4fd3f262df02c2e7.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/e9b2caa4db36a5bfcaa2e8bc9c47e554b21b4a73.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/e9e6a5b7675dc3864b2e97c0275837a7459d1e02.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/ea4e3479ed095aef555dd8b725297c547e49708c.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/eab620850f153b0d18505710a12136a20bba3501.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/eb17e74d37f6dc3d8ea9c63cf99f8ae2260084eb.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "valid/ee6ab07bfbd589c317f6851b0b0e22e0dedcd3ee.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/f0a4278a568e5f46daa2deed2d9ab0601724c196.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/f24b71aa06d50c3885de3dfcb374f2b64b6506ac.story highlight 4 has 43 words:  The bottom line : The average person ca n't run out and get an MRI that shows if they 're on the road to Alzheimer 's just yet . But these findings could help drugmakers as well as patients in the future .\n",
            "train4/f863c4a0f0b8f58e903ef070f42bbe5ae413793a.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n",
            "train4/ffa58b2550e343b43bfdf91cf364a8d8ca7200dc.story highlight 2 has 43 words:  At the bottom of the page , comment for a chance to be mentioned on CNN Student News . You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTd2VKc25S8r"
      },
      "source": [
        "Okay, that was grim. All those items with 43 words seem identical. Let's look at one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0Mi96021PGG",
        "outputId": "387c104e-bdf2-40d2-d146-9cf19888d126"
      },
      "source": [
        "print(fullitems[92451].replace('\\n', '').replace(' .', '.\\n').replace('@', '\\n@'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-LRB- CNN Student News -RRB- -- October 29 , 2014If you know what the southernmost U.S. state is , then you know where we 're going for one of the stories on today 's show.\n",
            " The location 's slow-moving natural disaster is a study in geology and earth science.\n",
            " Also featured is a debate concerning quarantines and the Ebola virus.\n",
            " And we look inside a plane that appears to be made of giant windows , though it actually has none for passengers.\n",
            "On this page you will find today 's show Transcript and a place for you to request to be on the CNN Student News Roll Call.\n",
            "TRANSCRIPTClick here to access the transcript of today 's CNN Student News program.\n",
            "Please note that there may be a delay between the time when the video is available and when the transcript is published.\n",
            "CNN Student News is created by a team of journalists who consider the Common Core State Standards , national standards in different subject areas , and state standards when producing the show.\n",
            "ROLL CALLFor a chance to be mentioned on the next CNN Student News , comment on the bottom of this page with your school name , mascot , city and state.\n",
            " We will be selecting schools from the comments of the previous show.\n",
            " You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call !Thank you for using CNN Student News !\n",
            "@highlightThis page includes the show Transcript\n",
            "@highlightUse the Transcript to help students with reading comprehension and vocabulary\n",
            "@highlightAt the bottom of the page , comment for a chance to be mentioned on CNN Student News.\n",
            " You must be a teacher or a student age 13 or older to request a mention on the CNN Student News Roll Call.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5JYKve0wfkn",
        "outputId": "ed0e6ef0-40c0-434b-fd82-13b6cba0dbc5"
      },
      "source": [
        "for index, row in all_stories.iterrows():\n",
        "    for hl in range(len(row['hl_lens'])):\n",
        "        if row['hl_lens'][hl] < 3:\n",
        "            print(index, 'highlight', hl, 'has', row['hl_lens'][hl], 'words: ',\n",
        "                  row['highlights'][hl])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train1/0e4bc29471e4ffa620e415545bc346125978579a.story highlight 2 has 1 words:  Hoy\n",
            "train2/67b80daf1af70b724a810788b8714e8a63af2a19.story highlight 0 has 2 words:  NEW :\n",
            "train2/705c762be1488af409da95bece16ee68c1f2671e.story highlight 0 has 2 words:  NEW :\n",
            "train3/9f5a789344f92ea35c2fba5f7bc83ad36ba64973.story highlight 0 has 2 words:  NEW :\n",
            "train3/a6608069b26f03b21bfff528e66e26389de0d48c.story highlight 0 has 1 words:  NEW\n",
            "train3/ac89f9a5d16ddc35ad5bf273f2252ea19e5dc3f4.story highlight 3 has 1 words:  She\n",
            "train3/b6cdedc2ed9f81b26474cec4df5b733727972778.story highlight 0 has 2 words:  NEW :\n",
            "train3/beb65b8103bfb7ca229752a8ccfba077a41a5039.story highlight 0 has 2 words:  NEW :\n",
            "train3/c44fb6d8cf7a2a8339aad828befdc0e298c41f49.story highlight 0 has 2 words:  Shooter identified\n",
            "train4/cf3e5d84d63daf2b7ac121370b9e10d3ec437e87.story highlight 0 has 2 words:  NEW :\n",
            "train4/f2422026814d2464b5f59db2668213fca26ebb05.story highlight 3 has 2 words:  Brown has\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAO-j6DB6GYz"
      },
      "source": [
        "It's a landing page for a set of articles, not an article itself. While the short highlights (\"NEW\"!) are poor, these items look utterly disposable. For simplicity, we choose to tag all highlights over 40 words as `broken` in the metadata. This amounts to 143 articles, increasing our broken count to 221."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce0a6G3o6M-T",
        "outputId": "0f782453-fd08-4d05-f5af-4049f65b3099"
      },
      "source": [
        "np.sum(list(cnn_meta['broken']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "221"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jI_EoGb7f1Gj"
      },
      "source": [
        "Further broken stories were identified when looking at small files on disk (searching for a nice short example to show in the paper). Specifically, there is a population of articles that represent landing pages for video articles.\n",
        "\n",
        "```\n",
        "highlights[1785]\n",
        "[\"-LRB- CNN Student News -RRB- -- February 8 , 2013\\n\\nDownload PDF maps related to today 's show :\\n\\nMontgomery , Alabama ; Selma , Alabama ; Little Rock , Arkansas ; Washington , D.C.\\nChina\\n\\nClick here to access the transcript of today 's CNN Student News program .\\n\\nPlease note that there may be a delay between the time when the video is available and when the transcript is published .\",\n",
        " \"The daily transcript is a written version of each day 's CNN Student News program\",\n",
        " 'Use this transcript to help students with reading comprehension and vocabulary',\n",
        " 'Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News']\n",
        " ```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65OWsZHjgEP6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a70be3bd-bb1c-469c-bd93-0896bb61cc38"
      },
      "source": [
        "text = 'Use this transcript to help students'\n",
        "\n",
        "for index, row in all_stories.iterrows():\n",
        "    for hl in range(len(row['highlights'])):\n",
        "        if text in row['highlights'][hl]:\n",
        "            print(index, 'has highlights', *list(row['highlights']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train1/04ff02e815e31dd23dbc3adfbf2ba0a86da2b80d.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/07f388a7cc70c658e7b954984dfed448cfd80524.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/0cd484f58d6ec1873d685aec7f73fa641ed180e1.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/0f080014669246c0ed9dd117f4bcc0e3a4944bcf.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/14aff1aa3338887018090f54b882ab59c703338d.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/17abcd15187a18677ce785e654c25b1bca29317f.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/187913f971eef6dd359493e4600df355cf46a393.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/26856fe68bc5d8c50aefeaba1db56dafca68f2eb.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/276a3642d02cbfb2f0cc941d9846a20f8b13225a.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/27be082b1615a0037079b6a1d6d73c77e91beef8.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/2a973d5c9c6c146636a538c12cf2bbaf7523ebdf.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/2e0c57099d5bdd51c4c2b7f2d8af14445d4d0ef9.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/2fa7f2d3863c8f9683975486f03cf31a920626ec.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/3389da9681076572640658cc6cc8c03f1792abc7.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the End of Year Quiz to test your knowledge of stories you saw on CNN Student News in 2011\n",
            "train1/3f488329d2b3cb7209eb0034de81ba74bb33bb15.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train1/442fba185f37862ff78d06729b4dba3cc650e368.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/48e10f7b199e900139976e82039329ef042a8edf.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/49b0d3d7ea9fc9e79e9359ef21a14098ed23ac61.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/4cdfb894deae5469827b86d82a20af342509f2e8.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/4f4796b5c3aea578517f2b5ddeaff656448b5692.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/50c363caf9cd2ce0dc3b04df621f643deffd188f.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/5245906f224a07f69babefdc28adb5208c715afa.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/54c24522da296412beb5257fd4fdf1b235f4b7ad.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary The weekly Newsquiz tests students ' knowledge of stories covered on CNN Student News\n",
            "train2/568074e9c80929509aaf6dabd387458e86d811ed.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/58f9fc222872584214973f0eaf867337c7f9be21.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/5926d8059c77e06a233c3020eeea3e4415e07e73.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/5cf1dfe43e610195a68388fa6f4e59672c9ec9a5.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/66c07ec8f0fa65d4db8f31539413380e44e5e51f.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/67ea001d1cc5f062a977780eabbe6286ee840678.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/6c43efaf7f71f223dffc67bc649a4329c89df08b.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/6dea4de76039b0b4b74904d4eee5ebaf8731b2c5.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/6e6183c81a89a224abc45754ce8ebf931a788bc2.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/7a264282ce176d05cb756ebf70508e04ed28f776.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/7ca3dca1f48edb4c6d03f6b0164b8edfcba68ec2.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/84e52ac8f12179f9bb40ec0652ead0abd4f35941.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/8638e8a3a7fa6ca4c453d8ca7e44389b58cfd912.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train2/8843ba2ffe0240cb8581bd28670f477466147435.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary The weekly Newsquiz tests students ' knowledge of stories covered on CNN Student News\n",
            "train3/8c529736fd10bc65700207b0ae6abde8d743b097.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/8caf785105061a41a79e4764624e8b8ae9268721.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/8d7e8ef5ea990750b81880852a4d79786b8f0a6b.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the end of year Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/8ec9552e9bed0f4e3f384311252f798c43f9324e.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/8f13aeb672d961d247ffeb1b9efd8c3cc8785a25.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/916ea0d61d9513834bc0cce4e8b3dac4de3d5d9a.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/9d4308089edce67a4229ab82500a41402ca39f01.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/9ec26636034aecea8377f0bc1d99a32a471ba20a.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/a2954c2bb1fb4eb0c69d3c143e701f0dad36c279.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/a93061d1ac217234c5a7b1bc3d105e60c94020ac.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/abc9815021fd87f1bb028cc8762745ecc581fd4d.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/b0924f7f30accd8c2092d73988a24172f4cd18db.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/b2559b360910731bdf85819e4d3888d90280481a.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/b3a69dc46f462cb2b1fd91d817ff7da68cc41d79.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/b591862a073e7a77cfd84ea3cf7794f13cbdbccb.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c080376cc1497c9b9ae407f65b2719623e669913.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the End of Year Newsquiz to test your knowledge of stories you saw on CNN Student News this school year\n",
            "train3/c19a1807eb56b1a69a0e943b8cfbffd8fdfa1818.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c200c792e3c14c0698ceb42d1707ac72a5bb0af5.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c29dff119426e8aeef8540a244f3a450e22c29a6.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c4a3964a23eef6b2de8790e7f096f4e696c7f9e3.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c4ad05f26414bb3a6f46e4528ac80e31af3b264d.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c4f958715bf4d96c3edc85680da4fb88d326e1e0.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c57f8ffb261d6fbcd1be519ed3e9e53a61eb8bed.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/c8c08940407bbacbc73828a9e380b61f9f3d53a1.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/cb05c077b3f883a2fdc59b24d7389ee98ea8fe51.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train3/cb863037a172a6890bae0e505a5c97b2350e92a8.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary The weekly Newsquiz tests students ' knowledge of stories covered on CNN Student News\n",
            "train4/d59b43c5306ea8f141f19e61b5a932b202f65870.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/d86f82598574b02d31230cb051b423ce01a772a1.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/d931967b00b6cadc1e401de9a897ae8ab941ef13.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/db18ee6d393c7ad9856f1a5c56dc3d760eddfe0b.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/de32aad6b7568131afcbdd54ead0bd8f5ab798a1.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/df273f3704bfdf607802e3c5a3e1dcccd59d7ca6.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/e8ff9028286644a8f17816cccb784033b1296df5.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/e9e6ccc98493d8c2736d1f4b0f59cb19bd7fbee5.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/ea6c2b9b8479325e3c081252b59c61047988736c.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/f123f518e2e7b25813062984f9e536e4574198bd.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/f4a27192d7ee5773c9a08c9cceecbc2cdf19bd6f.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/f548cf8b12740ab3356775ca46672b8f36e4977f.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the End of Year 2012 Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/fac8d8e2c6b3185ec03e20b5bc59b30c20eb888d.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/fd18d1c79d58540dd9047473f23673a1e6651662.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n",
            "train4/fd64d53bc79c61c80613d8cbd12b92c1fc0ba5db.story has highlights The daily transcript is a written version of each day 's CNN Student News program Use this transcript to help students with reading comprehension and vocabulary Use the weekly Newsquiz to test your knowledge of stories you saw on CNN Student News\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRh9XBtU555K"
      },
      "source": [
        "#### Broken stories\n",
        "\n",
        "Some stories have no text at all. Any story less than 200 characters has little meaning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4gYF46r6FBB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d2fd31-7c04-47df-c702-be59cbad9500"
      },
      "source": [
        "matches = all_stories[ all_stories['story'].str.len() < 200 ]\n",
        "\n",
        "for index, row in matches.iterrows():\n",
        "    print(index, 'story < 200 words:', row['story'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train1/00465603227f7f56fcd37e10f4cd44e57d7647d8.story story < 200 words: \n",
            "train1/07bca124a8b784800aa38e6237524eb756792c7c.story story < 200 words: \n",
            "train1/09ac649da7f5ae27d3a7ef47eeb9b8ea4cbd799c.story story < 200 words: \n",
            "train1/0de44dd3a66327f990e4f21b2c672424a07dbfb7.story story < 200 words: \n",
            "train1/118e01697b58e37f56bd03004a608a6b3126e384.story story < 200 words: \n",
            "train1/13abd3e35628071686b33a3b9201cd09da4e1a01.story story < 200 words: \n",
            "train1/158501546d04c78797fe4aa887540514469fa87d.story story < 200 words: \n",
            "train1/15e1efd925d246b6a93cda25fc3677e923081722.story story < 200 words: \n",
            "train1/17201d7e00b4227c985c8dd6ce80c925865d4720.story story < 200 words: \n",
            "train1/17c0b96f8b303bcf8ae6d4fc3f9776e535805d4f.story story < 200 words: \n",
            "train1/1963afbc839123244fa3c483b7d5256bf057f758.story story < 200 words: \n",
            "train1/1a00c87c9d1d192f103d6ccf2ad76062f2229609.story story < 200 words: \n",
            "train1/1a918e011f1952ed5c37c48c0b5f7350f804f865.story story < 200 words: \n",
            "train1/1b281d4630a5f53acbdf0add94372b0dbf52ccac.story story < 200 words: -LRB- CNN -RRB- -- Qatar plans to build nine fully air-conditioned open-air stadiums to stage matches at the 2022 FIFA World Cup .\n",
            "\n",
            "Click through the gallery above to see how the stadiums will look .\n",
            "train1/1efddd4caf41c8ecabbc6534b7ee38518c11f481.story story < 200 words: \n",
            "train1/200dc5cb7d6a675fc527b9b8d8d6b9c4223f9a95.story story < 200 words: \n",
            "train1/226ca83313bb4db0917847f80fcf4a2d2af5007d.story story < 200 words: \n",
            "train1/23b70ea2a46e6a9abbe4e46c0d80088c76f6a0af.story story < 200 words: \n",
            "train1/252a4b88464ec3219f3a541f3a87dc34c2d176c0.story story < 200 words: \n",
            "train1/2cb398794fea7b2dd83501c401c034ca73362323.story story < 200 words: \n",
            "train1/2dc5e01db66fa12817c2f4913f70db1d519604df.story story < 200 words: \n",
            "train1/2ea9af851da7da8e53247253f2c17425650f7aeb.story story < 200 words: \n",
            "train1/3135481259a4abea442463b5085d9b18c0665168.story story < 200 words: \n",
            "train1/325a0a28b647358e32731e174d5bcd2a7179f6bf.story story < 200 words: \n",
            "train1/3374e475b259036b025c137c7947b4a4305ad514.story story < 200 words: \n",
            "train1/3b522b0f65c9f2b0f57b6158a9363706cf4093c4.story story < 200 words: \n",
            "train1/3b61e966c25eb515ad82e5c51d78220b2a9a1f79.story story < 200 words: \n",
            "train1/3cc2fa9d872cea16c23b243301499ffc86369708.story story < 200 words: -LRB- CNN -RRB- -- CNN explores Beijing 's underground music scene and the bands making the rest of the world sit up and listen .\n",
            "train1/3d78806f43f1d7d4aac43f12d2fa6450d3e7c734.story story < 200 words: \n",
            "train2/47e8e99e7142974b06d6ca7184fcf84f4ac417de.story story < 200 words: \n",
            "train2/4a524c9714a5651a5f02497d23a8164f868babcd.story story < 200 words: \n",
            "train2/4b6e5184eadd3837a6f488cc02230bd4ac08c9cb.story story < 200 words: \n",
            "train2/4f61c32c9f68551fa37f1ba9e0ac8eb3183c2f9f.story story < 200 words: \n",
            "train2/50b9ecc13e205f4fde650e2cf46af980b805f5ce.story story < 200 words: \n",
            "train2/556de3fae14f7e33cc8b97805213049423155b54.story story < 200 words: \n",
            "train2/5652311e3db90786c75f994a1803e1d9eb8fb595.story story < 200 words: \n",
            "train2/59509e92b7cf482e8717549585cbd9b9b9a4d9f2.story story < 200 words: \n",
            "train2/59552b2a2e96a618d5bffbece2849438aa5d4ce1.story story < 200 words: \n",
            "train2/5bb46b196aebdef40d69305d66edb849fd1ab0e1.story story < 200 words: \n",
            "train2/5bd1db164dad9a37333f952502f65ce3d285534a.story story < 200 words: \n",
            "train2/5d221a50d10208b82177cd2b92ccd83dd6b9a501.story story < 200 words: \n",
            "train2/5dd9aaaca2b6ef805f4d8c50ea042e805fa427a6.story story < 200 words: \n",
            "train2/5f84ad5a9258b6620f9a1f7707af68d7980763d3.story story < 200 words: \n",
            "train2/62afb1f0c48eb670641f26fadc82b09c34ed6d9d.story story < 200 words: \n",
            "train2/63951f504244c71e2e821c7780643b56e103145c.story story < 200 words: \n",
            "train2/66a28ddaceb55e3f7b2777522f3c1062237cb25f.story story < 200 words: \n",
            "train2/6972eff7e00f5c5b8bc864de2db115f31d53df61.story story < 200 words: \n",
            "train2/6bb512326b18c236aca8d2de2633eae28fd3cb86.story story < 200 words: \n",
            "train2/6ce5c33e786e149d50e78aeadc5cf449e6da09ce.story story < 200 words: \n",
            "train2/6e782862e65a315c67f8de9b6c6d0b4de21a6126.story story < 200 words: \n",
            "train2/72855376efdaa110e598e17d36aaa5a46ed14425.story story < 200 words: \n",
            "train2/72aba2f58178f2d19d3fae89d5f3e9a4686bc4bb.story story < 200 words: \n",
            "train2/7ccc6f78ca5a895e729e10dbcd46a361eda88370.story story < 200 words: \n",
            "train2/7d7f347f8bff07d5a3c071e321f3b8747525558e.story story < 200 words: \n",
            "train2/7e94c09d00811e544d2d87deacc98b11de685cda.story story < 200 words: \n",
            "train2/7fa8a466d78b5232c91feceae35025b4c190e049.story story < 200 words: \n",
            "train2/84b530bc2b81c7f6b917906e0f6fc6dec87d4e8e.story story < 200 words: \n",
            "train2/85ae2133c0eb01a1cf6dc762b76b2bf5c0303a5b.story story < 200 words: \n",
            "train2/86bd905861391cbd3a98de15c83768b6d1400304.story story < 200 words: Congressman Jared Polis\n",
            "\n",
            "-LRB- D -RRB- Colorado : District 02\n",
            "\n",
            "Congressman Jason Chaffetz\n",
            "\n",
            "-LRB- R -RRB- Utah : District 03\n",
            "train3/8f9eabb8b4c50b7ec02943b27b172bf63058ae37.story story < 200 words: \n",
            "train3/9405f6cd3c08cb4232e255a8a0f940bd3fafd1d5.story story < 200 words: \n",
            "train3/97bb70b3837cff47e258c3ddf3aba494e6f4a139.story story < 200 words: \n",
            "train3/97be96d338ad91149bbf6c6376675f8d74bf2ac9.story story < 200 words: \n",
            "train3/9860d928e14347953050d94320165221e6a13b9c.story story < 200 words: \n",
            "train3/998ffb4a915b976983716d5aa19ec419eb4113b2.story story < 200 words: -LRB- CNN -RRB- -- Take advantage of the long weekend with a jolt to your senses . We asked iReporters to share photos of their favorite theme park thrills .\n",
            "train3/9a43998000a5ccd2c601ab3d4ee737f94fa1dd3f.story story < 200 words: \n",
            "train3/9d37272018e78cc2c08a03d0785134751559113f.story story < 200 words: \n",
            "train3/a188c6c37efedf75d6fed22b163384ed06159985.story story < 200 words: \n",
            "train3/a25f9cb21da1f59d6b8999cc11aedd0a9f4d4c7a.story story < 200 words: \n",
            "train3/a2a2ff9da575558d8d8544d0726b9a1c2aa20203.story story < 200 words: \n",
            "train3/a2dbee50cac09763a3c2bb2967f4c845e378feab.story story < 200 words: \n",
            "train3/a31c3eea301a7347b3f5add677c2d131f962e983.story story < 200 words: \n",
            "train3/a3725da94f0abe3dd115c8b4ca2f300f8fec2f5e.story story < 200 words: \n",
            "train3/a3b6e7cc664dc625625c52c1c75065f4d64fdf59.story story < 200 words: \n",
            "train3/a460c7560b149c25fd89f46115b8a7865b4b1062.story story < 200 words: \n",
            "train3/a548421e8517a8d92240bc09249776c051cdd392.story story < 200 words: \n",
            "train3/a57bddf79c4f96d69701a27d50cf17cd032d1c7a.story story < 200 words: \n",
            "train3/aa90a73e5663c5a0563411da4538b4cabf225528.story story < 200 words: \n",
            "train3/abb74e2466084a9968a305ac439229302e5de164.story story < 200 words: \n",
            "train3/ac10b63f389ac2b694b7e7ca48eb5a35e8dd82fd.story story < 200 words: \n",
            "train3/ad68b7fcf4f629c048d0367afab738ea1252e78e.story story < 200 words: \n",
            "train3/b2b5d58df498c2cd9fdd7d85a593d9723af5b8a1.story story < 200 words: \n",
            "train3/b584f48a1ca687e1a4092403478fccf9e49f38f5.story story < 200 words: \n",
            "train3/b5cf3c7ca910ef126f4f090421af860e8b16e598.story story < 200 words: \n",
            "train3/b750310e3a3537e2324539a055424849377bad66.story story < 200 words: \n",
            "train3/bab4d871c260eb84f1fc26aa7144ab202536bc70.story story < 200 words: \n",
            "train3/bc8c6a91d9a3b37f5175a6ee52afdd5857c1accf.story story < 200 words: \n",
            "train3/c08fb8961d12365b11a8af63fad93219d607f8b7.story story < 200 words: \n",
            "train3/c36fb222cee4c1f4e38cf62ad37e2eb8dd0a85be.story story < 200 words: \n",
            "train3/c4019c2d8811a0dce6111ca901e8f87dfd12d1ed.story story < 200 words: \n",
            "train3/c9d5ec30c9e88efebf792a243a064f9173312ef3.story story < 200 words: \n",
            "train3/ce3271caecf540a64b4ae1b5943194ee1084b712.story story < 200 words: \n",
            "train3/cf0c6788b92a287cccc4523776a442469796c4a8.story story < 200 words: \n",
            "train4/cfa9c2b6ef031670fc61f17a386836ec6b2e6c2d.story story < 200 words: \n",
            "train4/d444271ebb079b8f78c34197bfbcf87ae5a41129.story story < 200 words: \n",
            "train4/d49fae80783e17fd19776cc5e726eb7074c82821.story story < 200 words: \n",
            "train4/d4b4ee22583e0490d5e41e93941e8e6ec182d7ab.story story < 200 words: \n",
            "train4/d694a384c370848ffaac9ec9c64362b4dbaa4056.story story < 200 words: \n",
            "train4/d79f84285d756ff2f2837f2537f446a3dc77e531.story story < 200 words: \n",
            "train4/d91e44938337b408add4f1526811ac6e3258e570.story story < 200 words: \n",
            "train4/e302c9577379867f809dcf5bb81ef4cd4981e670.story story < 200 words: \n",
            "train4/e5509455d119c584a808ed400a875b9866a5760c.story story < 200 words: \n",
            "train4/e637d817aaf0a8df2e52ef9beed6b2abd33324f3.story story < 200 words: \n",
            "train4/edee4665e5bd44bf184218f870d3db7bbc56a24b.story story < 200 words: \n",
            "train4/ee2db1f51d7d77ca6b449668f256b78762645390.story story < 200 words: \n",
            "train4/ef65cc0a69ac25f86c5fa94354ae3b2b71aec3f4.story story < 200 words: \n",
            "train4/f2df67d1f4e67f8feff519039b9a424933d82655.story story < 200 words: \n",
            "train4/f3e35fbb7821204344e8f40336f68bd5a396e330.story story < 200 words: \n",
            "train4/f40905e6c4222e41e0ce8c3a58173bc08afd4ee9.story story < 200 words: \n",
            "train4/f7a82862d878453c968f343c1cc128695b33e78d.story story < 200 words: \n",
            "train4/f8de6d76c2ca4c519e5835ce7bee9175bb2bbd5f.story story < 200 words: \n",
            "train4/fa65f3da733c62eabd0fef3af46a3e71006f94e8.story story < 200 words: \n",
            "train4/fb2de39e76598a7df3fc83586bc8e8b76ee66000.story story < 200 words: \n",
            "train4/fc1a99c9e7171a3ac1e3be97fae1fc34b0fa2c9e.story story < 200 words: \n",
            "train4/fc7d6be6a3591ae1647324ed31e07245b9835b6c.story story < 200 words: \n",
            "train4/fd4bd93f0e11cec9a6c3f50441b6023b1e582581.story story < 200 words: Congressman Jared Polis\n",
            "\n",
            "-LRB- D -RRB- Colorado : District 02\n",
            "\n",
            "Congressman Jason Chaffetz\n",
            "\n",
            "-LRB- R -RRB- Utah : District 03\n",
            "train4/fe679e0c6630a794a04a40e5b892e024dbf860fe.story story < 200 words: \n",
            "train4/fe9b7daaab79553adaea3e9aa086510bca4b1fb3.story story < 200 words: \n",
            "train4/feda63ce911f8523fb191b5038377de670d7deae.story story < 200 words: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpyL6C21AeA-"
      },
      "source": [
        "### Multiple Data Directories\n",
        "\n",
        "Owing to the vaguaries of Colab, we needed to split our data into multiple directories. This was simply achieved by creating subdirectories, and adding names to our metadata that includes those names (the variable `dirstory`).\n",
        "\n",
        "Names were generated in Excel using the formula:\n",
        "```Excel\n",
        "=IF(E2,CONCAT(\"valid/\",B2),IF(F2,CONCAT(\"test/\",B2),IF(D2,CONCAT(\"train\",INT(A2/25000)+1,\"/\",B2),\"None\")))\n",
        "```\n",
        "* Column E = `validation`\n",
        "* Column F = `test`\n",
        "* Column D = `train` ; Column A = `filenum`\n",
        "* Column B = `story` (.story filename)\n",
        "\n",
        "Simply, test stories go into a test subdirectory; validation stories go into a \"valid\" subdirectory; and train stories go into train directory number int(filenum/25000)+1. The 90101 train stories end up in 4 directories.\n",
        "\n",
        "The code to move the files is shown below. It was executed on our laptops, the result was zipped and uploaded to our Google Drive as described above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7HpLB806YPv"
      },
      "source": [
        "import shutil\n",
        "\n",
        "SUBDIR_THE_FILES = False\n",
        "\n",
        "if SUBDIR_THE_FILES:\n",
        "    os.chdir(DATADIR)\n",
        "\n",
        "    if not os.path.isdir('train1'):\n",
        "        os.mkdir('train1')\n",
        "    if not os.path.isdir('train2'):\n",
        "        os.mkdir('train2')\n",
        "    if not os.path.isdir('train3'):\n",
        "        os.mkdir('train3')\n",
        "    if not os.path.isdir('train4'):\n",
        "        os.mkdir('train4')\n",
        "    if not os.path.isdir('test'):\n",
        "        os.mkdir('test')\n",
        "    if not os.path.isdir('valid'):\n",
        "        os.mkdir('valid')\n",
        "\n",
        "    i = 0\n",
        "    for i in range(len(list(cnn_meta['dirstory']))):\n",
        "        if i % 5000 == 1:\n",
        "            print(\"Story\", i)\n",
        "        if os.path.isfile(\"./\" + stories[i]):\n",
        "            shutil.move(\"./\" + stories[i], \"./\" + file_list[i])\n",
        "\n",
        "    os.chdir(TOPDIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYda7wwlH7cR"
      },
      "source": [
        "### \"Known Data\" Testing\n",
        "\n",
        "Prior to any modeling, we can assess how similar (or not) the \"known data\" (our first highlight) is to the other highlights. Ideally they won't match!\n",
        "\n",
        "We begin this exercise using just ROUGE-1-F, as that was the metric chosen in previous CNN studies. But instead of comparing a generated summary with a reference summary, we compare the first reference summary with the other reference summaries.\n",
        "\n",
        "Because we're so rigorous, we also compare the last reference summary with the other reference summaries, to see if perhaps the CNN editors get lazy, or maybe even better with time...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZau7bMRJtk3",
        "outputId": "cc528c79-dba1-427d-fb6a-4d547d5afec9"
      },
      "source": [
        "# all_hlighlights was defined above to capture all of the highlights\n",
        "# according to:\n",
        "# all_highlights[i] = fullitems[i].split('\\n\\n@highlight\\n\\n')[1:]\n",
        "\n",
        "print(fullitems[1].replace('\\n', '').replace(' .', '.\\n').replace('@', '\\n@'))\n",
        "print(\"   - - - - - - - - - \")\n",
        "all_highlights[1]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-LRB- CNN -RRB- -- Usain Bolt rounded off the world championships Sunday by claiming his third gold in Moscow as he anchored Jamaica to victory in the men 's 4x100m relay.\n",
            "The fastest man in the world charged clear of United States rival Justin Gatlin as the Jamaican quartet of Nesta Carter , Kemar Bailey-Cole , Nickel Ashmeade and Bolt won in 37.36 seconds.\n",
            "The U.S finished second in 37.56 seconds with Canada taking the bronze after Britain were disqualified for a faulty handover.\n",
            "The 26-year-old Bolt has now collected eight gold medals at world championships , equaling the record held by American trio Carl Lewis , Michael Johnson and Allyson Felix , not to mention the small matter of six Olympic titles.\n",
            "The relay triumph followed individual successes in the 100 and 200 meters in the Russian capital.\n",
            "`` I 'm proud of myself and I 'll continue to work to dominate for as long as possible , '' Bolt said , having previously expressed his intention to carry on until the 2016 Rio Olympics.\n",
            "Victory was never seriously in doubt once he got the baton safely in hand from Ashmeade , while Gatlin and the United States third leg runner Rakieem Salaam had problems.\n",
            "Gatlin strayed out of his lane as he struggled to get full control of their baton and was never able to get on terms with Bolt.\n",
            "Earlier , Jamaica 's women underlined their dominance in the sprint events by winning the 4x100m relay gold , anchored by Shelly-Ann Fraser-Pryce , who like Bolt was completing a triple.\n",
            "Their quartet recorded a championship record of 41.29 seconds , well clear of France , who crossed the line in second place in 42.73 seconds.\n",
            "Defending champions , the United States , were initially back in the bronze medal position after losing time on the second handover between Alexandria Anderson and English Gardner , but promoted to silver when France were subsequently disqualified for an illegal handover.\n",
            "The British quartet , who were initially fourth , were promoted to the bronze which eluded their men 's team.\n",
            "Fraser-Pryce , like Bolt aged 26 , became the first woman to achieve three golds in the 100-200 and the relay.\n",
            "In other final action on the last day of the championships , France 's Teddy Tamgho became the third man to leap over 18m in the triple jump , exceeding the mark by four centimeters to take gold.\n",
            "Germany 's Christina Obergfoll finally took gold at global level in the women 's javelin after five previous silvers , while Kenya 's Asbel Kiprop easily won a tactical men 's 1500m final.\n",
            "Kiprop 's compatriot Eunice Jepkoech Sum was a surprise winner of the women 's 800m.\n",
            "Bolt 's final dash for golden glory brought the eight-day championship to a rousing finale , but while the hosts topped the medal table from the United States there was criticism of the poor attendances in the Luzhniki Stadium.\n",
            "There was further concern when their pole vault gold medalist Yelena Isinbayeva made controversial remarks in support of Russia 's new laws , which make `` the propagandizing of non-traditional sexual relations among minors '' a criminal offense.\n",
            "She later attempted to clarify her comments , but there were renewed calls by gay rights groups for a boycott of the 2014 Winter Games in Sochi , the next major sports event in Russia.\n",
            "\n",
            "@highlightUsain Bolt wins third gold of world championship\n",
            "@highlightAnchors Jamaica to 4x100m relay victory\n",
            "@highlightEighth gold at the championships for Bolt\n",
            "@highlightJamaica double up in women 's 4x100m relay\n",
            "   - - - - - - - - - \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Usain Bolt wins third gold of world championship',\n",
              " 'Anchors Jamaica to 4x100m relay victory',\n",
              " 'Eighth gold at the championships for Bolt',\n",
              " \"Jamaica double up in women 's 4x100m relay\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 153
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz8L_JhvM3YK"
      },
      "source": [
        "So let's ROUGE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INgERb8SPLl2",
        "outputId": "5cd4bf0b-50fd-49c3-cfc3-af7b3fb57312"
      },
      "source": [
        "\n",
        "\n",
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge.get_scores(highlights[1][0], highlights[1][1])[0]['rouge-1']['f']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.024054982546734216"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 165
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nqo3_VyXC3eE",
        "outputId": "1d86b448-aa9d-4c9d-e827-61ac72037dcc"
      },
      "source": [
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "\n",
        "highs = 0\n",
        "scores = [ [] for x in range(len(all_highlights)) ]\n",
        "rouge = Rouge()\n",
        "\n",
        "for story in range(len(all_highlights)):\n",
        "    if story % 5000 == 1:\n",
        "        print(\"Story:\", story)\n",
        "    highs += len(all_highlights[story]) - 1\n",
        "    hlsc = []\n",
        "    for hl in range(1, len(all_highlights[story])):\n",
        "        hlsc.append(rouge.get_scores(all_highlights[story][0],\n",
        "                                     all_highlights[story][hl])[0]['rouge-1']['f'])\n",
        "    scores[story] = hlsc\n",
        "#    scores[story] = max(hlsc)\n",
        "\n",
        "print(highs / 90101)\n",
        "flat_scores = [ x for nums in scores for x in nums ]\n",
        "len(flat_scores)\n",
        "# len(scores)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Story: 1\n",
            "Story: 1001\n",
            "Story: 2001\n",
            "Story: 3001\n",
            "Story: 4001\n",
            "Story: 5001\n",
            "Story: 6001\n",
            "Story: 7001\n",
            "Story: 8001\n",
            "Story: 9001\n",
            "Story: 10001\n",
            "Story: 11001\n",
            "Story: 12001\n",
            "Story: 13001\n",
            "Story: 14001\n",
            "Story: 15001\n",
            "Story: 16001\n",
            "Story: 17001\n",
            "Story: 18001\n",
            "Story: 19001\n",
            "Story: 20001\n",
            "Story: 21001\n",
            "Story: 22001\n",
            "Story: 23001\n",
            "Story: 24001\n",
            "Story: 25001\n",
            "Story: 26001\n",
            "Story: 27001\n",
            "Story: 28001\n",
            "Story: 29001\n",
            "Story: 30001\n",
            "Story: 31001\n",
            "Story: 32001\n",
            "Story: 33001\n",
            "Story: 34001\n",
            "Story: 35001\n",
            "Story: 36001\n",
            "Story: 37001\n",
            "Story: 38001\n",
            "Story: 39001\n",
            "Story: 40001\n",
            "Story: 41001\n",
            "Story: 42001\n",
            "Story: 43001\n",
            "Story: 44001\n",
            "Story: 45001\n",
            "Story: 46001\n",
            "Story: 47001\n",
            "Story: 48001\n",
            "Story: 49001\n",
            "Story: 50001\n",
            "Story: 51001\n",
            "Story: 52001\n",
            "Story: 53001\n",
            "Story: 54001\n",
            "Story: 55001\n",
            "Story: 56001\n",
            "Story: 57001\n",
            "Story: 58001\n",
            "Story: 59001\n",
            "Story: 60001\n",
            "Story: 61001\n",
            "Story: 62001\n",
            "Story: 63001\n",
            "Story: 64001\n",
            "Story: 65001\n",
            "Story: 66001\n",
            "Story: 67001\n",
            "Story: 68001\n",
            "Story: 69001\n",
            "Story: 70001\n",
            "Story: 71001\n",
            "Story: 72001\n",
            "Story: 73001\n",
            "Story: 74001\n",
            "Story: 75001\n",
            "Story: 76001\n",
            "Story: 77001\n",
            "Story: 78001\n",
            "Story: 79001\n",
            "Story: 80001\n",
            "Story: 81001\n",
            "Story: 82001\n",
            "Story: 83001\n",
            "Story: 84001\n",
            "Story: 85001\n",
            "Story: 86001\n",
            "Story: 87001\n",
            "Story: 88001\n",
            "Story: 89001\n",
            "Story: 90001\n",
            "Story: 91001\n",
            "Story: 92001\n",
            "2.6288276489717095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "236860"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoDb54NlFyyk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6af2baa6-487a-4937-d7e0-b5fde121e309"
      },
      "source": [
        "np.mean(flat_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10749471120951053"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3crO4NpjSUZV",
        "outputId": "4bfe737b-c3e1-4683-abb6-08b0e96c1321"
      },
      "source": [
        "r_highs = 0\n",
        "r_scores = [ [] for x in range(len(all_highlights)) ]\n",
        "rouge = Rouge()\n",
        "\n",
        "for story in range(len(all_highlights)):\n",
        "    if story % 5000 == 1:\n",
        "        print(\"Story:\", story)\n",
        "    r_highs += len(all_highlights[story]) - 1\n",
        "    r_hlsc = []\n",
        "    for hl in range(0, len(all_highlights[story]) - 1):\n",
        "        r_hlsc.append(rouge.get_scores(all_highlights[story][-1],\n",
        "                                     all_highlights[story][hl])[0]['rouge-1']['f'])\n",
        "    r_scores[story] = r_hlsc\n",
        "#    scores[story] = max(hlsc)\n",
        "\n",
        "print(highs / 92579)\n",
        "r_flat_scores = [ x for nums in r_scores for x in nums ]\n",
        "len(r_flat_scores)\n",
        "# len(scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Story: 1\n",
            "Story: 5001\n",
            "Story: 10001\n",
            "Story: 15001\n",
            "Story: 20001\n",
            "Story: 25001\n",
            "Story: 30001\n",
            "Story: 35001\n",
            "Story: 40001\n",
            "Story: 45001\n",
            "Story: 50001\n",
            "Story: 55001\n",
            "Story: 60001\n",
            "Story: 65001\n",
            "Story: 70001\n",
            "Story: 75001\n",
            "Story: 80001\n",
            "Story: 85001\n",
            "Story: 90001\n",
            "2.6288276489717095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "236860"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgY1qd2QTTJL",
        "outputId": "c3ac081c-6320-41ae-efa8-90222681cb8d"
      },
      "source": [
        "print(r_highs / 92579)\n",
        "print(highs / 92579)\n",
        "np.mean(r_flat_scores)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5584635824539044\n",
            "2.5584635824539044\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1019264738939428"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A21kNDsNdG8"
      },
      "source": [
        "T5 paper: The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence.\n",
        "\n",
        "Overall, our encoder-decoder Transformer implementation closely follows its originally proposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of “blocks”, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent. We use a simplified version of layer normalization where\n",
        "the activations are only rescaled and no additive bias is applied. After layer normalization, a residual skip connection (He et al., 2016) adds each subcomponent’s input to its output.\n",
        "Dropout (Srivastava et al., 2014) is applied within the feed-forward network, on the skip connection, on the attention weights, and at the input and output of the entire stack. The decoder is similar in structure to the encoder except that it includes a standard attention mechanism after each self-attention layer that attends to the output of the encoder. The self-attention mechanism in the decoder also uses a form of autoregressive or causal selfattention,\n",
        "which only allows the model to attend to past outputs. The output of the final\n",
        "decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix. All attention mechanisms in the Transformer are split up into independent “heads” whose outputs are concatenated before being further processed.\n",
        "\n",
        "In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers\n",
        "can build a sensitivity to larger offsets by combining local information from previous layers.\n",
        "\n",
        "we use a combination of model and data parallelism and train models on “slices” of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with\n",
        "supporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014).\n",
        "\n",
        "The CNN/Daily Mail (Hermann et al., 2015) data set was introduced as a question answering task but was adapted for text summarization by Nallapati et al. (2016); we use the non-anonymized version from See et al. (2017) as an abstractive summarization task. SQuAD (Rajpurkar et al., 2016) is a common question- answering benchmark. In our experiments, the model is fed the question and its context and asked to generate the answer token-by-token.\n",
        "\n",
        "Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices.\n",
        "\n",
        "Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a “BERTBASE” (Devlin et al., 2018) stack. Specifically, both the encoder and decoder consist of 12 blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of dff = 3072 followed by a ReLU nonlinearity and another dense layer. The “key” and “value” matrices of all attention mechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12 heads. All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total, this results in a model with about 220 million parameters. This is roughly twice the number of parameters of BERTBASE since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied in the model.\n",
        "\n",
        "we use a vocabulary of 32,000 wordpieces.\n",
        "\n",
        "As a cheaper alternative, we train our baseline model 10 times from scratch (i.e. with different random initializations and data set shuffling) and assume that the variance over these runs of the base model also applies to each experimental variant. We don’t expect most of the changes we make to have a dramatic effect on the inter-run variance, so this should provide a reasonable indication of the significance of different changes.\n",
        "\n",
        "For CNN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under the heading “CNNDM”.\n",
        "\n",
        "[regarding baselines, pre-training is marginal for very large datasets but our CNN dataset is not very large]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_bp4bs2U3Hx"
      },
      "source": [
        "# I. Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "gGQVSUNdTyeS",
        "outputId": "42c6e580-9f74-433f-924c-8d4f62bf8b36"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import collections\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, T5ForConditionalGeneration, TFT5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from datasets import Dataset\n",
        "from rouge import Rouge\n",
        "import nltk.data\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c48cba76cb07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTFT5ForConditionalGeneration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpairwise\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcosine_similarity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytPQG3MWTyeS"
      },
      "source": [
        "#### Some functions for convenience later"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YL0PFgATyeT"
      },
      "source": [
        "def cos_sims(out_sent, ref_sents):\n",
        "    \"\"\"gets cosine similarities for an output sentence with respect to the\n",
        "    highlight sentences. Returns the sum of values.\"\"\"\n",
        "    \n",
        "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
        "    \n",
        "    # get sentence level vectors with tf-idf\n",
        "    tfidf = vect.fit_transform([out_sent] + ref_sents)\n",
        "    \n",
        "    # get similarity matrix\n",
        "    similarity_mat = tfidf * tfidf.T\n",
        "    \n",
        "    # only values comparing \"out_sent\" with each sent in \"ref_sents\"\n",
        "    return similarity_mat.toarray()[:1,1:][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jKClW3Ow0_x",
        "outputId": "c47dc4b8-7723-4ed9-9ffd-040e18efc290"
      },
      "source": [
        "# move up one directory (models will be stored here)\n",
        "os.chdir(TOPDIR)\n",
        "print(os.getcwd())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/cnn_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcflvDOBTyeW"
      },
      "source": [
        "# II. Models\n",
        "\n",
        "## A. Pre-trained\n",
        "\n",
        "Load a Pre-trained T5 Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeYsKoVsTyeU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213,
          "referenced_widgets": [
            "1b8bdb3da74f498aa3d75a07b6b5ed35",
            "6ce74e6c51034f2bbc96d064af49b2f9",
            "78f9ec46abc14cd8bc5a7bd9bfe44662",
            "4512c744eecd4f65bd34660b4f7018fc",
            "b8e1e2bd34764eb08508221925de1165",
            "ce2b37fe08bf42f6b7e2139063d8589b",
            "ba3551d4da8c4e12950bd323e612f762",
            "cd2280f5dabb4490bc6736309dc55a9b",
            "0f5fb7672bf448d6a59b370e7ccb09a9",
            "eb6612edd211431db061341d320f8a09",
            "4b6649230f8d4a4796120bf7f88e9cde",
            "01a6350104b94e67b4929d4ec6e8aa39",
            "a0ba1b222a314d2eaa8f27e04a6695da",
            "aaa3dbbaac1649e4983b0c3c1435eb9f",
            "e512f16f86684ac3ad0432cf2c0ed265",
            "ef076476e9d742908f9193ee0addc3ec",
            "8df7bf0ba3b548868d635360ae00cfe1",
            "b8c136b620974a6dae0fcbb7fe7b1a3c",
            "f762948f2928495db2cb282a913bf1cc",
            "fec4e736c1ad4bf0b40f558069f01f08",
            "738c13e75d7a46db883dd1df532412a5",
            "4c80bfe584564b639739a61f19b0dcca",
            "03c7b733910d4d00bad88120fcbe45ec",
            "4c40ee9549cb41bba5c7236499d02fae",
            "1aa1fec87b9f420bbbb795d9e2b19008",
            "d8e3fabed2b64ce7abc48ebacb817328",
            "5c2d3b96c74242fea9258ea95e02e4a3",
            "9b4a3f2b85a1493da081136d4083457e",
            "5c8823c249b44b898d4431069af0d364",
            "d9dd2214a0464d249f73b6b2abf84f6c",
            "676b238e9c304c73bfe01bbb532aade4",
            "8eb838639c0f4e39bc6829f55b7181d2"
          ]
        },
        "outputId": "357c8908-b0a4-439c-a488-9a3caab9c445"
      },
      "source": [
        "# pretrained t5 model\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
        "\n",
        "# t5 tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('t5-small')\n",
        "\n",
        "def tokenize(batch):\n",
        "    \"\"\"Applies tokenizer to a whole dataset at once. Input is a dataset with raw text data, \n",
        "    and output is a dataset with tokenized data\"\"\"\n",
        "    \n",
        "    tokenized_input = tokenizer(batch['source'], padding='max_length', truncation=True)\n",
        "    tokenized_label = tokenizer(batch['target'], padding='max_length', truncation=True)\n",
        "    tokenized_input['labels'] = tokenized_label['input_ids']\n",
        "    return tokenized_input"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b8bdb3da74f498aa3d75a07b6b5ed35",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1197.0, style=ProgressStyle(description…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f5fb7672bf448d6a59b370e7ccb09a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=242065649.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8df7bf0ba3b548868d635360ae00cfe1",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=791656.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1aa1fec87b9f420bbbb795d9e2b19008",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1389353.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWcyAfO_TyeX"
      },
      "source": [
        "### Let's get some baseline loss values\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXVfbiIoTyeX",
        "outputId": "65e5e577-549a-48ea-be97-d07f8642ccdf"
      },
      "source": [
        "# check some stories for pre-training loss\n",
        "for i in range(5):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    # tokenize output and labels\n",
        "    input_ids = tokenizer('summarize: ' + story, return_tensors='pt').input_ids\n",
        "    labels = tokenizer(highlights[0], return_tensors='pt').input_ids\n",
        "    \n",
        "    # compute loss (this returns an array of things)\n",
        "    loss = model(input_ids=input_ids, labels=labels).loss\n",
        "    \n",
        "    # print loss (sum of array values above)\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (604 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor(1.6187, grad_fn=<NllLossBackward>)\n",
            "tensor(3.1245, grad_fn=<NllLossBackward>)\n",
            "tensor(4.4393, grad_fn=<NllLossBackward>)\n",
            "tensor(4.4850, grad_fn=<NllLossBackward>)\n",
            "tensor(4.3491, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYAflvOpbNGg"
      },
      "source": [
        "## A. Multisentence Baseline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KrQiiO17Yvgs"
      },
      "source": [
        "### Data Formatting\n",
        "\n",
        "Right now, we have a text file for each story. T5 requires a single matrix (a dataset object is perfect) with source/target columns. For this model, the target is now a paragraph composed of the joined highlights for a given story."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUhkrbAObO1D",
        "outputId": "618f5842-1d7b-47e4-e5f7-ef770a594db3"
      },
      "source": [
        "# collect train data in the format required for this next model\n",
        "source_text_train = []\n",
        "target_text_train = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(train_files)):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    # story, highlights = format_story(train_files[i])\n",
        "    story = all_stories.loc[train_files[i],'story']\n",
        "    highlights = all_stories.loc[train_files[i],'highlights']\n",
        "    \n",
        "    \n",
        "    # format data as story/joined highlights pairs\n",
        "    source_text_train.append(story)\n",
        "    target_text_train.append('. '.join(highlights)+'.')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "print('')\n",
        "\n",
        "train_df = pd.DataFrame(list(zip(source_text_train, target_text_train)),columns =['source', 'target'])\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.03051092227300008 minutes\n",
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 89904\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zs1nFVMpcTWD",
        "outputId": "c1dfdf8e-e3c8-49c7-9ff0-cc982fea13dc"
      },
      "source": [
        "# collect val data in the format required for this next model\n",
        "source_text_val = []\n",
        "target_text_val = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(valid_files)):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/joined highlights pairs\n",
        "    source_text_val.append(story)\n",
        "    target_text_val.append('. '.join(highlights)+'.')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format as a dataset\n",
        "val_df=pd.DataFrame(list(zip(source_text_val,target_text_val)),columns=['source','target'])\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.0004948814709981283 minutes\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 1182\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C6p_65OygGDE",
        "outputId": "b59c1f78-8627-4221-c7eb-eba8991ab38d"
      },
      "source": [
        "print('='*60+'\\nEXAMPLE INPUT TEXT\\n'+'='*60)\n",
        "print(source_text_train[0])\n",
        "print('='*60+'\\nEXAMPLE TARGET TEXT\\n'+'='*60)\n",
        "print(target_text_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXAMPLE INPUT TEXT\n",
            "============================================================\n",
            "It 's official : U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria .\n",
            "\n",
            "Obama sent a letter to the heads of the House and Senate on Saturday night , hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons .\n",
            "\n",
            "The proposed legislation from Obama asks Congress to approve the use of military force `` to deter , disrupt , prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction . ''\n",
            "\n",
            "It 's a step that is set to turn an international crisis into a fierce domestic political battle .\n",
            "\n",
            "There are key questions looming over the debate : What did U.N. weapons inspectors find in Syria ? What happens if Congress votes no ? And how will the Syrian government react ?\n",
            "\n",
            "In a televised address from the White House Rose Garden earlier Saturday , the president said he would take his case to Congress , not because he has to -- but because he wants to .\n",
            "\n",
            "`` While I believe I have the authority to carry out this military action without specific congressional authorization , I know that the country will be stronger if we take this course , and our actions will be even more effective , '' he said . `` We should have this debate , because the issues are too big for business as usual . ''\n",
            "\n",
            "Obama said top congressional leaders had agreed to schedule a debate when the body returns to Washington on September 9 . The Senate Foreign Relations Committee will hold a hearing over the matter on Tuesday , Sen. Robert Menendez said .\n",
            "\n",
            "Transcript : Read Obama 's full remarks\n",
            "\n",
            "Syrian crisis : Latest developments\n",
            "\n",
            "U.N. inspectors leave Syria\n",
            "\n",
            "Obama 's remarks came shortly after U.N. inspectors left Syria , carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb .\n",
            "\n",
            "`` The aim of the game here , the mandate , is very clear -- and that is to ascertain whether chemical weapons were used -- and not by whom , '' U.N. spokesman Martin Nesirky told reporters on Saturday .\n",
            "\n",
            "But who used the weapons in the reported toxic gas attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisis .\n",
            "\n",
            "Top U.S. officials have said there 's no doubt that the Syrian government was behind it , while Syrian officials have denied responsibility and blamed jihadists fighting with the rebels .\n",
            "\n",
            "British and U.S. intelligence reports say the attack involved chemical weapons , but U.N. officials have stressed the importance of waiting for an official report from inspectors .\n",
            "\n",
            "The inspectors will share their findings with U.N. Secretary-General Ban Ki-moon Ban , who has said he wants to wait until the U.N. team 's final report is completed before presenting it to the U.N. Security Council .\n",
            "\n",
            "The Organization for the Prohibition of Chemical Weapons , which nine of the inspectors belong to , said Saturday that it could take up to three weeks to analyze the evidence they collected .\n",
            "\n",
            "`` It needs time to be able to analyze the information and the samples , '' Nesirky said .\n",
            "\n",
            "He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria , and that `` a military solution is not an option . ''\n",
            "\n",
            "Bergen : Syria is a problem from hell for the U.S.\n",
            "\n",
            "Obama : ` This menace must be confronted '\n",
            "\n",
            "Obama 's senior advisers have debated the next steps to take , and the president 's comments Saturday came amid mounting political pressure over the situation in Syria . Some U.S. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire .\n",
            "\n",
            "Some global leaders have expressed support , but the British Parliament 's vote against military action earlier this week was a blow to Obama 's hopes of getting strong backing from key NATO allies .\n",
            "\n",
            "On Saturday , Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad . Any military attack would not be open-ended or include U.S. ground forces , he said .\n",
            "\n",
            "Syria 's alleged use of chemical weapons earlier this month `` is an assault on human dignity , '' the president said .\n",
            "\n",
            "A failure to respond with force , Obama argued , `` could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm . In a world with many dangers , this menace must be confronted . ''\n",
            "\n",
            "Syria missile strike : What would happen next ?\n",
            "\n",
            "Map : U.S. and allied assets around Syria\n",
            "\n",
            "Obama decision came Friday night\n",
            "\n",
            "On Friday night , the president made a last-minute decision to consult lawmakers .\n",
            "\n",
            "What will happen if they vote no ?\n",
            "\n",
            "It 's unclear . A senior administration official told CNN that Obama has the authority to act without Congress -- even if Congress rejects his request for authorization to use force .\n",
            "\n",
            "Obama on Saturday continued to shore up support for a strike on the al-Assad government .\n",
            "\n",
            "He spoke by phone with French President Francois Hollande before his Rose Garden speech .\n",
            "\n",
            "`` The two leaders agreed that the international community must deliver a resolute message to the Assad regime -- and others who would consider using chemical weapons -- that these crimes are unacceptable and those who violate this international norm will be held accountable by the world , '' the White House said .\n",
            "\n",
            "Meanwhile , as uncertainty loomed over how Congress would weigh in , U.S. military officials said they remained at the ready .\n",
            "\n",
            "5 key assertions : U.S. intelligence report on Syria\n",
            "\n",
            "Syria : Who wants what after chemical weapons horror\n",
            "\n",
            "Reactions mixed to Obama 's speech\n",
            "\n",
            "A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama 's announcement .\n",
            "\n",
            "`` Our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way , '' said spokesman Louay Safi . `` So we are quite concerned . ''\n",
            "\n",
            "Some members of Congress applauded Obama 's decision .\n",
            "\n",
            "House Speaker John Boehner , Majority Leader Eric Cantor , Majority Whip Kevin McCarthy and Conference Chair Cathy McMorris Rodgers issued a statement Saturday praising the president .\n",
            "\n",
            "`` Under the Constitution , the responsibility to declare war lies with Congress , '' the Republican lawmakers said . `` We are glad the president is seeking authorization for any military action in Syria in response to serious , substantive questions being raised . ''\n",
            "\n",
            "More than 160 legislators , including 63 of Obama 's fellow Democrats , had signed letters calling for either a vote or at least a `` full debate '' before any U.S. action .\n",
            "\n",
            "British Prime Minister David Cameron , whose own attempt to get lawmakers in his country to support military action in Syria failed earlier this week , responded to Obama 's speech in a Twitter post Saturday .\n",
            "\n",
            "`` I understand and support Barack Obama 's position on Syria , '' Cameron said .\n",
            "\n",
            "An influential lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theory .\n",
            "\n",
            "`` The main reason Obama is turning to the Congress : the military operation did not get enough support either in the world , among allies of the US or in the United States itself , '' Alexei Pushkov , chairman of the international-affairs committee of the Russian State Duma , said in a Twitter post .\n",
            "\n",
            "In the United States , scattered groups of anti-war protesters around the country took to the streets Saturday .\n",
            "\n",
            "`` Like many other Americans ... we 're just tired of the United States getting involved and invading and bombing other countries , '' said Robin Rosecrans , who was among hundreds at a Los Angeles demonstration .\n",
            "\n",
            "What do Syria 's neighbors think ?\n",
            "\n",
            "Why Russia , China , Iran stand by Assad\n",
            "\n",
            "Syria 's government unfazed\n",
            "\n",
            "After Obama 's speech , a military and political analyst on Syrian state TV said Obama is `` embarrassed '' that Russia opposes military action against Syria , is `` crying for help '' for someone to come to his rescue and is facing two defeats -- on the political and military levels .\n",
            "\n",
            "Syria 's prime minister appeared unfazed by the saber-rattling .\n",
            "\n",
            "`` The Syrian Army 's status is on maximum readiness and fingers are on the trigger to confront all challenges , '' Wael Nader al-Halqi said during a meeting with a delegation of Syrian expatriates from Italy , according to a banner on Syria State TV that was broadcast prior to Obama 's address .\n",
            "\n",
            "An anchor on Syrian state television said Obama `` appeared to be preparing for an aggression on Syria based on repeated lies . ''\n",
            "\n",
            "A top Syrian diplomat told the state television network that Obama was facing pressure to take military action from Israel , Turkey , some Arabs and right-wing extremists in the United States .\n",
            "\n",
            "`` I think he has done well by doing what Cameron did in terms of taking the issue to Parliament , '' said Bashar Jaafari , Syria 's ambassador to the United Nations .\n",
            "\n",
            "Both Obama and Cameron , he said , `` climbed to the top of the tree and do n't know how to get down . ''\n",
            "\n",
            "The Syrian government has denied that it used chemical weapons in the August 21 attack , saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it .\n",
            "\n",
            "British intelligence had put the number of people killed in the attack at more than 350 .\n",
            "\n",
            "On Saturday , Obama said `` all told , well over 1,000 people were murdered . '' U.S. Secretary of State John Kerry on Friday cited a death toll of 1,429 , more than 400 of them children . No explanation was offered for the discrepancy .\n",
            "\n",
            "Iran : U.S. military action in Syria would spark ` disaster '\n",
            "\n",
            "Opinion : Why strikes in Syria are a bad idea\n",
            "============================================================\n",
            "EXAMPLE TARGET TEXT\n",
            "============================================================\n",
            "Syrian official : Obama climbed to the top of the tree , `` does n't know how to get down ''. Obama sends a letter to the heads of the House and Senate. Obama to seek congressional approval on military action against Syria. Aim is to determine whether CW were used , not by whom , says U.N. spokesman.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvWftXbleU1R"
      },
      "source": [
        "### Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "74bb764298514884a191443ebfc30063",
            "aeb0142ba7b446d2a41fea1f21f35624",
            "abccfc6eaed0425fafc498a2155873ec",
            "b16cb2c81f9d45b0b81d8941c20e2f0a",
            "b98dfdabb5b7446bbfb80d2978115293",
            "14a3011766a34df68a641d358d17a6af",
            "8c823d11603a4a6cae24f4270899c8cb",
            "c022cfa60e614df3b4d3f0a27f8c8677"
          ]
        },
        "id": "u4J0g8cLeUEa",
        "outputId": "b17ef7fd-ae1a-4fc4-c12a-2f3789027b6a"
      },
      "source": [
        "train_tokenized = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
        "val_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "print(val_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74bb764298514884a191443ebfc30063",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=176.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNgbsYggenT_"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_MsiwL9qeo3D",
        "outputId": "46584558-b662-49ba-a2ae-820829d1c1c0"
      },
      "source": [
        "output_dir = 'multisentence_model'\n",
        "\n",
        "# training arguments to feed to Trainer object\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = 'baseline_model', # trained model will be saved here\n",
        "    num_train_epochs = 2, # number of times each story will be touched\n",
        "    per_device_train_batch_size = 8, # number of examples per batch\n",
        "    per_device_eval_batch_size = 8, # number of examples per batch\n",
        "    eval_accumulation_steps = 1,\n",
        "    prediction_loss_only = True,\n",
        "    learning_rate = 0.001,\n",
        "    evaluation_strategy = 'steps',\n",
        "    save_steps = 10,\n",
        "    save_total_limit = 1,\n",
        "    remove_unused_columns = True,\n",
        "    run_name = 'run_name',\n",
        "    logging_steps = 500, # print loss after this many steps\n",
        "    eval_steps = 500, # calculate loss after this many steps\n",
        "    logging_first_step = False,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"loss\", \n",
        "    greater_is_better = False\n",
        ")\n",
        "\n",
        "# create Trainer to feed the train/dev data\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_tokenized,\n",
        "    eval_dataset = val_tokenized\n",
        ")\n",
        "\n",
        "# train the model and save it to our directory\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir + '/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='22476' max='22476' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [22476/22476 2:53:50, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.294500</td>\n",
              "      <td>0.222355</td>\n",
              "      <td>21.765000</td>\n",
              "      <td>54.307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.296300</td>\n",
              "      <td>0.223391</td>\n",
              "      <td>21.768200</td>\n",
              "      <td>54.299000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.293100</td>\n",
              "      <td>0.222032</td>\n",
              "      <td>21.787000</td>\n",
              "      <td>54.253000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.290700</td>\n",
              "      <td>0.221767</td>\n",
              "      <td>21.848300</td>\n",
              "      <td>54.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.294800</td>\n",
              "      <td>0.221399</td>\n",
              "      <td>21.795700</td>\n",
              "      <td>54.231000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.292700</td>\n",
              "      <td>0.223435</td>\n",
              "      <td>21.798000</td>\n",
              "      <td>54.225000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.288700</td>\n",
              "      <td>0.222823</td>\n",
              "      <td>21.876000</td>\n",
              "      <td>54.032000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.289300</td>\n",
              "      <td>0.221721</td>\n",
              "      <td>21.853300</td>\n",
              "      <td>54.088000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.292700</td>\n",
              "      <td>0.221715</td>\n",
              "      <td>21.840000</td>\n",
              "      <td>54.121000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.291100</td>\n",
              "      <td>0.220129</td>\n",
              "      <td>21.855800</td>\n",
              "      <td>54.082000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.290000</td>\n",
              "      <td>0.220085</td>\n",
              "      <td>21.832100</td>\n",
              "      <td>54.140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.288300</td>\n",
              "      <td>0.219382</td>\n",
              "      <td>21.840300</td>\n",
              "      <td>54.120000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.287700</td>\n",
              "      <td>0.218265</td>\n",
              "      <td>21.829900</td>\n",
              "      <td>54.146000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.288400</td>\n",
              "      <td>0.218170</td>\n",
              "      <td>21.838100</td>\n",
              "      <td>54.125000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.285800</td>\n",
              "      <td>0.218255</td>\n",
              "      <td>21.856600</td>\n",
              "      <td>54.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.287600</td>\n",
              "      <td>0.216602</td>\n",
              "      <td>21.834500</td>\n",
              "      <td>54.134000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.287200</td>\n",
              "      <td>0.219338</td>\n",
              "      <td>21.787500</td>\n",
              "      <td>54.251000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.283900</td>\n",
              "      <td>0.215214</td>\n",
              "      <td>21.808300</td>\n",
              "      <td>54.200000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.287000</td>\n",
              "      <td>0.216560</td>\n",
              "      <td>21.829300</td>\n",
              "      <td>54.147000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.285600</td>\n",
              "      <td>0.216740</td>\n",
              "      <td>21.821700</td>\n",
              "      <td>54.166000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.285300</td>\n",
              "      <td>0.214625</td>\n",
              "      <td>21.805400</td>\n",
              "      <td>54.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.282100</td>\n",
              "      <td>0.215063</td>\n",
              "      <td>21.838600</td>\n",
              "      <td>54.124000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.276700</td>\n",
              "      <td>0.216237</td>\n",
              "      <td>21.872600</td>\n",
              "      <td>54.040000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.269400</td>\n",
              "      <td>0.215385</td>\n",
              "      <td>21.913000</td>\n",
              "      <td>53.940000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.272400</td>\n",
              "      <td>0.215260</td>\n",
              "      <td>21.841600</td>\n",
              "      <td>54.117000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.271100</td>\n",
              "      <td>0.215363</td>\n",
              "      <td>21.845500</td>\n",
              "      <td>54.107000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.268200</td>\n",
              "      <td>0.215469</td>\n",
              "      <td>21.824500</td>\n",
              "      <td>54.159000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.270100</td>\n",
              "      <td>0.214304</td>\n",
              "      <td>21.811000</td>\n",
              "      <td>54.193000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.269700</td>\n",
              "      <td>0.215407</td>\n",
              "      <td>21.856800</td>\n",
              "      <td>54.079000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.267900</td>\n",
              "      <td>0.215286</td>\n",
              "      <td>21.819400</td>\n",
              "      <td>54.172000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.267200</td>\n",
              "      <td>0.213857</td>\n",
              "      <td>21.869100</td>\n",
              "      <td>54.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.266900</td>\n",
              "      <td>0.213622</td>\n",
              "      <td>21.850700</td>\n",
              "      <td>54.094000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.262600</td>\n",
              "      <td>0.213729</td>\n",
              "      <td>21.870100</td>\n",
              "      <td>54.046000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.268700</td>\n",
              "      <td>0.213244</td>\n",
              "      <td>21.815400</td>\n",
              "      <td>54.182000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.265800</td>\n",
              "      <td>0.212899</td>\n",
              "      <td>21.760100</td>\n",
              "      <td>54.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.264200</td>\n",
              "      <td>0.212118</td>\n",
              "      <td>21.761200</td>\n",
              "      <td>54.317000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.268700</td>\n",
              "      <td>0.212721</td>\n",
              "      <td>21.931700</td>\n",
              "      <td>53.895000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.266600</td>\n",
              "      <td>0.212013</td>\n",
              "      <td>21.728800</td>\n",
              "      <td>54.398000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.266100</td>\n",
              "      <td>0.211546</td>\n",
              "      <td>21.780100</td>\n",
              "      <td>54.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.265400</td>\n",
              "      <td>0.211164</td>\n",
              "      <td>21.803800</td>\n",
              "      <td>54.211000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.265900</td>\n",
              "      <td>0.211059</td>\n",
              "      <td>21.833600</td>\n",
              "      <td>54.137000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.263000</td>\n",
              "      <td>0.211460</td>\n",
              "      <td>21.816300</td>\n",
              "      <td>54.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.265000</td>\n",
              "      <td>0.211262</td>\n",
              "      <td>21.779300</td>\n",
              "      <td>54.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.263600</td>\n",
              "      <td>0.211304</td>\n",
              "      <td>21.763100</td>\n",
              "      <td>54.312000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 173.88504872719446 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_OzCYwce80H"
      },
      "source": [
        "### Evaluate\n",
        "\n",
        "__ROUGE__"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoY5wLDyfJ7f",
        "outputId": "d722e3ac-5105-4e6b-9866-65eca083ffa8"
      },
      "source": [
        "# load our model\n",
        "multi_model = T5ForConditionalGeneration.from_pretrained('baseline_model/model')\n",
        "\n",
        "# for scoring outputs\n",
        "rouge = Rouge()\n",
        "\n",
        "scores = []\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "    # format the text to input/target format\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "\n",
        "    # encode the input\n",
        "    encoded = tokenizer.encode('summarize: '+story, return_tensors='pt')\n",
        "\n",
        "    # generate the output\n",
        "    output = multi_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
        "                             min_length=30, max_length=300, early_stopping=True)\n",
        "    summary = tokenizer.decode(output[0])\n",
        "\n",
        "    if 1%10==0:\n",
        "      print(i,'stories passed')\n",
        "    \n",
        "    # get ROUGE scores between output and highlights\n",
        "    score = rouge.get_scores(summary,'. '.join(highlights)+'.')[0]['rouge-1']['f']\n",
        "    \n",
        "    scores.append(score)\n",
        "\n",
        "print('ROUGE F1:',np.mean(scores))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 files passed\n",
            "1 files passed\n",
            "2 files passed\n",
            "3 files passed\n",
            "4 files passed\n",
            "5 files passed\n",
            "6 files passed\n",
            "7 files passed\n",
            "8 files passed\n",
            "9 files passed\n",
            "10 files passed\n",
            "11 files passed\n",
            "12 files passed\n",
            "13 files passed\n",
            "14 files passed\n",
            "15 files passed\n",
            "16 files passed\n",
            "17 files passed\n",
            "18 files passed\n",
            "19 files passed\n",
            "20 files passed\n",
            "21 files passed\n",
            "22 files passed\n",
            "23 files passed\n",
            "24 files passed\n",
            "25 files passed\n",
            "26 files passed\n",
            "27 files passed\n",
            "28 files passed\n",
            "29 files passed\n",
            "30 files passed\n",
            "31 files passed\n",
            "32 files passed\n",
            "33 files passed\n",
            "34 files passed\n",
            "35 files passed\n",
            "36 files passed\n",
            "37 files passed\n",
            "38 files passed\n",
            "39 files passed\n",
            "40 files passed\n",
            "41 files passed\n",
            "42 files passed\n",
            "43 files passed\n",
            "44 files passed\n",
            "45 files passed\n",
            "46 files passed\n",
            "47 files passed\n",
            "48 files passed\n",
            "49 files passed\n",
            "50 files passed\n",
            "51 files passed\n",
            "52 files passed\n",
            "53 files passed\n",
            "54 files passed\n",
            "55 files passed\n",
            "56 files passed\n",
            "57 files passed\n",
            "58 files passed\n",
            "59 files passed\n",
            "60 files passed\n",
            "61 files passed\n",
            "62 files passed\n",
            "63 files passed\n",
            "64 files passed\n",
            "65 files passed\n",
            "66 files passed\n",
            "67 files passed\n",
            "68 files passed\n",
            "69 files passed\n",
            "70 files passed\n",
            "71 files passed\n",
            "72 files passed\n",
            "73 files passed\n",
            "74 files passed\n",
            "75 files passed\n",
            "76 files passed\n",
            "77 files passed\n",
            "78 files passed\n",
            "79 files passed\n",
            "80 files passed\n",
            "81 files passed\n",
            "82 files passed\n",
            "83 files passed\n",
            "84 files passed\n",
            "85 files passed\n",
            "86 files passed\n",
            "87 files passed\n",
            "88 files passed\n",
            "89 files passed\n",
            "90 files passed\n",
            "91 files passed\n",
            "92 files passed\n",
            "93 files passed\n",
            "94 files passed\n",
            "95 files passed\n",
            "96 files passed\n",
            "97 files passed\n",
            "98 files passed\n",
            "99 files passed\n",
            "100 files passed\n",
            "101 files passed\n",
            "102 files passed\n",
            "103 files passed\n",
            "104 files passed\n",
            "105 files passed\n",
            "106 files passed\n",
            "107 files passed\n",
            "108 files passed\n",
            "109 files passed\n",
            "110 files passed\n",
            "111 files passed\n",
            "112 files passed\n",
            "113 files passed\n",
            "114 files passed\n",
            "115 files passed\n",
            "116 files passed\n",
            "117 files passed\n",
            "118 files passed\n",
            "119 files passed\n",
            "120 files passed\n",
            "121 files passed\n",
            "122 files passed\n",
            "123 files passed\n",
            "124 files passed\n",
            "125 files passed\n",
            "126 files passed\n",
            "127 files passed\n",
            "128 files passed\n",
            "129 files passed\n",
            "130 files passed\n",
            "131 files passed\n",
            "132 files passed\n",
            "133 files passed\n",
            "134 files passed\n",
            "135 files passed\n",
            "136 files passed\n",
            "137 files passed\n",
            "138 files passed\n",
            "139 files passed\n",
            "140 files passed\n",
            "141 files passed\n",
            "142 files passed\n",
            "143 files passed\n",
            "144 files passed\n",
            "145 files passed\n",
            "146 files passed\n",
            "147 files passed\n",
            "148 files passed\n",
            "149 files passed\n",
            "150 files passed\n",
            "151 files passed\n",
            "152 files passed\n",
            "153 files passed\n",
            "154 files passed\n",
            "155 files passed\n",
            "156 files passed\n",
            "157 files passed\n",
            "158 files passed\n",
            "159 files passed\n",
            "160 files passed\n",
            "161 files passed\n",
            "162 files passed\n",
            "163 files passed\n",
            "164 files passed\n",
            "165 files passed\n",
            "166 files passed\n",
            "167 files passed\n",
            "168 files passed\n",
            "169 files passed\n",
            "170 files passed\n",
            "171 files passed\n",
            "172 files passed\n",
            "173 files passed\n",
            "174 files passed\n",
            "175 files passed\n",
            "176 files passed\n",
            "177 files passed\n",
            "178 files passed\n",
            "179 files passed\n",
            "180 files passed\n",
            "181 files passed\n",
            "182 files passed\n",
            "183 files passed\n",
            "184 files passed\n",
            "185 files passed\n",
            "186 files passed\n",
            "187 files passed\n",
            "188 files passed\n",
            "189 files passed\n",
            "190 files passed\n",
            "191 files passed\n",
            "192 files passed\n",
            "193 files passed\n",
            "194 files passed\n",
            "195 files passed\n",
            "196 files passed\n",
            "197 files passed\n",
            "198 files passed\n",
            "199 files passed\n",
            "200 files passed\n",
            "201 files passed\n",
            "202 files passed\n",
            "203 files passed\n",
            "204 files passed\n",
            "205 files passed\n",
            "206 files passed\n",
            "207 files passed\n",
            "208 files passed\n",
            "209 files passed\n",
            "210 files passed\n",
            "211 files passed\n",
            "212 files passed\n",
            "213 files passed\n",
            "214 files passed\n",
            "215 files passed\n",
            "216 files passed\n",
            "217 files passed\n",
            "218 files passed\n",
            "219 files passed\n",
            "220 files passed\n",
            "221 files passed\n",
            "222 files passed\n",
            "223 files passed\n",
            "224 files passed\n",
            "225 files passed\n",
            "226 files passed\n",
            "227 files passed\n",
            "228 files passed\n",
            "229 files passed\n",
            "230 files passed\n",
            "231 files passed\n",
            "232 files passed\n",
            "233 files passed\n",
            "234 files passed\n",
            "235 files passed\n",
            "236 files passed\n",
            "237 files passed\n",
            "238 files passed\n",
            "239 files passed\n",
            "240 files passed\n",
            "241 files passed\n",
            "242 files passed\n",
            "243 files passed\n",
            "244 files passed\n",
            "245 files passed\n",
            "246 files passed\n",
            "247 files passed\n",
            "248 files passed\n",
            "249 files passed\n",
            "250 files passed\n",
            "251 files passed\n",
            "252 files passed\n",
            "253 files passed\n",
            "254 files passed\n",
            "255 files passed\n",
            "256 files passed\n",
            "257 files passed\n",
            "258 files passed\n",
            "259 files passed\n",
            "260 files passed\n",
            "261 files passed\n",
            "262 files passed\n",
            "263 files passed\n",
            "264 files passed\n",
            "265 files passed\n",
            "266 files passed\n",
            "267 files passed\n",
            "268 files passed\n",
            "269 files passed\n",
            "270 files passed\n",
            "271 files passed\n",
            "272 files passed\n",
            "273 files passed\n",
            "274 files passed\n",
            "275 files passed\n",
            "276 files passed\n",
            "277 files passed\n",
            "278 files passed\n",
            "279 files passed\n",
            "280 files passed\n",
            "281 files passed\n",
            "282 files passed\n",
            "283 files passed\n",
            "284 files passed\n",
            "285 files passed\n",
            "286 files passed\n",
            "287 files passed\n",
            "288 files passed\n",
            "289 files passed\n",
            "290 files passed\n",
            "291 files passed\n",
            "292 files passed\n",
            "293 files passed\n",
            "294 files passed\n",
            "295 files passed\n",
            "296 files passed\n",
            "297 files passed\n",
            "298 files passed\n",
            "299 files passed\n",
            "300 files passed\n",
            "301 files passed\n",
            "302 files passed\n",
            "303 files passed\n",
            "304 files passed\n",
            "305 files passed\n",
            "306 files passed\n",
            "307 files passed\n",
            "308 files passed\n",
            "309 files passed\n",
            "310 files passed\n",
            "311 files passed\n",
            "312 files passed\n",
            "313 files passed\n",
            "314 files passed\n",
            "315 files passed\n",
            "316 files passed\n",
            "317 files passed\n",
            "318 files passed\n",
            "319 files passed\n",
            "320 files passed\n",
            "321 files passed\n",
            "322 files passed\n",
            "323 files passed\n",
            "324 files passed\n",
            "325 files passed\n",
            "326 files passed\n",
            "327 files passed\n",
            "328 files passed\n",
            "329 files passed\n",
            "330 files passed\n",
            "331 files passed\n",
            "332 files passed\n",
            "333 files passed\n",
            "334 files passed\n",
            "335 files passed\n",
            "336 files passed\n",
            "337 files passed\n",
            "338 files passed\n",
            "339 files passed\n",
            "340 files passed\n",
            "341 files passed\n",
            "342 files passed\n",
            "343 files passed\n",
            "344 files passed\n",
            "345 files passed\n",
            "346 files passed\n",
            "347 files passed\n",
            "348 files passed\n",
            "349 files passed\n",
            "350 files passed\n",
            "351 files passed\n",
            "352 files passed\n",
            "353 files passed\n",
            "354 files passed\n",
            "355 files passed\n",
            "356 files passed\n",
            "357 files passed\n",
            "358 files passed\n",
            "359 files passed\n",
            "360 files passed\n",
            "361 files passed\n",
            "362 files passed\n",
            "363 files passed\n",
            "364 files passed\n",
            "365 files passed\n",
            "366 files passed\n",
            "367 files passed\n",
            "368 files passed\n",
            "369 files passed\n",
            "370 files passed\n",
            "371 files passed\n",
            "372 files passed\n",
            "373 files passed\n",
            "374 files passed\n",
            "375 files passed\n",
            "376 files passed\n",
            "377 files passed\n",
            "378 files passed\n",
            "379 files passed\n",
            "380 files passed\n",
            "381 files passed\n",
            "382 files passed\n",
            "383 files passed\n",
            "384 files passed\n",
            "385 files passed\n",
            "386 files passed\n",
            "387 files passed\n",
            "388 files passed\n",
            "389 files passed\n",
            "390 files passed\n",
            "391 files passed\n",
            "392 files passed\n",
            "393 files passed\n",
            "394 files passed\n",
            "395 files passed\n",
            "396 files passed\n",
            "397 files passed\n",
            "398 files passed\n",
            "399 files passed\n",
            "400 files passed\n",
            "401 files passed\n",
            "402 files passed\n",
            "403 files passed\n",
            "404 files passed\n",
            "405 files passed\n",
            "406 files passed\n",
            "407 files passed\n",
            "408 files passed\n",
            "409 files passed\n",
            "410 files passed\n",
            "411 files passed\n",
            "412 files passed\n",
            "413 files passed\n",
            "414 files passed\n",
            "415 files passed\n",
            "416 files passed\n",
            "417 files passed\n",
            "418 files passed\n",
            "419 files passed\n",
            "420 files passed\n",
            "421 files passed\n",
            "422 files passed\n",
            "423 files passed\n",
            "424 files passed\n",
            "425 files passed\n",
            "426 files passed\n",
            "427 files passed\n",
            "428 files passed\n",
            "429 files passed\n",
            "430 files passed\n",
            "431 files passed\n",
            "432 files passed\n",
            "433 files passed\n",
            "434 files passed\n",
            "435 files passed\n",
            "436 files passed\n",
            "437 files passed\n",
            "438 files passed\n",
            "439 files passed\n",
            "440 files passed\n",
            "441 files passed\n",
            "442 files passed\n",
            "443 files passed\n",
            "444 files passed\n",
            "445 files passed\n",
            "446 files passed\n",
            "447 files passed\n",
            "448 files passed\n",
            "449 files passed\n",
            "450 files passed\n",
            "451 files passed\n",
            "452 files passed\n",
            "453 files passed\n",
            "454 files passed\n",
            "455 files passed\n",
            "456 files passed\n",
            "457 files passed\n",
            "458 files passed\n",
            "459 files passed\n",
            "460 files passed\n",
            "461 files passed\n",
            "462 files passed\n",
            "463 files passed\n",
            "464 files passed\n",
            "465 files passed\n",
            "466 files passed\n",
            "467 files passed\n",
            "468 files passed\n",
            "469 files passed\n",
            "470 files passed\n",
            "471 files passed\n",
            "472 files passed\n",
            "473 files passed\n",
            "474 files passed\n",
            "475 files passed\n",
            "476 files passed\n",
            "477 files passed\n",
            "478 files passed\n",
            "479 files passed\n",
            "480 files passed\n",
            "481 files passed\n",
            "482 files passed\n",
            "483 files passed\n",
            "484 files passed\n",
            "485 files passed\n",
            "486 files passed\n",
            "487 files passed\n",
            "488 files passed\n",
            "489 files passed\n",
            "490 files passed\n",
            "491 files passed\n",
            "492 files passed\n",
            "493 files passed\n",
            "494 files passed\n",
            "495 files passed\n",
            "496 files passed\n",
            "497 files passed\n",
            "498 files passed\n",
            "499 files passed\n",
            "500 files passed\n",
            "501 files passed\n",
            "502 files passed\n",
            "503 files passed\n",
            "504 files passed\n",
            "505 files passed\n",
            "506 files passed\n",
            "507 files passed\n",
            "508 files passed\n",
            "509 files passed\n",
            "510 files passed\n",
            "511 files passed\n",
            "512 files passed\n",
            "513 files passed\n",
            "514 files passed\n",
            "515 files passed\n",
            "516 files passed\n",
            "517 files passed\n",
            "518 files passed\n",
            "519 files passed\n",
            "520 files passed\n",
            "521 files passed\n",
            "522 files passed\n",
            "523 files passed\n",
            "524 files passed\n",
            "525 files passed\n",
            "526 files passed\n",
            "527 files passed\n",
            "528 files passed\n",
            "529 files passed\n",
            "530 files passed\n",
            "531 files passed\n",
            "532 files passed\n",
            "533 files passed\n",
            "534 files passed\n",
            "535 files passed\n",
            "536 files passed\n",
            "537 files passed\n",
            "538 files passed\n",
            "539 files passed\n",
            "540 files passed\n",
            "541 files passed\n",
            "542 files passed\n",
            "543 files passed\n",
            "544 files passed\n",
            "545 files passed\n",
            "546 files passed\n",
            "547 files passed\n",
            "548 files passed\n",
            "549 files passed\n",
            "550 files passed\n",
            "551 files passed\n",
            "552 files passed\n",
            "553 files passed\n",
            "554 files passed\n",
            "555 files passed\n",
            "556 files passed\n",
            "557 files passed\n",
            "558 files passed\n",
            "559 files passed\n",
            "560 files passed\n",
            "561 files passed\n",
            "562 files passed\n",
            "563 files passed\n",
            "564 files passed\n",
            "565 files passed\n",
            "566 files passed\n",
            "567 files passed\n",
            "568 files passed\n",
            "569 files passed\n",
            "570 files passed\n",
            "571 files passed\n",
            "572 files passed\n",
            "573 files passed\n",
            "574 files passed\n",
            "575 files passed\n",
            "576 files passed\n",
            "577 files passed\n",
            "578 files passed\n",
            "579 files passed\n",
            "580 files passed\n",
            "581 files passed\n",
            "582 files passed\n",
            "583 files passed\n",
            "584 files passed\n",
            "585 files passed\n",
            "586 files passed\n",
            "587 files passed\n",
            "588 files passed\n",
            "589 files passed\n",
            "590 files passed\n",
            "591 files passed\n",
            "592 files passed\n",
            "593 files passed\n",
            "594 files passed\n",
            "595 files passed\n",
            "596 files passed\n",
            "597 files passed\n",
            "598 files passed\n",
            "599 files passed\n",
            "600 files passed\n",
            "601 files passed\n",
            "602 files passed\n",
            "603 files passed\n",
            "604 files passed\n",
            "605 files passed\n",
            "606 files passed\n",
            "607 files passed\n",
            "608 files passed\n",
            "609 files passed\n",
            "610 files passed\n",
            "611 files passed\n",
            "612 files passed\n",
            "613 files passed\n",
            "614 files passed\n",
            "615 files passed\n",
            "616 files passed\n",
            "617 files passed\n",
            "618 files passed\n",
            "619 files passed\n",
            "620 files passed\n",
            "621 files passed\n",
            "622 files passed\n",
            "623 files passed\n",
            "624 files passed\n",
            "625 files passed\n",
            "626 files passed\n",
            "627 files passed\n",
            "628 files passed\n",
            "629 files passed\n",
            "630 files passed\n",
            "631 files passed\n",
            "632 files passed\n",
            "633 files passed\n",
            "634 files passed\n",
            "635 files passed\n",
            "636 files passed\n",
            "637 files passed\n",
            "638 files passed\n",
            "639 files passed\n",
            "640 files passed\n",
            "641 files passed\n",
            "642 files passed\n",
            "643 files passed\n",
            "644 files passed\n",
            "645 files passed\n",
            "646 files passed\n",
            "647 files passed\n",
            "648 files passed\n",
            "649 files passed\n",
            "650 files passed\n",
            "651 files passed\n",
            "652 files passed\n",
            "653 files passed\n",
            "654 files passed\n",
            "655 files passed\n",
            "656 files passed\n",
            "657 files passed\n",
            "658 files passed\n",
            "659 files passed\n",
            "660 files passed\n",
            "661 files passed\n",
            "662 files passed\n",
            "663 files passed\n",
            "664 files passed\n",
            "665 files passed\n",
            "666 files passed\n",
            "667 files passed\n",
            "668 files passed\n",
            "669 files passed\n",
            "670 files passed\n",
            "671 files passed\n",
            "672 files passed\n",
            "673 files passed\n",
            "674 files passed\n",
            "675 files passed\n",
            "676 files passed\n",
            "677 files passed\n",
            "678 files passed\n",
            "679 files passed\n",
            "680 files passed\n",
            "681 files passed\n",
            "682 files passed\n",
            "683 files passed\n",
            "684 files passed\n",
            "685 files passed\n",
            "686 files passed\n",
            "687 files passed\n",
            "688 files passed\n",
            "689 files passed\n",
            "690 files passed\n",
            "691 files passed\n",
            "692 files passed\n",
            "693 files passed\n",
            "694 files passed\n",
            "695 files passed\n",
            "696 files passed\n",
            "697 files passed\n",
            "698 files passed\n",
            "699 files passed\n",
            "700 files passed\n",
            "701 files passed\n",
            "702 files passed\n",
            "703 files passed\n",
            "704 files passed\n",
            "705 files passed\n",
            "706 files passed\n",
            "707 files passed\n",
            "708 files passed\n",
            "709 files passed\n",
            "710 files passed\n",
            "711 files passed\n",
            "712 files passed\n",
            "713 files passed\n",
            "714 files passed\n",
            "715 files passed\n",
            "716 files passed\n",
            "717 files passed\n",
            "718 files passed\n",
            "719 files passed\n",
            "720 files passed\n",
            "721 files passed\n",
            "722 files passed\n",
            "723 files passed\n",
            "724 files passed\n",
            "725 files passed\n",
            "726 files passed\n",
            "727 files passed\n",
            "728 files passed\n",
            "729 files passed\n",
            "730 files passed\n",
            "731 files passed\n",
            "732 files passed\n",
            "733 files passed\n",
            "734 files passed\n",
            "735 files passed\n",
            "736 files passed\n",
            "737 files passed\n",
            "738 files passed\n",
            "739 files passed\n",
            "740 files passed\n",
            "741 files passed\n",
            "742 files passed\n",
            "743 files passed\n",
            "744 files passed\n",
            "745 files passed\n",
            "746 files passed\n",
            "747 files passed\n",
            "748 files passed\n",
            "749 files passed\n",
            "750 files passed\n",
            "751 files passed\n",
            "752 files passed\n",
            "753 files passed\n",
            "754 files passed\n",
            "755 files passed\n",
            "756 files passed\n",
            "757 files passed\n",
            "758 files passed\n",
            "759 files passed\n",
            "760 files passed\n",
            "761 files passed\n",
            "762 files passed\n",
            "763 files passed\n",
            "764 files passed\n",
            "765 files passed\n",
            "766 files passed\n",
            "767 files passed\n",
            "768 files passed\n",
            "769 files passed\n",
            "770 files passed\n",
            "771 files passed\n",
            "772 files passed\n",
            "773 files passed\n",
            "774 files passed\n",
            "775 files passed\n",
            "776 files passed\n",
            "777 files passed\n",
            "778 files passed\n",
            "779 files passed\n",
            "780 files passed\n",
            "781 files passed\n",
            "782 files passed\n",
            "783 files passed\n",
            "784 files passed\n",
            "785 files passed\n",
            "786 files passed\n",
            "787 files passed\n",
            "788 files passed\n",
            "789 files passed\n",
            "790 files passed\n",
            "791 files passed\n",
            "792 files passed\n",
            "793 files passed\n",
            "794 files passed\n",
            "795 files passed\n",
            "796 files passed\n",
            "797 files passed\n",
            "798 files passed\n",
            "799 files passed\n",
            "800 files passed\n",
            "801 files passed\n",
            "802 files passed\n",
            "803 files passed\n",
            "804 files passed\n",
            "805 files passed\n",
            "806 files passed\n",
            "807 files passed\n",
            "808 files passed\n",
            "809 files passed\n",
            "810 files passed\n",
            "811 files passed\n",
            "812 files passed\n",
            "813 files passed\n",
            "814 files passed\n",
            "815 files passed\n",
            "816 files passed\n",
            "817 files passed\n",
            "818 files passed\n",
            "819 files passed\n",
            "820 files passed\n",
            "821 files passed\n",
            "822 files passed\n",
            "823 files passed\n",
            "824 files passed\n",
            "825 files passed\n",
            "826 files passed\n",
            "827 files passed\n",
            "828 files passed\n",
            "829 files passed\n",
            "830 files passed\n",
            "831 files passed\n",
            "832 files passed\n",
            "833 files passed\n",
            "834 files passed\n",
            "835 files passed\n",
            "836 files passed\n",
            "837 files passed\n",
            "838 files passed\n",
            "839 files passed\n",
            "840 files passed\n",
            "841 files passed\n",
            "842 files passed\n",
            "843 files passed\n",
            "844 files passed\n",
            "845 files passed\n",
            "846 files passed\n",
            "847 files passed\n",
            "848 files passed\n",
            "849 files passed\n",
            "850 files passed\n",
            "851 files passed\n",
            "852 files passed\n",
            "853 files passed\n",
            "854 files passed\n",
            "855 files passed\n",
            "856 files passed\n",
            "857 files passed\n",
            "858 files passed\n",
            "859 files passed\n",
            "860 files passed\n",
            "861 files passed\n",
            "862 files passed\n",
            "863 files passed\n",
            "864 files passed\n",
            "865 files passed\n",
            "866 files passed\n",
            "867 files passed\n",
            "868 files passed\n",
            "869 files passed\n",
            "870 files passed\n",
            "871 files passed\n",
            "872 files passed\n",
            "873 files passed\n",
            "874 files passed\n",
            "875 files passed\n",
            "876 files passed\n",
            "877 files passed\n",
            "878 files passed\n",
            "879 files passed\n",
            "880 files passed\n",
            "881 files passed\n",
            "882 files passed\n",
            "883 files passed\n",
            "884 files passed\n",
            "885 files passed\n",
            "886 files passed\n",
            "887 files passed\n",
            "888 files passed\n",
            "889 files passed\n",
            "890 files passed\n",
            "891 files passed\n",
            "892 files passed\n",
            "893 files passed\n",
            "894 files passed\n",
            "895 files passed\n",
            "896 files passed\n",
            "897 files passed\n",
            "898 files passed\n",
            "899 files passed\n",
            "900 files passed\n",
            "901 files passed\n",
            "902 files passed\n",
            "903 files passed\n",
            "904 files passed\n",
            "905 files passed\n",
            "906 files passed\n",
            "907 files passed\n",
            "908 files passed\n",
            "909 files passed\n",
            "910 files passed\n",
            "911 files passed\n",
            "912 files passed\n",
            "913 files passed\n",
            "914 files passed\n",
            "915 files passed\n",
            "916 files passed\n",
            "917 files passed\n",
            "918 files passed\n",
            "919 files passed\n",
            "920 files passed\n",
            "921 files passed\n",
            "922 files passed\n",
            "923 files passed\n",
            "924 files passed\n",
            "925 files passed\n",
            "926 files passed\n",
            "927 files passed\n",
            "928 files passed\n",
            "929 files passed\n",
            "930 files passed\n",
            "931 files passed\n",
            "932 files passed\n",
            "933 files passed\n",
            "934 files passed\n",
            "935 files passed\n",
            "936 files passed\n",
            "937 files passed\n",
            "938 files passed\n",
            "939 files passed\n",
            "940 files passed\n",
            "941 files passed\n",
            "942 files passed\n",
            "943 files passed\n",
            "944 files passed\n",
            "945 files passed\n",
            "946 files passed\n",
            "947 files passed\n",
            "948 files passed\n",
            "949 files passed\n",
            "950 files passed\n",
            "951 files passed\n",
            "952 files passed\n",
            "953 files passed\n",
            "954 files passed\n",
            "955 files passed\n",
            "956 files passed\n",
            "957 files passed\n",
            "958 files passed\n",
            "959 files passed\n",
            "960 files passed\n",
            "961 files passed\n",
            "962 files passed\n",
            "963 files passed\n",
            "964 files passed\n",
            "965 files passed\n",
            "966 files passed\n",
            "967 files passed\n",
            "968 files passed\n",
            "969 files passed\n",
            "970 files passed\n",
            "971 files passed\n",
            "972 files passed\n",
            "973 files passed\n",
            "974 files passed\n",
            "975 files passed\n",
            "976 files passed\n",
            "977 files passed\n",
            "978 files passed\n",
            "979 files passed\n",
            "980 files passed\n",
            "981 files passed\n",
            "982 files passed\n",
            "983 files passed\n",
            "984 files passed\n",
            "985 files passed\n",
            "986 files passed\n",
            "987 files passed\n",
            "988 files passed\n",
            "989 files passed\n",
            "990 files passed\n",
            "991 files passed\n",
            "992 files passed\n",
            "993 files passed\n",
            "994 files passed\n",
            "995 files passed\n",
            "996 files passed\n",
            "997 files passed\n",
            "998 files passed\n",
            "999 files passed\n",
            "1000 files passed\n",
            "1001 files passed\n",
            "1002 files passed\n",
            "1003 files passed\n",
            "1004 files passed\n",
            "1005 files passed\n",
            "1006 files passed\n",
            "1007 files passed\n",
            "1008 files passed\n",
            "1009 files passed\n",
            "1010 files passed\n",
            "1011 files passed\n",
            "1012 files passed\n",
            "1013 files passed\n",
            "1014 files passed\n",
            "1015 files passed\n",
            "1016 files passed\n",
            "1017 files passed\n",
            "1018 files passed\n",
            "1019 files passed\n",
            "1020 files passed\n",
            "1021 files passed\n",
            "1022 files passed\n",
            "1023 files passed\n",
            "1024 files passed\n",
            "1025 files passed\n",
            "1026 files passed\n",
            "1027 files passed\n",
            "1028 files passed\n",
            "1029 files passed\n",
            "1030 files passed\n",
            "1031 files passed\n",
            "1032 files passed\n",
            "1033 files passed\n",
            "1034 files passed\n",
            "1035 files passed\n",
            "1036 files passed\n",
            "1037 files passed\n",
            "1038 files passed\n",
            "1039 files passed\n",
            "1040 files passed\n",
            "1041 files passed\n",
            "1042 files passed\n",
            "1043 files passed\n",
            "1044 files passed\n",
            "1045 files passed\n",
            "1046 files passed\n",
            "1047 files passed\n",
            "1048 files passed\n",
            "1049 files passed\n",
            "1050 files passed\n",
            "1051 files passed\n",
            "1052 files passed\n",
            "1053 files passed\n",
            "1054 files passed\n",
            "1055 files passed\n",
            "1056 files passed\n",
            "1057 files passed\n",
            "1058 files passed\n",
            "1059 files passed\n",
            "1060 files passed\n",
            "1061 files passed\n",
            "1062 files passed\n",
            "1063 files passed\n",
            "1064 files passed\n",
            "1065 files passed\n",
            "1066 files passed\n",
            "1067 files passed\n",
            "1068 files passed\n",
            "1069 files passed\n",
            "1070 files passed\n",
            "1071 files passed\n",
            "1072 files passed\n",
            "1073 files passed\n",
            "1074 files passed\n",
            "1075 files passed\n",
            "1076 files passed\n",
            "1077 files passed\n",
            "1078 files passed\n",
            "1079 files passed\n",
            "1080 files passed\n",
            "ROUGE F1: 0.25997761074902337\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3pSQwdSe4gD"
      },
      "source": [
        "### How did Training affect the loss?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeyPcY2SF6ZH"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "90b14e60c1654ea6a82cd668bd1cc05f",
            "565c00437e2a45e99cc7c38fec798b5c",
            "d732db6a8c7a45c68aafd83cea5ca4af",
            "1ec80fbb2a574130a9fd1aa65ee949c7",
            "d0c6a37cb9e242149b8b0d4ebec93b51",
            "f359f64d87c5449588074b973d10ee87",
            "b7d32588d9b94604a292506d6e234c5d",
            "07857130a0da4c1db2bf9c2dc998c966"
          ]
        },
        "id": "UztBOD4ivFy5",
        "outputId": "f932951e-a51d-41b7-eba2-f2197bec6dc4"
      },
      "source": [
        "# collect test data in the format required for this next model\n",
        "source_text_test = []\n",
        "target_text_test = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/joined highlights pairs\n",
        "    source_text_test.append(story)\n",
        "    target_text_test.append('. '.join(highlights)+'.')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format test data as dataset\n",
        "test_df = pd.DataFrame(list(zip(source_text_test, target_text_test)),columns =['source', 'target'])\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_tokenized = test_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "print(test_dataset)\n",
        "\n",
        "# evaluate the model on the test dataset\n",
        "eval_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=8,\n",
        "    # remove_unused_columns=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    output_dir = 'output'\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=multi_model, args=eval_args)\n",
        "\n",
        "trainer.evaluate(test_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.0005159616470336914 minutes\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 1081\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90b14e60c1654ea6a82cd668bd1cc05f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [136/136 00:21]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.20515315234661102,\n",
              " 'eval_mem_cpu_alloc_delta': 117921,\n",
              " 'eval_mem_cpu_peaked_delta': 170250,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 1279395840,\n",
              " 'eval_runtime': 21.2569,\n",
              " 'eval_samples_per_second': 50.854,\n",
              " 'init_mem_cpu_alloc_delta': 531146,\n",
              " 'init_mem_cpu_peaked_delta': 30967345,\n",
              " 'init_mem_gpu_alloc_delta': 0,\n",
              " 'init_mem_gpu_peaked_delta': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "8700de48429740eea4502fb55e85924c",
            "582ab86d7169421899cfc3f1acaec79a",
            "ddbbf86f6a1649d4afbfcf800606f21a",
            "4a79c4d44d514f78b5c928c01012fbd7",
            "a59e5decfa2c4b8eb22e0b1ebcbc9865",
            "3f900dd033cd46f8832446c88ffbfd2d",
            "2caf9621481c4e828fb0ac85b6f8bf7d",
            "d38c1f1f85fd48d5a55133de6aad06f0"
          ]
        },
        "id": "i-sT_ZtnAr8S",
        "outputId": "e1fe0955-d3ca-454e-c4ce-f0ede1f58ae9"
      },
      "source": [
        "multi_model = T5ForConditionalGeneration.from_pretrained('baseline_model/model')\n",
        "\n",
        "# collect test data in the format required for this next model\n",
        "source_text_test = []\n",
        "target_text_test = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/joined highlights pairs\n",
        "    source_text_test.append(story)\n",
        "    target_text_test.append('. '.join(highlights)+'.')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format test data as dataset\n",
        "test_df = pd.DataFrame(list(zip(source_text_test, target_text_test)),columns =['source', 'target'])\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_tokenized = test_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "print(test_dataset)\n",
        "\n",
        "# evaluate the model on the test dataset\n",
        "eval_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=8,\n",
        "    # remove_unused_columns=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    output_dir = 'output'\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=multi_model, args=eval_args)\n",
        "\n",
        "trainer.evaluate(test_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.0003548860549926758 minutes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8700de48429740eea4502fb55e85924c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 1081\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='136' max='136' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [136/136 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.29979559779167175,\n",
              " 'eval_mem_cpu_alloc_delta': -30633984,\n",
              " 'eval_mem_cpu_peaked_delta': 30633984,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 1279395840,\n",
              " 'eval_runtime': 21.0785,\n",
              " 'eval_samples_per_second': 51.285,\n",
              " 'init_mem_cpu_alloc_delta': -11374592,\n",
              " 'init_mem_cpu_peaked_delta': 11374592,\n",
              " 'init_mem_gpu_alloc_delta': 242026496,\n",
              " 'init_mem_gpu_peaked_delta': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAcNm9EABeHa"
      },
      "source": [
        "### Excluding Known Information with TF-IDF\n",
        "\n",
        "Let's trim the sentence most like a \"known highlight\". We can use TF-IDF to vectorize each sentence, and cmopare cosine similarities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnxluBQ8Bu7B"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# make up some similar sentences\n",
        "s0 = \"\"\"I am trying as best I can, using as many words as I can, to make this sentence as long as possible,\n",
        "        though it has little meaning.\"\"\"\n",
        "s1 = 'I am trying as best I can, using as many words as I can, to make this sentence as long as possible.'\n",
        "s2 = 'I am trying as best I can to make this sentence as long as possible.'\n",
        "s3 = 'I am trying to make this sentence as long as possible.'\n",
        "\n",
        "# add in a totally different sentence\n",
        "s4 = 'Liver tastes terrible.'\n",
        "\n",
        "def cos_sims(out_sent, ref_sents):\n",
        "    vect = TfidfVectorizer(min_df=1, stop_words=\"english\")                                                                                                                                                                                                   \n",
        "    tfidf = vect.fit_transform([out_sent] + ref_sents)  \n",
        "\n",
        "    similarity_mat = tfidf * tfidf.T\n",
        "\n",
        "    return similarity_mat.toarray()[:1,1:][0]\n",
        "\n",
        "similarities = cos_sims(s0,[s1,s2,s3,s4])\n",
        "\n",
        "print('similarity of s0 to:') \n",
        "for i in range(4):\n",
        "    print('s'+str(i+1),'=',similarities[i])\n",
        "    print(str(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvEwJQKyClEM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "b60cd830-05f2-4ee1-c6f8-041f04e6159b"
      },
      "source": [
        "# encode the summary\n",
        "encoded = tokenizer.encode('summarize: ' + story.replace('\\n',' '), return_tensors='tf')\n",
        "\n",
        "# decode\n",
        "output = multi_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
        "                         min_length=30, max_length=300, early_stopping=True)\n",
        "summary = tokenizer.decode(output[0])\n",
        "\n",
        "# split the summary into sentences\n",
        "splitter = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "scores = []\n",
        "for sent in summ_sentences:\n",
        "    score = cos_sims(sent,[highlights[0]])\n",
        "    scores.append(score)\n",
        "\n",
        "\n",
        "# output the sentence with the least similarity to highlights\n",
        "print('Known Info:')\n",
        "print(highlights[0])\n",
        "print('~'*60)\n",
        "print('Closest Match:')\n",
        "summ_sentences[scores.index(max(scores))]\n",
        "print('~'*60)\n",
        "print('Novel Info:')\n",
        "\n",
        "# remove the sentence most similar\n",
        "summ_sentences.pop(scores.index(max(scores)))\n",
        "print('. '.join(summ_sentences))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 512). Running this sequence through the model will result in indexing errors\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-1d37e6857376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# decode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m output = multi_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n\u001b[0m\u001b[1;32m      6\u001b[0m                          min_length=30, max_length=300, early_stopping=True)\n\u001b[1;32m      7\u001b[0m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'multi_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpahqwzwWLlZ"
      },
      "source": [
        "## B. Single Sentence Summaries\n",
        "\n",
        "This model uses single sentence summaries, more in line with the summarization benchmarking tokenization task often referenced in the literature."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV-Ge6KcYc1S"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56i0DpzpYdWc"
      },
      "source": [
        "### Data Formatting\n",
        "\n",
        "Right now, we have a text file for each story. T5 requires a single matrix (a dataset object is perfect) with source/target columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "id": "1eBVTuGBTyeW",
        "outputId": "a9680e0b-88af-449b-de6f-7c54f1b5e4c2"
      },
      "source": [
        "# collect train data from all_stories\n",
        "source_text_train = []\n",
        "target_text_train = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(train_files)):\n",
        "# for i in range(10):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[train_files[i],'story']\n",
        "    highlights = all_stories.loc[train_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/single highlight pairs\n",
        "    for j in range(len(highlights)):\n",
        "        source_text_train.append(story)\n",
        "        target_text_train.append(highlights[j])\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "print('')\n",
        "\n",
        "# format as a dataset\n",
        "train_df = pd.DataFrame(list(zip(source_text_train, target_text_train)),columns =['source', 'target'])\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.023931705951690675 minutes\n",
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 322487\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Syrian official : Obama climbed to the top of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Obama sends a letter to the heads of the House...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Obama to seek congressional approval on milita...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Aim is to determine whether CW were used , not...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-LRB- CNN -RRB- -- Usain Bolt rounded off the ...</td>\n",
              "      <td>Usain Bolt wins third gold of world championship</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              source                                             target\n",
              "0  It 's official : U.S. President Barack Obama w...  Syrian official : Obama climbed to the top of ...\n",
              "1  It 's official : U.S. President Barack Obama w...  Obama sends a letter to the heads of the House...\n",
              "2  It 's official : U.S. President Barack Obama w...  Obama to seek congressional approval on milita...\n",
              "3  It 's official : U.S. President Barack Obama w...  Aim is to determine whether CW were used , not...\n",
              "4  -LRB- CNN -RRB- -- Usain Bolt rounded off the ...   Usain Bolt wins third gold of world championship"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzksETe0TyeX",
        "outputId": "44d807ea-b316-4692-89b4-47fdb0b386f4"
      },
      "source": [
        "# collect val data from all_stories\n",
        "source_text_val = []\n",
        "target_text_val = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(valid_files)):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/single highlight pairs\n",
        "    for j in range(len(highlights)):\n",
        "        source_text_val.append(story)\n",
        "        target_text_val.append(highlights[j])\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "print('')\n",
        "\n",
        "# format as a dataset\n",
        "val_df = pd.DataFrame(list(zip(source_text_val, target_text_val)),columns =['source', 'target'])\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.00033032894134521484 minutes\n",
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 3054\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mHKP_OQgAFF",
        "outputId": "5c0bad96-5969-4b05-dde1-89d9d8ed5464"
      },
      "source": [
        "print('='*60+'\\nEXAMPLE INPUT TEXT\\n'+'='*60)\n",
        "print(source_text_train[0])\n",
        "print('='*60+'\\nEXAMPLE TARGET TEXT\\n'+'='*60)\n",
        "print(target_text_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXAMPLE INPUT TEXT\n",
            "============================================================\n",
            "It 's official : U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria .\n",
            "\n",
            "Obama sent a letter to the heads of the House and Senate on Saturday night , hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons .\n",
            "\n",
            "The proposed legislation from Obama asks Congress to approve the use of military force `` to deter , disrupt , prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction . ''\n",
            "\n",
            "It 's a step that is set to turn an international crisis into a fierce domestic political battle .\n",
            "\n",
            "There are key questions looming over the debate : What did U.N. weapons inspectors find in Syria ? What happens if Congress votes no ? And how will the Syrian government react ?\n",
            "\n",
            "In a televised address from the White House Rose Garden earlier Saturday , the president said he would take his case to Congress , not because he has to -- but because he wants to .\n",
            "\n",
            "`` While I believe I have the authority to carry out this military action without specific congressional authorization , I know that the country will be stronger if we take this course , and our actions will be even more effective , '' he said . `` We should have this debate , because the issues are too big for business as usual . ''\n",
            "\n",
            "Obama said top congressional leaders had agreed to schedule a debate when the body returns to Washington on September 9 . The Senate Foreign Relations Committee will hold a hearing over the matter on Tuesday , Sen. Robert Menendez said .\n",
            "\n",
            "Transcript : Read Obama 's full remarks\n",
            "\n",
            "Syrian crisis : Latest developments\n",
            "\n",
            "U.N. inspectors leave Syria\n",
            "\n",
            "Obama 's remarks came shortly after U.N. inspectors left Syria , carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb .\n",
            "\n",
            "`` The aim of the game here , the mandate , is very clear -- and that is to ascertain whether chemical weapons were used -- and not by whom , '' U.N. spokesman Martin Nesirky told reporters on Saturday .\n",
            "\n",
            "But who used the weapons in the reported toxic gas attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisis .\n",
            "\n",
            "Top U.S. officials have said there 's no doubt that the Syrian government was behind it , while Syrian officials have denied responsibility and blamed jihadists fighting with the rebels .\n",
            "\n",
            "British and U.S. intelligence reports say the attack involved chemical weapons , but U.N. officials have stressed the importance of waiting for an official report from inspectors .\n",
            "\n",
            "The inspectors will share their findings with U.N. Secretary-General Ban Ki-moon Ban , who has said he wants to wait until the U.N. team 's final report is completed before presenting it to the U.N. Security Council .\n",
            "\n",
            "The Organization for the Prohibition of Chemical Weapons , which nine of the inspectors belong to , said Saturday that it could take up to three weeks to analyze the evidence they collected .\n",
            "\n",
            "`` It needs time to be able to analyze the information and the samples , '' Nesirky said .\n",
            "\n",
            "He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria , and that `` a military solution is not an option . ''\n",
            "\n",
            "Bergen : Syria is a problem from hell for the U.S.\n",
            "\n",
            "Obama : ` This menace must be confronted '\n",
            "\n",
            "Obama 's senior advisers have debated the next steps to take , and the president 's comments Saturday came amid mounting political pressure over the situation in Syria . Some U.S. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire .\n",
            "\n",
            "Some global leaders have expressed support , but the British Parliament 's vote against military action earlier this week was a blow to Obama 's hopes of getting strong backing from key NATO allies .\n",
            "\n",
            "On Saturday , Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad . Any military attack would not be open-ended or include U.S. ground forces , he said .\n",
            "\n",
            "Syria 's alleged use of chemical weapons earlier this month `` is an assault on human dignity , '' the president said .\n",
            "\n",
            "A failure to respond with force , Obama argued , `` could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm . In a world with many dangers , this menace must be confronted . ''\n",
            "\n",
            "Syria missile strike : What would happen next ?\n",
            "\n",
            "Map : U.S. and allied assets around Syria\n",
            "\n",
            "Obama decision came Friday night\n",
            "\n",
            "On Friday night , the president made a last-minute decision to consult lawmakers .\n",
            "\n",
            "What will happen if they vote no ?\n",
            "\n",
            "It 's unclear . A senior administration official told CNN that Obama has the authority to act without Congress -- even if Congress rejects his request for authorization to use force .\n",
            "\n",
            "Obama on Saturday continued to shore up support for a strike on the al-Assad government .\n",
            "\n",
            "He spoke by phone with French President Francois Hollande before his Rose Garden speech .\n",
            "\n",
            "`` The two leaders agreed that the international community must deliver a resolute message to the Assad regime -- and others who would consider using chemical weapons -- that these crimes are unacceptable and those who violate this international norm will be held accountable by the world , '' the White House said .\n",
            "\n",
            "Meanwhile , as uncertainty loomed over how Congress would weigh in , U.S. military officials said they remained at the ready .\n",
            "\n",
            "5 key assertions : U.S. intelligence report on Syria\n",
            "\n",
            "Syria : Who wants what after chemical weapons horror\n",
            "\n",
            "Reactions mixed to Obama 's speech\n",
            "\n",
            "A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama 's announcement .\n",
            "\n",
            "`` Our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way , '' said spokesman Louay Safi . `` So we are quite concerned . ''\n",
            "\n",
            "Some members of Congress applauded Obama 's decision .\n",
            "\n",
            "House Speaker John Boehner , Majority Leader Eric Cantor , Majority Whip Kevin McCarthy and Conference Chair Cathy McMorris Rodgers issued a statement Saturday praising the president .\n",
            "\n",
            "`` Under the Constitution , the responsibility to declare war lies with Congress , '' the Republican lawmakers said . `` We are glad the president is seeking authorization for any military action in Syria in response to serious , substantive questions being raised . ''\n",
            "\n",
            "More than 160 legislators , including 63 of Obama 's fellow Democrats , had signed letters calling for either a vote or at least a `` full debate '' before any U.S. action .\n",
            "\n",
            "British Prime Minister David Cameron , whose own attempt to get lawmakers in his country to support military action in Syria failed earlier this week , responded to Obama 's speech in a Twitter post Saturday .\n",
            "\n",
            "`` I understand and support Barack Obama 's position on Syria , '' Cameron said .\n",
            "\n",
            "An influential lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theory .\n",
            "\n",
            "`` The main reason Obama is turning to the Congress : the military operation did not get enough support either in the world , among allies of the US or in the United States itself , '' Alexei Pushkov , chairman of the international-affairs committee of the Russian State Duma , said in a Twitter post .\n",
            "\n",
            "In the United States , scattered groups of anti-war protesters around the country took to the streets Saturday .\n",
            "\n",
            "`` Like many other Americans ... we 're just tired of the United States getting involved and invading and bombing other countries , '' said Robin Rosecrans , who was among hundreds at a Los Angeles demonstration .\n",
            "\n",
            "What do Syria 's neighbors think ?\n",
            "\n",
            "Why Russia , China , Iran stand by Assad\n",
            "\n",
            "Syria 's government unfazed\n",
            "\n",
            "After Obama 's speech , a military and political analyst on Syrian state TV said Obama is `` embarrassed '' that Russia opposes military action against Syria , is `` crying for help '' for someone to come to his rescue and is facing two defeats -- on the political and military levels .\n",
            "\n",
            "Syria 's prime minister appeared unfazed by the saber-rattling .\n",
            "\n",
            "`` The Syrian Army 's status is on maximum readiness and fingers are on the trigger to confront all challenges , '' Wael Nader al-Halqi said during a meeting with a delegation of Syrian expatriates from Italy , according to a banner on Syria State TV that was broadcast prior to Obama 's address .\n",
            "\n",
            "An anchor on Syrian state television said Obama `` appeared to be preparing for an aggression on Syria based on repeated lies . ''\n",
            "\n",
            "A top Syrian diplomat told the state television network that Obama was facing pressure to take military action from Israel , Turkey , some Arabs and right-wing extremists in the United States .\n",
            "\n",
            "`` I think he has done well by doing what Cameron did in terms of taking the issue to Parliament , '' said Bashar Jaafari , Syria 's ambassador to the United Nations .\n",
            "\n",
            "Both Obama and Cameron , he said , `` climbed to the top of the tree and do n't know how to get down . ''\n",
            "\n",
            "The Syrian government has denied that it used chemical weapons in the August 21 attack , saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it .\n",
            "\n",
            "British intelligence had put the number of people killed in the attack at more than 350 .\n",
            "\n",
            "On Saturday , Obama said `` all told , well over 1,000 people were murdered . '' U.S. Secretary of State John Kerry on Friday cited a death toll of 1,429 , more than 400 of them children . No explanation was offered for the discrepancy .\n",
            "\n",
            "Iran : U.S. military action in Syria would spark ` disaster '\n",
            "\n",
            "Opinion : Why strikes in Syria are a bad idea\n",
            "============================================================\n",
            "EXAMPLE TARGET TEXT\n",
            "============================================================\n",
            "Syrian official : Obama climbed to the top of the tree , `` does n't know how to get down ''\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IJ-eIANTyeY"
      },
      "source": [
        "### Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184,
          "referenced_widgets": [
            "3faf12ab539943dd8e42ae8d6f1a4ca0",
            "aa53fd1f681c42978e249b966458b519",
            "88bf736d7d4a43c2abb2aa5a4c61c81c",
            "8d2f7885ba3140db92da1a420e9ec0b9",
            "7896a5fd7e3245c299779687cab17b63",
            "791c5e88c5de41dfb208b43fffed3261",
            "e39465870be14e77b2ed7bc0a67ac27c",
            "c037331445ab4d0ba5691577c3652bb6",
            "7b3fb4faf9b14e1ba83d650c9a2319c2",
            "68a4146bcbce47eab18be5396bae28aa",
            "f07927274d154344888ad616ce1b1398",
            "e117d5bd058540f3ba9309ec28488957",
            "e99c399b408a4f7c9fe688ed12982b43",
            "10c832b8ccba473488855761cbe5c607",
            "26a870f9dd454dd192b06045d5b5de63",
            "39400e003b774206ad26c3b95fc6e116"
          ]
        },
        "id": "U39wiB1LTyeY",
        "outputId": "94de6f2f-4f11-4a1d-d60d-f8155ff99cef"
      },
      "source": [
        "train_tokenized = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
        "val_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "print(val_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3faf12ab539943dd8e42ae8d6f1a4ca0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=630.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b3fb4faf9b14e1ba83d650c9a2319c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset({\n",
            "    features: ['attention_mask', 'input_ids', 'labels', 'source', 'target'],\n",
            "    num_rows: 3054\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EboJySWyevTS"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RSzymHKgTyeZ",
        "outputId": "c1b92e2d-28ba-456c-ac82-35e97c69f881"
      },
      "source": [
        "output_dir = 'baseline_model'\n",
        "\n",
        "# training arguments to feed to Trainer object\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_dir, # trained model will be saved here\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size = 8, # number of examples per batch\n",
        "    per_device_eval_batch_size = 8, # number of examples per batch\n",
        "    eval_accumulation_steps = 1,\n",
        "    prediction_loss_only = True,\n",
        "    learning_rate = 0.001, # best learning rate based on our quick experiment\n",
        "    evaluation_strategy = 'steps',\n",
        "    save_steps = 10,\n",
        "    save_total_limit = 1,\n",
        "    remove_unused_columns = True,\n",
        "    run_name = 'run_name',\n",
        "    logging_steps = 500, # print loss after this many steps\n",
        "    eval_steps = 500, # calculate loss after this many steps\n",
        "    logging_first_step = False,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"loss\", \n",
        "    greater_is_better = False\n",
        ")\n",
        "\n",
        "# create Trainer to feed the train/dev data\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_tokenized,\n",
        "    eval_dataset = val_tokenized\n",
        ")\n",
        "\n",
        "# train the model and save it to our directory\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir + '/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='40311' max='40311' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40311/40311 5:57:10, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.171400</td>\n",
              "      <td>0.094636</td>\n",
              "      <td>56.218500</td>\n",
              "      <td>54.324000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.095400</td>\n",
              "      <td>0.094809</td>\n",
              "      <td>56.136900</td>\n",
              "      <td>54.403000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.095700</td>\n",
              "      <td>0.094786</td>\n",
              "      <td>56.127000</td>\n",
              "      <td>54.412000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.094900</td>\n",
              "      <td>0.093045</td>\n",
              "      <td>56.140200</td>\n",
              "      <td>54.399000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.094400</td>\n",
              "      <td>0.092938</td>\n",
              "      <td>56.114500</td>\n",
              "      <td>54.424000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.092300</td>\n",
              "      <td>0.092473</td>\n",
              "      <td>56.057400</td>\n",
              "      <td>54.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.094000</td>\n",
              "      <td>0.092874</td>\n",
              "      <td>56.106100</td>\n",
              "      <td>54.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.094000</td>\n",
              "      <td>0.092910</td>\n",
              "      <td>56.102700</td>\n",
              "      <td>54.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.094100</td>\n",
              "      <td>0.092156</td>\n",
              "      <td>56.100400</td>\n",
              "      <td>54.438000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.093100</td>\n",
              "      <td>0.092147</td>\n",
              "      <td>56.086700</td>\n",
              "      <td>54.451000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.092200</td>\n",
              "      <td>0.091985</td>\n",
              "      <td>56.103500</td>\n",
              "      <td>54.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.093700</td>\n",
              "      <td>0.091323</td>\n",
              "      <td>56.077100</td>\n",
              "      <td>54.461000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.092200</td>\n",
              "      <td>0.091614</td>\n",
              "      <td>56.072000</td>\n",
              "      <td>54.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.093200</td>\n",
              "      <td>0.091311</td>\n",
              "      <td>56.119700</td>\n",
              "      <td>54.419000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.091100</td>\n",
              "      <td>0.092597</td>\n",
              "      <td>56.142800</td>\n",
              "      <td>54.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.091500</td>\n",
              "      <td>0.090739</td>\n",
              "      <td>56.107400</td>\n",
              "      <td>54.431000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.093100</td>\n",
              "      <td>0.090770</td>\n",
              "      <td>56.096900</td>\n",
              "      <td>54.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.091300</td>\n",
              "      <td>0.090182</td>\n",
              "      <td>56.136800</td>\n",
              "      <td>54.403000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.091500</td>\n",
              "      <td>0.090399</td>\n",
              "      <td>56.091100</td>\n",
              "      <td>54.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.090114</td>\n",
              "      <td>56.087300</td>\n",
              "      <td>54.451000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.090212</td>\n",
              "      <td>56.106700</td>\n",
              "      <td>54.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.091200</td>\n",
              "      <td>0.090972</td>\n",
              "      <td>56.153800</td>\n",
              "      <td>54.386000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.089100</td>\n",
              "      <td>0.089738</td>\n",
              "      <td>56.138200</td>\n",
              "      <td>54.401000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.089800</td>\n",
              "      <td>0.089706</td>\n",
              "      <td>55.994400</td>\n",
              "      <td>54.541000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.088700</td>\n",
              "      <td>0.089648</td>\n",
              "      <td>56.204900</td>\n",
              "      <td>54.337000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.089600</td>\n",
              "      <td>0.089032</td>\n",
              "      <td>56.129100</td>\n",
              "      <td>54.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.088900</td>\n",
              "      <td>0.089022</td>\n",
              "      <td>56.202600</td>\n",
              "      <td>54.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.089400</td>\n",
              "      <td>0.089390</td>\n",
              "      <td>56.172200</td>\n",
              "      <td>54.369000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.089300</td>\n",
              "      <td>0.089156</td>\n",
              "      <td>56.156700</td>\n",
              "      <td>54.384000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.089000</td>\n",
              "      <td>0.088333</td>\n",
              "      <td>56.112000</td>\n",
              "      <td>54.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.088700</td>\n",
              "      <td>0.088240</td>\n",
              "      <td>56.055200</td>\n",
              "      <td>54.482000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.088200</td>\n",
              "      <td>0.088923</td>\n",
              "      <td>56.116200</td>\n",
              "      <td>54.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.089000</td>\n",
              "      <td>0.088175</td>\n",
              "      <td>56.078300</td>\n",
              "      <td>54.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.088300</td>\n",
              "      <td>0.088618</td>\n",
              "      <td>56.043700</td>\n",
              "      <td>54.493000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.088900</td>\n",
              "      <td>0.088320</td>\n",
              "      <td>56.074800</td>\n",
              "      <td>54.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.087700</td>\n",
              "      <td>0.087960</td>\n",
              "      <td>56.078300</td>\n",
              "      <td>54.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.088400</td>\n",
              "      <td>0.087466</td>\n",
              "      <td>56.086100</td>\n",
              "      <td>54.452000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.087300</td>\n",
              "      <td>0.087454</td>\n",
              "      <td>56.075300</td>\n",
              "      <td>54.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.087200</td>\n",
              "      <td>0.087131</td>\n",
              "      <td>56.073700</td>\n",
              "      <td>54.464000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.086300</td>\n",
              "      <td>0.087057</td>\n",
              "      <td>56.094800</td>\n",
              "      <td>54.444000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.087700</td>\n",
              "      <td>0.087135</td>\n",
              "      <td>56.093200</td>\n",
              "      <td>54.445000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.087700</td>\n",
              "      <td>0.086987</td>\n",
              "      <td>56.115100</td>\n",
              "      <td>54.424000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.087200</td>\n",
              "      <td>0.087306</td>\n",
              "      <td>56.105400</td>\n",
              "      <td>54.433000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.087200</td>\n",
              "      <td>0.086698</td>\n",
              "      <td>56.194700</td>\n",
              "      <td>54.347000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.086500</td>\n",
              "      <td>0.086710</td>\n",
              "      <td>56.229900</td>\n",
              "      <td>54.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.086500</td>\n",
              "      <td>0.086304</td>\n",
              "      <td>56.154700</td>\n",
              "      <td>54.385000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.084700</td>\n",
              "      <td>0.086379</td>\n",
              "      <td>56.143400</td>\n",
              "      <td>54.396000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.085600</td>\n",
              "      <td>0.086170</td>\n",
              "      <td>56.162600</td>\n",
              "      <td>54.378000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.084700</td>\n",
              "      <td>0.086265</td>\n",
              "      <td>56.164700</td>\n",
              "      <td>54.376000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.085800</td>\n",
              "      <td>0.085795</td>\n",
              "      <td>56.246400</td>\n",
              "      <td>54.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.086200</td>\n",
              "      <td>0.085523</td>\n",
              "      <td>56.158200</td>\n",
              "      <td>54.382000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.084600</td>\n",
              "      <td>0.085983</td>\n",
              "      <td>56.281800</td>\n",
              "      <td>54.263000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.084800</td>\n",
              "      <td>0.085518</td>\n",
              "      <td>56.369600</td>\n",
              "      <td>54.178000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.085000</td>\n",
              "      <td>0.085654</td>\n",
              "      <td>56.437400</td>\n",
              "      <td>54.113000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.086200</td>\n",
              "      <td>0.085335</td>\n",
              "      <td>56.377100</td>\n",
              "      <td>54.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>0.085391</td>\n",
              "      <td>56.180200</td>\n",
              "      <td>54.361000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.085200</td>\n",
              "      <td>0.085105</td>\n",
              "      <td>56.302700</td>\n",
              "      <td>54.242000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.084500</td>\n",
              "      <td>0.084720</td>\n",
              "      <td>56.323500</td>\n",
              "      <td>54.222000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>0.084977</td>\n",
              "      <td>56.451000</td>\n",
              "      <td>54.100000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.084900</td>\n",
              "      <td>0.084736</td>\n",
              "      <td>56.309000</td>\n",
              "      <td>54.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>0.084754</td>\n",
              "      <td>56.355700</td>\n",
              "      <td>54.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>0.084520</td>\n",
              "      <td>56.353300</td>\n",
              "      <td>54.194000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.084300</td>\n",
              "      <td>0.084415</td>\n",
              "      <td>56.367500</td>\n",
              "      <td>54.180000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.084300</td>\n",
              "      <td>0.084318</td>\n",
              "      <td>56.351200</td>\n",
              "      <td>54.196000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.084100</td>\n",
              "      <td>0.084315</td>\n",
              "      <td>56.346100</td>\n",
              "      <td>54.201000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.084500</td>\n",
              "      <td>0.084140</td>\n",
              "      <td>56.658600</td>\n",
              "      <td>53.902000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>0.083905</td>\n",
              "      <td>56.666500</td>\n",
              "      <td>53.894000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.081600</td>\n",
              "      <td>0.084351</td>\n",
              "      <td>56.661600</td>\n",
              "      <td>53.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.082700</td>\n",
              "      <td>0.083943</td>\n",
              "      <td>56.621600</td>\n",
              "      <td>53.937000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.082600</td>\n",
              "      <td>0.083819</td>\n",
              "      <td>56.690400</td>\n",
              "      <td>53.872000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.083000</td>\n",
              "      <td>0.083782</td>\n",
              "      <td>56.764100</td>\n",
              "      <td>53.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.081900</td>\n",
              "      <td>0.084010</td>\n",
              "      <td>56.659500</td>\n",
              "      <td>53.901000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.081100</td>\n",
              "      <td>0.083672</td>\n",
              "      <td>56.774600</td>\n",
              "      <td>53.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.083200</td>\n",
              "      <td>0.083599</td>\n",
              "      <td>56.807800</td>\n",
              "      <td>53.760000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.083400</td>\n",
              "      <td>0.083605</td>\n",
              "      <td>56.586600</td>\n",
              "      <td>53.970000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.081400</td>\n",
              "      <td>0.083464</td>\n",
              "      <td>56.675900</td>\n",
              "      <td>53.885000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.082400</td>\n",
              "      <td>0.083331</td>\n",
              "      <td>56.714000</td>\n",
              "      <td>53.849000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.082200</td>\n",
              "      <td>0.083240</td>\n",
              "      <td>56.596900</td>\n",
              "      <td>53.961000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.083100</td>\n",
              "      <td>0.083325</td>\n",
              "      <td>56.700800</td>\n",
              "      <td>53.862000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.082300</td>\n",
              "      <td>0.083265</td>\n",
              "      <td>56.725500</td>\n",
              "      <td>53.838000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 357.30914847056073 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKgAVXJpTyeZ"
      },
      "source": [
        "### How did Training affect the loss?\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7PQFk-NTyea",
        "outputId": "fcec70b7-8745-4f9b-e408-ef8b6666dbbd"
      },
      "source": [
        "### Check the same stories as before and pray the loss has decreased ###\n",
        "\n",
        "# load our model\n",
        "baseline_model = T5ForConditionalGeneration.from_pretrained('baseline_model/model')\n",
        "\n",
        "for i in range(5):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    # story, highlights = format_story(valid_files[i])\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    #train the model\n",
        "    input_ids = tokenizer('summarize: ' + story, return_tensors='pt').input_ids\n",
        "    labels = tokenizer(highlights[0], return_tensors='pt').input_ids\n",
        "    \n",
        "    # compute loss (this returns an array of things)\n",
        "    loss = baseline_model(input_ids=input_ids, labels=labels).loss\n",
        "    \n",
        "    # print loss (sum of array values above)\n",
        "    print(loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0.7232, grad_fn=<NllLossBackward>)\n",
            "tensor(1.5713, grad_fn=<NllLossBackward>)\n",
            "tensor(3.4547, grad_fn=<NllLossBackward>)\n",
            "tensor(2.0927, grad_fn=<NllLossBackward>)\n",
            "tensor(2.6209, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ee2Ymwa8Tyea"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMFO0XZgTyeb",
        "outputId": "0f39c7cf-4259-43da-9d68-d73a75b703c9"
      },
      "source": [
        "# for scoring outputs\n",
        "rouge = Rouge()\n",
        "\n",
        "# get list of training files\n",
        "# test_files = cnn_meta[cnn_meta['test']==1].reset_index()['story']\n",
        "\n",
        "mean_rouge = []\n",
        "max_rouge = []\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "    # format the text to input/target format\n",
        "    # story, highlights = format_story(test_files[i])\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "\n",
        "    # encode the input\n",
        "    encoded = tokenizer.encode('summarize: ' + story, return_tensors='pt')\n",
        "\n",
        "    # generate the output\n",
        "    output = baseline_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
        "                             min_length=30, max_length=300, early_stopping=True)\n",
        "    summary = tokenizer.decode(output[0])\n",
        "    # print(summary)\n",
        "    # print('')\n",
        "    \n",
        "    # get ROUGE scores between thoutput and highlights\n",
        "    scores = [rouge.get_scores(summary,highlight)[0]['rouge-1']['f'] for highlight in highlights]\n",
        "\n",
        "    mean_rouge.append(np.mean(scores))\n",
        "    max_rouge.append(max(scores))\n",
        "\n",
        "    if 1%1==0:\n",
        "      print(i+1,'stories passed.',(i+1)/len(test_files),'%')\n",
        "\n",
        "print('ROUGE F1 (mean):',np.mean(mean_rouge))\n",
        "print('ROUGE F1 (best):',np.mean(max_rouge))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 stories passed. 0.0009250693802035153 %\n",
            "2 stories passed. 0.0018501387604070306 %\n",
            "3 stories passed. 0.0027752081406105457 %\n",
            "4 stories passed. 0.0037002775208140612 %\n",
            "5 stories passed. 0.004625346901017576 %\n",
            "6 stories passed. 0.005550416281221091 %\n",
            "7 stories passed. 0.0064754856614246065 %\n",
            "8 stories passed. 0.0074005550416281225 %\n",
            "9 stories passed. 0.008325624421831638 %\n",
            "10 stories passed. 0.009250693802035153 %\n",
            "11 stories passed. 0.010175763182238668 %\n",
            "12 stories passed. 0.011100832562442183 %\n",
            "13 stories passed. 0.012025901942645698 %\n",
            "14 stories passed. 0.012950971322849213 %\n",
            "15 stories passed. 0.013876040703052728 %\n",
            "16 stories passed. 0.014801110083256245 %\n",
            "17 stories passed. 0.01572617946345976 %\n",
            "18 stories passed. 0.016651248843663275 %\n",
            "19 stories passed. 0.01757631822386679 %\n",
            "20 stories passed. 0.018501387604070305 %\n",
            "21 stories passed. 0.01942645698427382 %\n",
            "22 stories passed. 0.020351526364477335 %\n",
            "23 stories passed. 0.02127659574468085 %\n",
            "24 stories passed. 0.022201665124884366 %\n",
            "25 stories passed. 0.02312673450508788 %\n",
            "26 stories passed. 0.024051803885291396 %\n",
            "27 stories passed. 0.02497687326549491 %\n",
            "28 stories passed. 0.025901942645698426 %\n",
            "29 stories passed. 0.02682701202590194 %\n",
            "30 stories passed. 0.027752081406105456 %\n",
            "31 stories passed. 0.028677150786308975 %\n",
            "32 stories passed. 0.02960222016651249 %\n",
            "33 stories passed. 0.030527289546716005 %\n",
            "34 stories passed. 0.03145235892691952 %\n",
            "35 stories passed. 0.032377428307123035 %\n",
            "36 stories passed. 0.03330249768732655 %\n",
            "37 stories passed. 0.034227567067530065 %\n",
            "38 stories passed. 0.03515263644773358 %\n",
            "39 stories passed. 0.036077705827937095 %\n",
            "40 stories passed. 0.03700277520814061 %\n",
            "41 stories passed. 0.037927844588344126 %\n",
            "42 stories passed. 0.03885291396854764 %\n",
            "43 stories passed. 0.039777983348751156 %\n",
            "44 stories passed. 0.04070305272895467 %\n",
            "45 stories passed. 0.041628122109158186 %\n",
            "46 stories passed. 0.0425531914893617 %\n",
            "47 stories passed. 0.043478260869565216 %\n",
            "48 stories passed. 0.04440333024976873 %\n",
            "49 stories passed. 0.045328399629972246 %\n",
            "50 stories passed. 0.04625346901017576 %\n",
            "51 stories passed. 0.04717853839037928 %\n",
            "52 stories passed. 0.04810360777058279 %\n",
            "53 stories passed. 0.04902867715078631 %\n",
            "54 stories passed. 0.04995374653098982 %\n",
            "55 stories passed. 0.05087881591119334 %\n",
            "56 stories passed. 0.05180388529139685 %\n",
            "57 stories passed. 0.05272895467160037 %\n",
            "58 stories passed. 0.05365402405180388 %\n",
            "59 stories passed. 0.0545790934320074 %\n",
            "60 stories passed. 0.05550416281221091 %\n",
            "61 stories passed. 0.056429232192414434 %\n",
            "62 stories passed. 0.05735430157261795 %\n",
            "63 stories passed. 0.058279370952821465 %\n",
            "64 stories passed. 0.05920444033302498 %\n",
            "65 stories passed. 0.060129509713228495 %\n",
            "66 stories passed. 0.06105457909343201 %\n",
            "67 stories passed. 0.061979648473635525 %\n",
            "68 stories passed. 0.06290471785383904 %\n",
            "69 stories passed. 0.06382978723404255 %\n",
            "70 stories passed. 0.06475485661424607 %\n",
            "71 stories passed. 0.06567992599444958 %\n",
            "72 stories passed. 0.0666049953746531 %\n",
            "73 stories passed. 0.06753006475485661 %\n",
            "74 stories passed. 0.06845513413506013 %\n",
            "75 stories passed. 0.06938020351526364 %\n",
            "76 stories passed. 0.07030527289546716 %\n",
            "77 stories passed. 0.07123034227567067 %\n",
            "78 stories passed. 0.07215541165587419 %\n",
            "79 stories passed. 0.0730804810360777 %\n",
            "80 stories passed. 0.07400555041628122 %\n",
            "81 stories passed. 0.07493061979648474 %\n",
            "82 stories passed. 0.07585568917668825 %\n",
            "83 stories passed. 0.07678075855689177 %\n",
            "84 stories passed. 0.07770582793709528 %\n",
            "85 stories passed. 0.0786308973172988 %\n",
            "86 stories passed. 0.07955596669750231 %\n",
            "87 stories passed. 0.08048103607770583 %\n",
            "88 stories passed. 0.08140610545790934 %\n",
            "89 stories passed. 0.08233117483811286 %\n",
            "90 stories passed. 0.08325624421831637 %\n",
            "91 stories passed. 0.0841813135985199 %\n",
            "92 stories passed. 0.0851063829787234 %\n",
            "93 stories passed. 0.08603145235892692 %\n",
            "94 stories passed. 0.08695652173913043 %\n",
            "95 stories passed. 0.08788159111933395 %\n",
            "96 stories passed. 0.08880666049953746 %\n",
            "97 stories passed. 0.08973172987974098 %\n",
            "98 stories passed. 0.09065679925994449 %\n",
            "99 stories passed. 0.09158186864014801 %\n",
            "100 stories passed. 0.09250693802035152 %\n",
            "101 stories passed. 0.09343200740055504 %\n",
            "102 stories passed. 0.09435707678075855 %\n",
            "103 stories passed. 0.09528214616096208 %\n",
            "104 stories passed. 0.09620721554116558 %\n",
            "105 stories passed. 0.0971322849213691 %\n",
            "106 stories passed. 0.09805735430157261 %\n",
            "107 stories passed. 0.09898242368177614 %\n",
            "108 stories passed. 0.09990749306197964 %\n",
            "109 stories passed. 0.10083256244218317 %\n",
            "110 stories passed. 0.10175763182238667 %\n",
            "111 stories passed. 0.1026827012025902 %\n",
            "112 stories passed. 0.1036077705827937 %\n",
            "113 stories passed. 0.10453283996299723 %\n",
            "114 stories passed. 0.10545790934320073 %\n",
            "115 stories passed. 0.10638297872340426 %\n",
            "116 stories passed. 0.10730804810360776 %\n",
            "117 stories passed. 0.10823311748381129 %\n",
            "118 stories passed. 0.1091581868640148 %\n",
            "119 stories passed. 0.11008325624421832 %\n",
            "120 stories passed. 0.11100832562442182 %\n",
            "121 stories passed. 0.11193339500462535 %\n",
            "122 stories passed. 0.11285846438482887 %\n",
            "123 stories passed. 0.11378353376503238 %\n",
            "124 stories passed. 0.1147086031452359 %\n",
            "125 stories passed. 0.11563367252543941 %\n",
            "126 stories passed. 0.11655874190564293 %\n",
            "127 stories passed. 0.11748381128584644 %\n",
            "128 stories passed. 0.11840888066604996 %\n",
            "129 stories passed. 0.11933395004625347 %\n",
            "130 stories passed. 0.12025901942645699 %\n",
            "131 stories passed. 0.1211840888066605 %\n",
            "132 stories passed. 0.12210915818686402 %\n",
            "133 stories passed. 0.12303422756706753 %\n",
            "134 stories passed. 0.12395929694727105 %\n",
            "135 stories passed. 0.12488436632747456 %\n",
            "136 stories passed. 0.12580943570767808 %\n",
            "137 stories passed. 0.1267345050878816 %\n",
            "138 stories passed. 0.1276595744680851 %\n",
            "139 stories passed. 0.12858464384828863 %\n",
            "140 stories passed. 0.12950971322849214 %\n",
            "141 stories passed. 0.13043478260869565 %\n",
            "142 stories passed. 0.13135985198889916 %\n",
            "143 stories passed. 0.1322849213691027 %\n",
            "144 stories passed. 0.1332099907493062 %\n",
            "145 stories passed. 0.1341350601295097 %\n",
            "146 stories passed. 0.13506012950971322 %\n",
            "147 stories passed. 0.13598519888991675 %\n",
            "148 stories passed. 0.13691026827012026 %\n",
            "149 stories passed. 0.13783533765032377 %\n",
            "150 stories passed. 0.13876040703052728 %\n",
            "151 stories passed. 0.1396854764107308 %\n",
            "152 stories passed. 0.14061054579093432 %\n",
            "153 stories passed. 0.14153561517113783 %\n",
            "154 stories passed. 0.14246068455134134 %\n",
            "155 stories passed. 0.14338575393154487 %\n",
            "156 stories passed. 0.14431082331174838 %\n",
            "157 stories passed. 0.1452358926919519 %\n",
            "158 stories passed. 0.1461609620721554 %\n",
            "159 stories passed. 0.14708603145235893 %\n",
            "160 stories passed. 0.14801110083256244 %\n",
            "161 stories passed. 0.14893617021276595 %\n",
            "162 stories passed. 0.1498612395929695 %\n",
            "163 stories passed. 0.150786308973173 %\n",
            "164 stories passed. 0.1517113783533765 %\n",
            "165 stories passed. 0.15263644773358 %\n",
            "166 stories passed. 0.15356151711378355 %\n",
            "167 stories passed. 0.15448658649398705 %\n",
            "168 stories passed. 0.15541165587419056 %\n",
            "169 stories passed. 0.15633672525439407 %\n",
            "170 stories passed. 0.1572617946345976 %\n",
            "171 stories passed. 0.15818686401480112 %\n",
            "172 stories passed. 0.15911193339500462 %\n",
            "173 stories passed. 0.16003700277520813 %\n",
            "174 stories passed. 0.16096207215541167 %\n",
            "175 stories passed. 0.16188714153561518 %\n",
            "176 stories passed. 0.16281221091581868 %\n",
            "177 stories passed. 0.1637372802960222 %\n",
            "178 stories passed. 0.16466234967622573 %\n",
            "179 stories passed. 0.16558741905642924 %\n",
            "180 stories passed. 0.16651248843663274 %\n",
            "181 stories passed. 0.16743755781683625 %\n",
            "182 stories passed. 0.1683626271970398 %\n",
            "183 stories passed. 0.1692876965772433 %\n",
            "184 stories passed. 0.1702127659574468 %\n",
            "185 stories passed. 0.1711378353376503 %\n",
            "186 stories passed. 0.17206290471785385 %\n",
            "187 stories passed. 0.17298797409805736 %\n",
            "188 stories passed. 0.17391304347826086 %\n",
            "189 stories passed. 0.17483811285846437 %\n",
            "190 stories passed. 0.1757631822386679 %\n",
            "191 stories passed. 0.17668825161887142 %\n",
            "192 stories passed. 0.17761332099907493 %\n",
            "193 stories passed. 0.17853839037927843 %\n",
            "194 stories passed. 0.17946345975948197 %\n",
            "195 stories passed. 0.18038852913968548 %\n",
            "196 stories passed. 0.18131359851988899 %\n",
            "197 stories passed. 0.1822386679000925 %\n",
            "198 stories passed. 0.18316373728029603 %\n",
            "199 stories passed. 0.18408880666049954 %\n",
            "200 stories passed. 0.18501387604070305 %\n",
            "201 stories passed. 0.18593894542090658 %\n",
            "202 stories passed. 0.1868640148011101 %\n",
            "203 stories passed. 0.1877890841813136 %\n",
            "204 stories passed. 0.1887141535615171 %\n",
            "205 stories passed. 0.18963922294172064 %\n",
            "206 stories passed. 0.19056429232192415 %\n",
            "207 stories passed. 0.19148936170212766 %\n",
            "208 stories passed. 0.19241443108233117 %\n",
            "209 stories passed. 0.1933395004625347 %\n",
            "210 stories passed. 0.1942645698427382 %\n",
            "211 stories passed. 0.19518963922294172 %\n",
            "212 stories passed. 0.19611470860314523 %\n",
            "213 stories passed. 0.19703977798334876 %\n",
            "214 stories passed. 0.19796484736355227 %\n",
            "215 stories passed. 0.19888991674375578 %\n",
            "216 stories passed. 0.1998149861239593 %\n",
            "217 stories passed. 0.20074005550416282 %\n",
            "218 stories passed. 0.20166512488436633 %\n",
            "219 stories passed. 0.20259019426456984 %\n",
            "220 stories passed. 0.20351526364477335 %\n",
            "221 stories passed. 0.20444033302497688 %\n",
            "222 stories passed. 0.2053654024051804 %\n",
            "223 stories passed. 0.2062904717853839 %\n",
            "224 stories passed. 0.2072155411655874 %\n",
            "225 stories passed. 0.20814061054579094 %\n",
            "226 stories passed. 0.20906567992599445 %\n",
            "227 stories passed. 0.20999074930619796 %\n",
            "228 stories passed. 0.21091581868640147 %\n",
            "229 stories passed. 0.211840888066605 %\n",
            "230 stories passed. 0.2127659574468085 %\n",
            "231 stories passed. 0.21369102682701202 %\n",
            "232 stories passed. 0.21461609620721553 %\n",
            "233 stories passed. 0.21554116558741906 %\n",
            "234 stories passed. 0.21646623496762257 %\n",
            "235 stories passed. 0.21739130434782608 %\n",
            "236 stories passed. 0.2183163737280296 %\n",
            "237 stories passed. 0.21924144310823312 %\n",
            "238 stories passed. 0.22016651248843663 %\n",
            "239 stories passed. 0.22109158186864014 %\n",
            "240 stories passed. 0.22201665124884365 %\n",
            "241 stories passed. 0.22294172062904719 %\n",
            "242 stories passed. 0.2238667900092507 %\n",
            "243 stories passed. 0.2247918593894542 %\n",
            "244 stories passed. 0.22571692876965774 %\n",
            "245 stories passed. 0.22664199814986125 %\n",
            "246 stories passed. 0.22756706753006475 %\n",
            "247 stories passed. 0.22849213691026826 %\n",
            "248 stories passed. 0.2294172062904718 %\n",
            "249 stories passed. 0.2303422756706753 %\n",
            "250 stories passed. 0.23126734505087881 %\n",
            "251 stories passed. 0.23219241443108232 %\n",
            "252 stories passed. 0.23311748381128586 %\n",
            "253 stories passed. 0.23404255319148937 %\n",
            "254 stories passed. 0.23496762257169287 %\n",
            "255 stories passed. 0.23589269195189638 %\n",
            "256 stories passed. 0.23681776133209992 %\n",
            "257 stories passed. 0.23774283071230343 %\n",
            "258 stories passed. 0.23866790009250693 %\n",
            "259 stories passed. 0.23959296947271044 %\n",
            "260 stories passed. 0.24051803885291398 %\n",
            "261 stories passed. 0.2414431082331175 %\n",
            "262 stories passed. 0.242368177613321 %\n",
            "263 stories passed. 0.2432932469935245 %\n",
            "264 stories passed. 0.24421831637372804 %\n",
            "265 stories passed. 0.24514338575393155 %\n",
            "266 stories passed. 0.24606845513413506 %\n",
            "267 stories passed. 0.24699352451433856 %\n",
            "268 stories passed. 0.2479185938945421 %\n",
            "269 stories passed. 0.2488436632747456 %\n",
            "270 stories passed. 0.24976873265494912 %\n",
            "271 stories passed. 0.2506938020351526 %\n",
            "272 stories passed. 0.25161887141535616 %\n",
            "273 stories passed. 0.25254394079555964 %\n",
            "274 stories passed. 0.2534690101757632 %\n",
            "275 stories passed. 0.2543940795559667 %\n",
            "276 stories passed. 0.2553191489361702 %\n",
            "277 stories passed. 0.25624421831637373 %\n",
            "278 stories passed. 0.25716928769657726 %\n",
            "279 stories passed. 0.25809435707678074 %\n",
            "280 stories passed. 0.2590194264569843 %\n",
            "281 stories passed. 0.2599444958371878 %\n",
            "282 stories passed. 0.2608695652173913 %\n",
            "283 stories passed. 0.26179463459759483 %\n",
            "284 stories passed. 0.2627197039777983 %\n",
            "285 stories passed. 0.26364477335800185 %\n",
            "286 stories passed. 0.2645698427382054 %\n",
            "287 stories passed. 0.26549491211840887 %\n",
            "288 stories passed. 0.2664199814986124 %\n",
            "289 stories passed. 0.26734505087881594 %\n",
            "290 stories passed. 0.2682701202590194 %\n",
            "291 stories passed. 0.26919518963922295 %\n",
            "292 stories passed. 0.27012025901942643 %\n",
            "293 stories passed. 0.27104532839962997 %\n",
            "294 stories passed. 0.2719703977798335 %\n",
            "295 stories passed. 0.272895467160037 %\n",
            "296 stories passed. 0.2738205365402405 %\n",
            "297 stories passed. 0.27474560592044406 %\n",
            "298 stories passed. 0.27567067530064754 %\n",
            "299 stories passed. 0.2765957446808511 %\n",
            "300 stories passed. 0.27752081406105455 %\n",
            "301 stories passed. 0.2784458834412581 %\n",
            "302 stories passed. 0.2793709528214616 %\n",
            "303 stories passed. 0.2802960222016651 %\n",
            "304 stories passed. 0.28122109158186864 %\n",
            "305 stories passed. 0.2821461609620722 %\n",
            "306 stories passed. 0.28307123034227566 %\n",
            "307 stories passed. 0.2839962997224792 %\n",
            "308 stories passed. 0.2849213691026827 %\n",
            "309 stories passed. 0.2858464384828862 %\n",
            "310 stories passed. 0.28677150786308975 %\n",
            "311 stories passed. 0.28769657724329323 %\n",
            "312 stories passed. 0.28862164662349676 %\n",
            "313 stories passed. 0.2895467160037003 %\n",
            "314 stories passed. 0.2904717853839038 %\n",
            "315 stories passed. 0.2913968547641073 %\n",
            "316 stories passed. 0.2923219241443108 %\n",
            "317 stories passed. 0.29324699352451433 %\n",
            "318 stories passed. 0.29417206290471787 %\n",
            "319 stories passed. 0.29509713228492135 %\n",
            "320 stories passed. 0.2960222016651249 %\n",
            "321 stories passed. 0.2969472710453284 %\n",
            "322 stories passed. 0.2978723404255319 %\n",
            "323 stories passed. 0.29879740980573544 %\n",
            "324 stories passed. 0.299722479185939 %\n",
            "325 stories passed. 0.30064754856614245 %\n",
            "326 stories passed. 0.301572617946346 %\n",
            "327 stories passed. 0.30249768732654947 %\n",
            "328 stories passed. 0.303422756706753 %\n",
            "329 stories passed. 0.30434782608695654 %\n",
            "330 stories passed. 0.30527289546716 %\n",
            "331 stories passed. 0.30619796484736356 %\n",
            "332 stories passed. 0.3071230342275671 %\n",
            "333 stories passed. 0.3080481036077706 %\n",
            "334 stories passed. 0.3089731729879741 %\n",
            "335 stories passed. 0.3098982423681776 %\n",
            "336 stories passed. 0.3108233117483811 %\n",
            "337 stories passed. 0.31174838112858466 %\n",
            "338 stories passed. 0.31267345050878814 %\n",
            "339 stories passed. 0.3135985198889917 %\n",
            "340 stories passed. 0.3145235892691952 %\n",
            "341 stories passed. 0.3154486586493987 %\n",
            "342 stories passed. 0.31637372802960223 %\n",
            "343 stories passed. 0.3172987974098057 %\n",
            "344 stories passed. 0.31822386679000925 %\n",
            "345 stories passed. 0.3191489361702128 %\n",
            "346 stories passed. 0.32007400555041626 %\n",
            "347 stories passed. 0.3209990749306198 %\n",
            "348 stories passed. 0.32192414431082333 %\n",
            "349 stories passed. 0.3228492136910268 %\n",
            "350 stories passed. 0.32377428307123035 %\n",
            "351 stories passed. 0.32469935245143383 %\n",
            "352 stories passed. 0.32562442183163737 %\n",
            "353 stories passed. 0.3265494912118409 %\n",
            "354 stories passed. 0.3274745605920444 %\n",
            "355 stories passed. 0.3283996299722479 %\n",
            "356 stories passed. 0.32932469935245146 %\n",
            "357 stories passed. 0.33024976873265494 %\n",
            "358 stories passed. 0.33117483811285847 %\n",
            "359 stories passed. 0.33209990749306195 %\n",
            "360 stories passed. 0.3330249768732655 %\n",
            "361 stories passed. 0.333950046253469 %\n",
            "362 stories passed. 0.3348751156336725 %\n",
            "363 stories passed. 0.33580018501387604 %\n",
            "364 stories passed. 0.3367252543940796 %\n",
            "365 stories passed. 0.33765032377428306 %\n",
            "366 stories passed. 0.3385753931544866 %\n",
            "367 stories passed. 0.33950046253469013 %\n",
            "368 stories passed. 0.3404255319148936 %\n",
            "369 stories passed. 0.34135060129509714 %\n",
            "370 stories passed. 0.3422756706753006 %\n",
            "371 stories passed. 0.34320074005550416 %\n",
            "372 stories passed. 0.3441258094357077 %\n",
            "373 stories passed. 0.3450508788159112 %\n",
            "374 stories passed. 0.3459759481961147 %\n",
            "375 stories passed. 0.34690101757631825 %\n",
            "376 stories passed. 0.34782608695652173 %\n",
            "377 stories passed. 0.34875115633672527 %\n",
            "378 stories passed. 0.34967622571692875 %\n",
            "379 stories passed. 0.3506012950971323 %\n",
            "380 stories passed. 0.3515263644773358 %\n",
            "381 stories passed. 0.3524514338575393 %\n",
            "382 stories passed. 0.35337650323774283 %\n",
            "383 stories passed. 0.35430157261794637 %\n",
            "384 stories passed. 0.35522664199814985 %\n",
            "385 stories passed. 0.3561517113783534 %\n",
            "386 stories passed. 0.35707678075855687 %\n",
            "387 stories passed. 0.3580018501387604 %\n",
            "388 stories passed. 0.35892691951896394 %\n",
            "389 stories passed. 0.3598519888991674 %\n",
            "390 stories passed. 0.36077705827937095 %\n",
            "391 stories passed. 0.3617021276595745 %\n",
            "392 stories passed. 0.36262719703977797 %\n",
            "393 stories passed. 0.3635522664199815 %\n",
            "394 stories passed. 0.364477335800185 %\n",
            "395 stories passed. 0.3654024051803885 %\n",
            "396 stories passed. 0.36632747456059206 %\n",
            "397 stories passed. 0.36725254394079554 %\n",
            "398 stories passed. 0.3681776133209991 %\n",
            "399 stories passed. 0.3691026827012026 %\n",
            "400 stories passed. 0.3700277520814061 %\n",
            "401 stories passed. 0.3709528214616096 %\n",
            "402 stories passed. 0.37187789084181316 %\n",
            "403 stories passed. 0.37280296022201664 %\n",
            "404 stories passed. 0.3737280296022202 %\n",
            "405 stories passed. 0.37465309898242366 %\n",
            "406 stories passed. 0.3755781683626272 %\n",
            "407 stories passed. 0.37650323774283073 %\n",
            "408 stories passed. 0.3774283071230342 %\n",
            "409 stories passed. 0.37835337650323775 %\n",
            "410 stories passed. 0.3792784458834413 %\n",
            "411 stories passed. 0.38020351526364476 %\n",
            "412 stories passed. 0.3811285846438483 %\n",
            "413 stories passed. 0.3820536540240518 %\n",
            "414 stories passed. 0.3829787234042553 %\n",
            "415 stories passed. 0.38390379278445885 %\n",
            "416 stories passed. 0.38482886216466233 %\n",
            "417 stories passed. 0.38575393154486587 %\n",
            "418 stories passed. 0.3866790009250694 %\n",
            "419 stories passed. 0.3876040703052729 %\n",
            "420 stories passed. 0.3885291396854764 %\n",
            "421 stories passed. 0.3894542090656799 %\n",
            "422 stories passed. 0.39037927844588344 %\n",
            "423 stories passed. 0.391304347826087 %\n",
            "424 stories passed. 0.39222941720629045 %\n",
            "425 stories passed. 0.393154486586494 %\n",
            "426 stories passed. 0.3940795559666975 %\n",
            "427 stories passed. 0.395004625346901 %\n",
            "428 stories passed. 0.39592969472710454 %\n",
            "429 stories passed. 0.396854764107308 %\n",
            "430 stories passed. 0.39777983348751156 %\n",
            "431 stories passed. 0.3987049028677151 %\n",
            "432 stories passed. 0.3996299722479186 %\n",
            "433 stories passed. 0.4005550416281221 %\n",
            "434 stories passed. 0.40148011100832565 %\n",
            "435 stories passed. 0.4024051803885291 %\n",
            "436 stories passed. 0.40333024976873266 %\n",
            "437 stories passed. 0.40425531914893614 %\n",
            "438 stories passed. 0.4051803885291397 %\n",
            "439 stories passed. 0.4061054579093432 %\n",
            "440 stories passed. 0.4070305272895467 %\n",
            "441 stories passed. 0.40795559666975023 %\n",
            "442 stories passed. 0.40888066604995377 %\n",
            "443 stories passed. 0.40980573543015725 %\n",
            "444 stories passed. 0.4107308048103608 %\n",
            "445 stories passed. 0.4116558741905643 %\n",
            "446 stories passed. 0.4125809435707678 %\n",
            "447 stories passed. 0.41350601295097134 %\n",
            "448 stories passed. 0.4144310823311748 %\n",
            "449 stories passed. 0.41535615171137835 %\n",
            "450 stories passed. 0.4162812210915819 %\n",
            "451 stories passed. 0.41720629047178537 %\n",
            "452 stories passed. 0.4181313598519889 %\n",
            "453 stories passed. 0.41905642923219244 %\n",
            "454 stories passed. 0.4199814986123959 %\n",
            "455 stories passed. 0.42090656799259946 %\n",
            "456 stories passed. 0.42183163737280294 %\n",
            "457 stories passed. 0.4227567067530065 %\n",
            "458 stories passed. 0.42368177613321 %\n",
            "459 stories passed. 0.4246068455134135 %\n",
            "460 stories passed. 0.425531914893617 %\n",
            "461 stories passed. 0.42645698427382056 %\n",
            "462 stories passed. 0.42738205365402404 %\n",
            "463 stories passed. 0.4283071230342276 %\n",
            "464 stories passed. 0.42923219241443106 %\n",
            "465 stories passed. 0.4301572617946346 %\n",
            "466 stories passed. 0.43108233117483813 %\n",
            "467 stories passed. 0.4320074005550416 %\n",
            "468 stories passed. 0.43293246993524515 %\n",
            "469 stories passed. 0.4338575393154487 %\n",
            "470 stories passed. 0.43478260869565216 %\n",
            "471 stories passed. 0.4357076780758557 %\n",
            "472 stories passed. 0.4366327474560592 %\n",
            "473 stories passed. 0.4375578168362627 %\n",
            "474 stories passed. 0.43848288621646625 %\n",
            "475 stories passed. 0.43940795559666973 %\n",
            "476 stories passed. 0.44033302497687327 %\n",
            "477 stories passed. 0.4412580943570768 %\n",
            "478 stories passed. 0.4421831637372803 %\n",
            "479 stories passed. 0.4431082331174838 %\n",
            "480 stories passed. 0.4440333024976873 %\n",
            "481 stories passed. 0.44495837187789083 %\n",
            "482 stories passed. 0.44588344125809437 %\n",
            "483 stories passed. 0.44680851063829785 %\n",
            "484 stories passed. 0.4477335800185014 %\n",
            "485 stories passed. 0.4486586493987049 %\n",
            "486 stories passed. 0.4495837187789084 %\n",
            "487 stories passed. 0.45050878815911194 %\n",
            "488 stories passed. 0.4514338575393155 %\n",
            "489 stories passed. 0.45235892691951896 %\n",
            "490 stories passed. 0.4532839962997225 %\n",
            "491 stories passed. 0.45420906567992597 %\n",
            "492 stories passed. 0.4551341350601295 %\n",
            "493 stories passed. 0.45605920444033304 %\n",
            "494 stories passed. 0.4569842738205365 %\n",
            "495 stories passed. 0.45790934320074006 %\n",
            "496 stories passed. 0.4588344125809436 %\n",
            "497 stories passed. 0.4597594819611471 %\n",
            "498 stories passed. 0.4606845513413506 %\n",
            "499 stories passed. 0.4616096207215541 %\n",
            "500 stories passed. 0.46253469010175763 %\n",
            "501 stories passed. 0.46345975948196116 %\n",
            "502 stories passed. 0.46438482886216464 %\n",
            "503 stories passed. 0.4653098982423682 %\n",
            "504 stories passed. 0.4662349676225717 %\n",
            "505 stories passed. 0.4671600370027752 %\n",
            "506 stories passed. 0.46808510638297873 %\n",
            "507 stories passed. 0.4690101757631822 %\n",
            "508 stories passed. 0.46993524514338575 %\n",
            "509 stories passed. 0.4708603145235893 %\n",
            "510 stories passed. 0.47178538390379277 %\n",
            "511 stories passed. 0.4727104532839963 %\n",
            "512 stories passed. 0.47363552266419984 %\n",
            "513 stories passed. 0.4745605920444033 %\n",
            "514 stories passed. 0.47548566142460685 %\n",
            "515 stories passed. 0.47641073080481033 %\n",
            "516 stories passed. 0.47733580018501387 %\n",
            "517 stories passed. 0.4782608695652174 %\n",
            "518 stories passed. 0.4791859389454209 %\n",
            "519 stories passed. 0.4801110083256244 %\n",
            "520 stories passed. 0.48103607770582796 %\n",
            "521 stories passed. 0.48196114708603144 %\n",
            "522 stories passed. 0.482886216466235 %\n",
            "523 stories passed. 0.4838112858464385 %\n",
            "524 stories passed. 0.484736355226642 %\n",
            "525 stories passed. 0.4856614246068455 %\n",
            "526 stories passed. 0.486586493987049 %\n",
            "527 stories passed. 0.48751156336725254 %\n",
            "528 stories passed. 0.4884366327474561 %\n",
            "529 stories passed. 0.48936170212765956 %\n",
            "530 stories passed. 0.4902867715078631 %\n",
            "531 stories passed. 0.49121184088806663 %\n",
            "532 stories passed. 0.4921369102682701 %\n",
            "533 stories passed. 0.49306197964847365 %\n",
            "534 stories passed. 0.4939870490286771 %\n",
            "535 stories passed. 0.49491211840888066 %\n",
            "536 stories passed. 0.4958371877890842 %\n",
            "537 stories passed. 0.4967622571692877 %\n",
            "538 stories passed. 0.4976873265494912 %\n",
            "539 stories passed. 0.49861239592969475 %\n",
            "540 stories passed. 0.49953746530989823 %\n",
            "541 stories passed. 0.5004625346901017 %\n",
            "542 stories passed. 0.5013876040703052 %\n",
            "543 stories passed. 0.5023126734505088 %\n",
            "544 stories passed. 0.5032377428307123 %\n",
            "545 stories passed. 0.5041628122109159 %\n",
            "546 stories passed. 0.5050878815911193 %\n",
            "547 stories passed. 0.5060129509713228 %\n",
            "548 stories passed. 0.5069380203515264 %\n",
            "549 stories passed. 0.5078630897317299 %\n",
            "550 stories passed. 0.5087881591119334 %\n",
            "551 stories passed. 0.509713228492137 %\n",
            "552 stories passed. 0.5106382978723404 %\n",
            "553 stories passed. 0.5115633672525439 %\n",
            "554 stories passed. 0.5124884366327475 %\n",
            "555 stories passed. 0.513413506012951 %\n",
            "556 stories passed. 0.5143385753931545 %\n",
            "557 stories passed. 0.515263644773358 %\n",
            "558 stories passed. 0.5161887141535615 %\n",
            "559 stories passed. 0.517113783533765 %\n",
            "560 stories passed. 0.5180388529139686 %\n",
            "561 stories passed. 0.5189639222941721 %\n",
            "562 stories passed. 0.5198889916743756 %\n",
            "563 stories passed. 0.5208140610545791 %\n",
            "564 stories passed. 0.5217391304347826 %\n",
            "565 stories passed. 0.5226641998149861 %\n",
            "566 stories passed. 0.5235892691951897 %\n",
            "567 stories passed. 0.5245143385753932 %\n",
            "568 stories passed. 0.5254394079555966 %\n",
            "569 stories passed. 0.5263644773358002 %\n",
            "570 stories passed. 0.5272895467160037 %\n",
            "571 stories passed. 0.5282146160962072 %\n",
            "572 stories passed. 0.5291396854764108 %\n",
            "573 stories passed. 0.5300647548566142 %\n",
            "574 stories passed. 0.5309898242368177 %\n",
            "575 stories passed. 0.5319148936170213 %\n",
            "576 stories passed. 0.5328399629972248 %\n",
            "577 stories passed. 0.5337650323774283 %\n",
            "578 stories passed. 0.5346901017576319 %\n",
            "579 stories passed. 0.5356151711378353 %\n",
            "580 stories passed. 0.5365402405180388 %\n",
            "581 stories passed. 0.5374653098982424 %\n",
            "582 stories passed. 0.5383903792784459 %\n",
            "583 stories passed. 0.5393154486586494 %\n",
            "584 stories passed. 0.5402405180388529 %\n",
            "585 stories passed. 0.5411655874190564 %\n",
            "586 stories passed. 0.5420906567992599 %\n",
            "587 stories passed. 0.5430157261794635 %\n",
            "588 stories passed. 0.543940795559667 %\n",
            "589 stories passed. 0.5448658649398704 %\n",
            "590 stories passed. 0.545790934320074 %\n",
            "591 stories passed. 0.5467160037002775 %\n",
            "592 stories passed. 0.547641073080481 %\n",
            "593 stories passed. 0.5485661424606846 %\n",
            "594 stories passed. 0.5494912118408881 %\n",
            "595 stories passed. 0.5504162812210915 %\n",
            "596 stories passed. 0.5513413506012951 %\n",
            "597 stories passed. 0.5522664199814986 %\n",
            "598 stories passed. 0.5531914893617021 %\n",
            "599 stories passed. 0.5541165587419057 %\n",
            "600 stories passed. 0.5550416281221091 %\n",
            "601 stories passed. 0.5559666975023126 %\n",
            "602 stories passed. 0.5568917668825162 %\n",
            "603 stories passed. 0.5578168362627197 %\n",
            "604 stories passed. 0.5587419056429233 %\n",
            "605 stories passed. 0.5596669750231268 %\n",
            "606 stories passed. 0.5605920444033302 %\n",
            "607 stories passed. 0.5615171137835338 %\n",
            "608 stories passed. 0.5624421831637373 %\n",
            "609 stories passed. 0.5633672525439408 %\n",
            "610 stories passed. 0.5642923219241444 %\n",
            "611 stories passed. 0.5652173913043478 %\n",
            "612 stories passed. 0.5661424606845513 %\n",
            "613 stories passed. 0.5670675300647549 %\n",
            "614 stories passed. 0.5679925994449584 %\n",
            "615 stories passed. 0.5689176688251619 %\n",
            "616 stories passed. 0.5698427382053654 %\n",
            "617 stories passed. 0.5707678075855689 %\n",
            "618 stories passed. 0.5716928769657724 %\n",
            "619 stories passed. 0.572617946345976 %\n",
            "620 stories passed. 0.5735430157261795 %\n",
            "621 stories passed. 0.574468085106383 %\n",
            "622 stories passed. 0.5753931544865865 %\n",
            "623 stories passed. 0.57631822386679 %\n",
            "624 stories passed. 0.5772432932469935 %\n",
            "625 stories passed. 0.5781683626271971 %\n",
            "626 stories passed. 0.5790934320074006 %\n",
            "627 stories passed. 0.580018501387604 %\n",
            "628 stories passed. 0.5809435707678076 %\n",
            "629 stories passed. 0.5818686401480111 %\n",
            "630 stories passed. 0.5827937095282146 %\n",
            "631 stories passed. 0.5837187789084182 %\n",
            "632 stories passed. 0.5846438482886216 %\n",
            "633 stories passed. 0.5855689176688251 %\n",
            "634 stories passed. 0.5864939870490287 %\n",
            "635 stories passed. 0.5874190564292322 %\n",
            "636 stories passed. 0.5883441258094357 %\n",
            "637 stories passed. 0.5892691951896393 %\n",
            "638 stories passed. 0.5901942645698427 %\n",
            "639 stories passed. 0.5911193339500462 %\n",
            "640 stories passed. 0.5920444033302498 %\n",
            "641 stories passed. 0.5929694727104533 %\n",
            "642 stories passed. 0.5938945420906568 %\n",
            "643 stories passed. 0.5948196114708603 %\n",
            "644 stories passed. 0.5957446808510638 %\n",
            "645 stories passed. 0.5966697502312673 %\n",
            "646 stories passed. 0.5975948196114709 %\n",
            "647 stories passed. 0.5985198889916744 %\n",
            "648 stories passed. 0.599444958371878 %\n",
            "649 stories passed. 0.6003700277520814 %\n",
            "650 stories passed. 0.6012950971322849 %\n",
            "651 stories passed. 0.6022201665124884 %\n",
            "652 stories passed. 0.603145235892692 %\n",
            "653 stories passed. 0.6040703052728955 %\n",
            "654 stories passed. 0.6049953746530989 %\n",
            "655 stories passed. 0.6059204440333025 %\n",
            "656 stories passed. 0.606845513413506 %\n",
            "657 stories passed. 0.6077705827937095 %\n",
            "658 stories passed. 0.6086956521739131 %\n",
            "659 stories passed. 0.6096207215541165 %\n",
            "660 stories passed. 0.61054579093432 %\n",
            "661 stories passed. 0.6114708603145236 %\n",
            "662 stories passed. 0.6123959296947271 %\n",
            "663 stories passed. 0.6133209990749307 %\n",
            "664 stories passed. 0.6142460684551342 %\n",
            "665 stories passed. 0.6151711378353376 %\n",
            "666 stories passed. 0.6160962072155411 %\n",
            "667 stories passed. 0.6170212765957447 %\n",
            "668 stories passed. 0.6179463459759482 %\n",
            "669 stories passed. 0.6188714153561518 %\n",
            "670 stories passed. 0.6197964847363552 %\n",
            "671 stories passed. 0.6207215541165587 %\n",
            "672 stories passed. 0.6216466234967623 %\n",
            "673 stories passed. 0.6225716928769658 %\n",
            "674 stories passed. 0.6234967622571693 %\n",
            "675 stories passed. 0.6244218316373727 %\n",
            "676 stories passed. 0.6253469010175763 %\n",
            "677 stories passed. 0.6262719703977798 %\n",
            "678 stories passed. 0.6271970397779834 %\n",
            "679 stories passed. 0.6281221091581869 %\n",
            "680 stories passed. 0.6290471785383904 %\n",
            "681 stories passed. 0.6299722479185939 %\n",
            "682 stories passed. 0.6308973172987974 %\n",
            "683 stories passed. 0.6318223866790009 %\n",
            "684 stories passed. 0.6327474560592045 %\n",
            "685 stories passed. 0.633672525439408 %\n",
            "686 stories passed. 0.6345975948196114 %\n",
            "687 stories passed. 0.635522664199815 %\n",
            "688 stories passed. 0.6364477335800185 %\n",
            "689 stories passed. 0.637372802960222 %\n",
            "690 stories passed. 0.6382978723404256 %\n",
            "691 stories passed. 0.6392229417206291 %\n",
            "692 stories passed. 0.6401480111008325 %\n",
            "693 stories passed. 0.6410730804810361 %\n",
            "694 stories passed. 0.6419981498612396 %\n",
            "695 stories passed. 0.6429232192414431 %\n",
            "696 stories passed. 0.6438482886216467 %\n",
            "697 stories passed. 0.6447733580018501 %\n",
            "698 stories passed. 0.6456984273820536 %\n",
            "699 stories passed. 0.6466234967622572 %\n",
            "700 stories passed. 0.6475485661424607 %\n",
            "701 stories passed. 0.6484736355226642 %\n",
            "702 stories passed. 0.6493987049028677 %\n",
            "703 stories passed. 0.6503237742830712 %\n",
            "704 stories passed. 0.6512488436632747 %\n",
            "705 stories passed. 0.6521739130434783 %\n",
            "706 stories passed. 0.6530989824236818 %\n",
            "707 stories passed. 0.6540240518038853 %\n",
            "708 stories passed. 0.6549491211840888 %\n",
            "709 stories passed. 0.6558741905642923 %\n",
            "710 stories passed. 0.6567992599444958 %\n",
            "711 stories passed. 0.6577243293246994 %\n",
            "712 stories passed. 0.6586493987049029 %\n",
            "713 stories passed. 0.6595744680851063 %\n",
            "714 stories passed. 0.6604995374653099 %\n",
            "715 stories passed. 0.6614246068455134 %\n",
            "716 stories passed. 0.6623496762257169 %\n",
            "717 stories passed. 0.6632747456059205 %\n",
            "718 stories passed. 0.6641998149861239 %\n",
            "719 stories passed. 0.6651248843663274 %\n",
            "720 stories passed. 0.666049953746531 %\n",
            "721 stories passed. 0.6669750231267345 %\n",
            "722 stories passed. 0.667900092506938 %\n",
            "723 stories passed. 0.6688251618871416 %\n",
            "724 stories passed. 0.669750231267345 %\n",
            "725 stories passed. 0.6706753006475485 %\n",
            "726 stories passed. 0.6716003700277521 %\n",
            "727 stories passed. 0.6725254394079556 %\n",
            "728 stories passed. 0.6734505087881592 %\n",
            "729 stories passed. 0.6743755781683626 %\n",
            "730 stories passed. 0.6753006475485661 %\n",
            "731 stories passed. 0.6762257169287696 %\n",
            "732 stories passed. 0.6771507863089732 %\n",
            "733 stories passed. 0.6780758556891767 %\n",
            "734 stories passed. 0.6790009250693803 %\n",
            "735 stories passed. 0.6799259944495837 %\n",
            "736 stories passed. 0.6808510638297872 %\n",
            "737 stories passed. 0.6817761332099908 %\n",
            "738 stories passed. 0.6827012025901943 %\n",
            "739 stories passed. 0.6836262719703978 %\n",
            "740 stories passed. 0.6845513413506013 %\n",
            "741 stories passed. 0.6854764107308048 %\n",
            "742 stories passed. 0.6864014801110083 %\n",
            "743 stories passed. 0.6873265494912119 %\n",
            "744 stories passed. 0.6882516188714154 %\n",
            "745 stories passed. 0.6891766882516188 %\n",
            "746 stories passed. 0.6901017576318224 %\n",
            "747 stories passed. 0.6910268270120259 %\n",
            "748 stories passed. 0.6919518963922294 %\n",
            "749 stories passed. 0.692876965772433 %\n",
            "750 stories passed. 0.6938020351526365 %\n",
            "751 stories passed. 0.6947271045328399 %\n",
            "752 stories passed. 0.6956521739130435 %\n",
            "753 stories passed. 0.696577243293247 %\n",
            "754 stories passed. 0.6975023126734505 %\n",
            "755 stories passed. 0.6984273820536541 %\n",
            "756 stories passed. 0.6993524514338575 %\n",
            "757 stories passed. 0.700277520814061 %\n",
            "758 stories passed. 0.7012025901942646 %\n",
            "759 stories passed. 0.7021276595744681 %\n",
            "760 stories passed. 0.7030527289546716 %\n",
            "761 stories passed. 0.7039777983348752 %\n",
            "762 stories passed. 0.7049028677150786 %\n",
            "763 stories passed. 0.7058279370952821 %\n",
            "764 stories passed. 0.7067530064754857 %\n",
            "765 stories passed. 0.7076780758556892 %\n",
            "766 stories passed. 0.7086031452358927 %\n",
            "767 stories passed. 0.7095282146160962 %\n",
            "768 stories passed. 0.7104532839962997 %\n",
            "769 stories passed. 0.7113783533765032 %\n",
            "770 stories passed. 0.7123034227567068 %\n",
            "771 stories passed. 0.7132284921369103 %\n",
            "772 stories passed. 0.7141535615171137 %\n",
            "773 stories passed. 0.7150786308973173 %\n",
            "774 stories passed. 0.7160037002775208 %\n",
            "775 stories passed. 0.7169287696577243 %\n",
            "776 stories passed. 0.7178538390379279 %\n",
            "777 stories passed. 0.7187789084181314 %\n",
            "778 stories passed. 0.7197039777983348 %\n",
            "779 stories passed. 0.7206290471785384 %\n",
            "780 stories passed. 0.7215541165587419 %\n",
            "781 stories passed. 0.7224791859389454 %\n",
            "782 stories passed. 0.723404255319149 %\n",
            "783 stories passed. 0.7243293246993524 %\n",
            "784 stories passed. 0.7252543940795559 %\n",
            "785 stories passed. 0.7261794634597595 %\n",
            "786 stories passed. 0.727104532839963 %\n",
            "787 stories passed. 0.7280296022201665 %\n",
            "788 stories passed. 0.72895467160037 %\n",
            "789 stories passed. 0.7298797409805735 %\n",
            "790 stories passed. 0.730804810360777 %\n",
            "791 stories passed. 0.7317298797409806 %\n",
            "792 stories passed. 0.7326549491211841 %\n",
            "793 stories passed. 0.7335800185013877 %\n",
            "794 stories passed. 0.7345050878815911 %\n",
            "795 stories passed. 0.7354301572617946 %\n",
            "796 stories passed. 0.7363552266419982 %\n",
            "797 stories passed. 0.7372802960222017 %\n",
            "798 stories passed. 0.7382053654024052 %\n",
            "799 stories passed. 0.7391304347826086 %\n",
            "800 stories passed. 0.7400555041628122 %\n",
            "801 stories passed. 0.7409805735430157 %\n",
            "802 stories passed. 0.7419056429232193 %\n",
            "803 stories passed. 0.7428307123034228 %\n",
            "804 stories passed. 0.7437557816836263 %\n",
            "805 stories passed. 0.7446808510638298 %\n",
            "806 stories passed. 0.7456059204440333 %\n",
            "807 stories passed. 0.7465309898242368 %\n",
            "808 stories passed. 0.7474560592044404 %\n",
            "809 stories passed. 0.7483811285846439 %\n",
            "810 stories passed. 0.7493061979648473 %\n",
            "811 stories passed. 0.7502312673450509 %\n",
            "812 stories passed. 0.7511563367252544 %\n",
            "813 stories passed. 0.7520814061054579 %\n",
            "814 stories passed. 0.7530064754856615 %\n",
            "815 stories passed. 0.7539315448658649 %\n",
            "816 stories passed. 0.7548566142460684 %\n",
            "817 stories passed. 0.755781683626272 %\n",
            "818 stories passed. 0.7567067530064755 %\n",
            "819 stories passed. 0.757631822386679 %\n",
            "820 stories passed. 0.7585568917668826 %\n",
            "821 stories passed. 0.759481961147086 %\n",
            "822 stories passed. 0.7604070305272895 %\n",
            "823 stories passed. 0.7613320999074931 %\n",
            "824 stories passed. 0.7622571692876966 %\n",
            "825 stories passed. 0.7631822386679001 %\n",
            "826 stories passed. 0.7641073080481036 %\n",
            "827 stories passed. 0.7650323774283071 %\n",
            "828 stories passed. 0.7659574468085106 %\n",
            "829 stories passed. 0.7668825161887142 %\n",
            "830 stories passed. 0.7678075855689177 %\n",
            "831 stories passed. 0.7687326549491211 %\n",
            "832 stories passed. 0.7696577243293247 %\n",
            "833 stories passed. 0.7705827937095282 %\n",
            "834 stories passed. 0.7715078630897317 %\n",
            "835 stories passed. 0.7724329324699353 %\n",
            "836 stories passed. 0.7733580018501388 %\n",
            "837 stories passed. 0.7742830712303422 %\n",
            "838 stories passed. 0.7752081406105458 %\n",
            "839 stories passed. 0.7761332099907493 %\n",
            "840 stories passed. 0.7770582793709528 %\n",
            "841 stories passed. 0.7779833487511564 %\n",
            "842 stories passed. 0.7789084181313598 %\n",
            "843 stories passed. 0.7798334875115633 %\n",
            "844 stories passed. 0.7807585568917669 %\n",
            "845 stories passed. 0.7816836262719704 %\n",
            "846 stories passed. 0.782608695652174 %\n",
            "847 stories passed. 0.7835337650323775 %\n",
            "848 stories passed. 0.7844588344125809 %\n",
            "849 stories passed. 0.7853839037927844 %\n",
            "850 stories passed. 0.786308973172988 %\n",
            "851 stories passed. 0.7872340425531915 %\n",
            "852 stories passed. 0.788159111933395 %\n",
            "853 stories passed. 0.7890841813135985 %\n",
            "854 stories passed. 0.790009250693802 %\n",
            "855 stories passed. 0.7909343200740055 %\n",
            "856 stories passed. 0.7918593894542091 %\n",
            "857 stories passed. 0.7927844588344126 %\n",
            "858 stories passed. 0.793709528214616 %\n",
            "859 stories passed. 0.7946345975948196 %\n",
            "860 stories passed. 0.7955596669750231 %\n",
            "861 stories passed. 0.7964847363552267 %\n",
            "862 stories passed. 0.7974098057354302 %\n",
            "863 stories passed. 0.7983348751156337 %\n",
            "864 stories passed. 0.7992599444958371 %\n",
            "865 stories passed. 0.8001850138760407 %\n",
            "866 stories passed. 0.8011100832562442 %\n",
            "867 stories passed. 0.8020351526364478 %\n",
            "868 stories passed. 0.8029602220166513 %\n",
            "869 stories passed. 0.8038852913968547 %\n",
            "870 stories passed. 0.8048103607770583 %\n",
            "871 stories passed. 0.8057354301572618 %\n",
            "872 stories passed. 0.8066604995374653 %\n",
            "873 stories passed. 0.8075855689176689 %\n",
            "874 stories passed. 0.8085106382978723 %\n",
            "875 stories passed. 0.8094357076780758 %\n",
            "876 stories passed. 0.8103607770582794 %\n",
            "877 stories passed. 0.8112858464384829 %\n",
            "878 stories passed. 0.8122109158186864 %\n",
            "879 stories passed. 0.81313598519889 %\n",
            "880 stories passed. 0.8140610545790934 %\n",
            "881 stories passed. 0.8149861239592969 %\n",
            "882 stories passed. 0.8159111933395005 %\n",
            "883 stories passed. 0.816836262719704 %\n",
            "884 stories passed. 0.8177613320999075 %\n",
            "885 stories passed. 0.818686401480111 %\n",
            "886 stories passed. 0.8196114708603145 %\n",
            "887 stories passed. 0.820536540240518 %\n",
            "888 stories passed. 0.8214616096207216 %\n",
            "889 stories passed. 0.8223866790009251 %\n",
            "890 stories passed. 0.8233117483811286 %\n",
            "891 stories passed. 0.8242368177613321 %\n",
            "892 stories passed. 0.8251618871415356 %\n",
            "893 stories passed. 0.8260869565217391 %\n",
            "894 stories passed. 0.8270120259019427 %\n",
            "895 stories passed. 0.8279370952821462 %\n",
            "896 stories passed. 0.8288621646623496 %\n",
            "897 stories passed. 0.8297872340425532 %\n",
            "898 stories passed. 0.8307123034227567 %\n",
            "899 stories passed. 0.8316373728029602 %\n",
            "900 stories passed. 0.8325624421831638 %\n",
            "901 stories passed. 0.8334875115633672 %\n",
            "902 stories passed. 0.8344125809435707 %\n",
            "903 stories passed. 0.8353376503237743 %\n",
            "904 stories passed. 0.8362627197039778 %\n",
            "905 stories passed. 0.8371877890841813 %\n",
            "906 stories passed. 0.8381128584643849 %\n",
            "907 stories passed. 0.8390379278445883 %\n",
            "908 stories passed. 0.8399629972247918 %\n",
            "909 stories passed. 0.8408880666049954 %\n",
            "910 stories passed. 0.8418131359851989 %\n",
            "911 stories passed. 0.8427382053654024 %\n",
            "912 stories passed. 0.8436632747456059 %\n",
            "913 stories passed. 0.8445883441258094 %\n",
            "914 stories passed. 0.845513413506013 %\n",
            "915 stories passed. 0.8464384828862165 %\n",
            "916 stories passed. 0.84736355226642 %\n",
            "917 stories passed. 0.8482886216466234 %\n",
            "918 stories passed. 0.849213691026827 %\n",
            "919 stories passed. 0.8501387604070305 %\n",
            "920 stories passed. 0.851063829787234 %\n",
            "921 stories passed. 0.8519888991674376 %\n",
            "922 stories passed. 0.8529139685476411 %\n",
            "923 stories passed. 0.8538390379278445 %\n",
            "924 stories passed. 0.8547641073080481 %\n",
            "925 stories passed. 0.8556891766882516 %\n",
            "926 stories passed. 0.8566142460684552 %\n",
            "927 stories passed. 0.8575393154486587 %\n",
            "928 stories passed. 0.8584643848288621 %\n",
            "929 stories passed. 0.8593894542090657 %\n",
            "930 stories passed. 0.8603145235892692 %\n",
            "931 stories passed. 0.8612395929694727 %\n",
            "932 stories passed. 0.8621646623496763 %\n",
            "933 stories passed. 0.8630897317298798 %\n",
            "934 stories passed. 0.8640148011100832 %\n",
            "935 stories passed. 0.8649398704902868 %\n",
            "936 stories passed. 0.8658649398704903 %\n",
            "937 stories passed. 0.8667900092506938 %\n",
            "938 stories passed. 0.8677150786308974 %\n",
            "939 stories passed. 0.8686401480111008 %\n",
            "940 stories passed. 0.8695652173913043 %\n",
            "941 stories passed. 0.8704902867715079 %\n",
            "942 stories passed. 0.8714153561517114 %\n",
            "943 stories passed. 0.8723404255319149 %\n",
            "944 stories passed. 0.8732654949121184 %\n",
            "945 stories passed. 0.8741905642923219 %\n",
            "946 stories passed. 0.8751156336725254 %\n",
            "947 stories passed. 0.876040703052729 %\n",
            "948 stories passed. 0.8769657724329325 %\n",
            "949 stories passed. 0.877890841813136 %\n",
            "950 stories passed. 0.8788159111933395 %\n",
            "951 stories passed. 0.879740980573543 %\n",
            "952 stories passed. 0.8806660499537465 %\n",
            "953 stories passed. 0.8815911193339501 %\n",
            "954 stories passed. 0.8825161887141536 %\n",
            "955 stories passed. 0.883441258094357 %\n",
            "956 stories passed. 0.8843663274745606 %\n",
            "957 stories passed. 0.8852913968547641 %\n",
            "958 stories passed. 0.8862164662349676 %\n",
            "959 stories passed. 0.8871415356151712 %\n",
            "960 stories passed. 0.8880666049953746 %\n",
            "961 stories passed. 0.8889916743755781 %\n",
            "962 stories passed. 0.8899167437557817 %\n",
            "963 stories passed. 0.8908418131359852 %\n",
            "964 stories passed. 0.8917668825161887 %\n",
            "965 stories passed. 0.8926919518963923 %\n",
            "966 stories passed. 0.8936170212765957 %\n",
            "967 stories passed. 0.8945420906567992 %\n",
            "968 stories passed. 0.8954671600370028 %\n",
            "969 stories passed. 0.8963922294172063 %\n",
            "970 stories passed. 0.8973172987974098 %\n",
            "971 stories passed. 0.8982423681776133 %\n",
            "972 stories passed. 0.8991674375578168 %\n",
            "973 stories passed. 0.9000925069380203 %\n",
            "974 stories passed. 0.9010175763182239 %\n",
            "975 stories passed. 0.9019426456984274 %\n",
            "976 stories passed. 0.902867715078631 %\n",
            "977 stories passed. 0.9037927844588344 %\n",
            "978 stories passed. 0.9047178538390379 %\n",
            "979 stories passed. 0.9056429232192414 %\n",
            "980 stories passed. 0.906567992599445 %\n",
            "981 stories passed. 0.9074930619796485 %\n",
            "982 stories passed. 0.9084181313598519 %\n",
            "983 stories passed. 0.9093432007400555 %\n",
            "984 stories passed. 0.910268270120259 %\n",
            "985 stories passed. 0.9111933395004626 %\n",
            "986 stories passed. 0.9121184088806661 %\n",
            "987 stories passed. 0.9130434782608695 %\n",
            "988 stories passed. 0.913968547641073 %\n",
            "989 stories passed. 0.9148936170212766 %\n",
            "990 stories passed. 0.9158186864014801 %\n",
            "991 stories passed. 0.9167437557816837 %\n",
            "992 stories passed. 0.9176688251618872 %\n",
            "993 stories passed. 0.9185938945420906 %\n",
            "994 stories passed. 0.9195189639222942 %\n",
            "995 stories passed. 0.9204440333024977 %\n",
            "996 stories passed. 0.9213691026827012 %\n",
            "997 stories passed. 0.9222941720629048 %\n",
            "998 stories passed. 0.9232192414431082 %\n",
            "999 stories passed. 0.9241443108233117 %\n",
            "1000 stories passed. 0.9250693802035153 %\n",
            "1001 stories passed. 0.9259944495837188 %\n",
            "1002 stories passed. 0.9269195189639223 %\n",
            "1003 stories passed. 0.9278445883441259 %\n",
            "1004 stories passed. 0.9287696577243293 %\n",
            "1005 stories passed. 0.9296947271045328 %\n",
            "1006 stories passed. 0.9306197964847364 %\n",
            "1007 stories passed. 0.9315448658649399 %\n",
            "1008 stories passed. 0.9324699352451434 %\n",
            "1009 stories passed. 0.9333950046253469 %\n",
            "1010 stories passed. 0.9343200740055504 %\n",
            "1011 stories passed. 0.9352451433857539 %\n",
            "1012 stories passed. 0.9361702127659575 %\n",
            "1013 stories passed. 0.937095282146161 %\n",
            "1014 stories passed. 0.9380203515263644 %\n",
            "1015 stories passed. 0.938945420906568 %\n",
            "1016 stories passed. 0.9398704902867715 %\n",
            "1017 stories passed. 0.940795559666975 %\n",
            "1018 stories passed. 0.9417206290471786 %\n",
            "1019 stories passed. 0.9426456984273821 %\n",
            "1020 stories passed. 0.9435707678075855 %\n",
            "1021 stories passed. 0.9444958371877891 %\n",
            "1022 stories passed. 0.9454209065679926 %\n",
            "1023 stories passed. 0.9463459759481961 %\n",
            "1024 stories passed. 0.9472710453283997 %\n",
            "1025 stories passed. 0.9481961147086031 %\n",
            "1026 stories passed. 0.9491211840888066 %\n",
            "1027 stories passed. 0.9500462534690102 %\n",
            "1028 stories passed. 0.9509713228492137 %\n",
            "1029 stories passed. 0.9518963922294172 %\n",
            "1030 stories passed. 0.9528214616096207 %\n",
            "1031 stories passed. 0.9537465309898242 %\n",
            "1032 stories passed. 0.9546716003700277 %\n",
            "1033 stories passed. 0.9555966697502313 %\n",
            "1034 stories passed. 0.9565217391304348 %\n",
            "1035 stories passed. 0.9574468085106383 %\n",
            "1036 stories passed. 0.9583718778908418 %\n",
            "1037 stories passed. 0.9592969472710453 %\n",
            "1038 stories passed. 0.9602220166512488 %\n",
            "1039 stories passed. 0.9611470860314524 %\n",
            "1040 stories passed. 0.9620721554116559 %\n",
            "1041 stories passed. 0.9629972247918593 %\n",
            "1042 stories passed. 0.9639222941720629 %\n",
            "1043 stories passed. 0.9648473635522664 %\n",
            "1044 stories passed. 0.96577243293247 %\n",
            "1045 stories passed. 0.9666975023126735 %\n",
            "1046 stories passed. 0.967622571692877 %\n",
            "1047 stories passed. 0.9685476410730804 %\n",
            "1048 stories passed. 0.969472710453284 %\n",
            "1049 stories passed. 0.9703977798334875 %\n",
            "1050 stories passed. 0.971322849213691 %\n",
            "1051 stories passed. 0.9722479185938946 %\n",
            "1052 stories passed. 0.973172987974098 %\n",
            "1053 stories passed. 0.9740980573543015 %\n",
            "1054 stories passed. 0.9750231267345051 %\n",
            "1055 stories passed. 0.9759481961147086 %\n",
            "1056 stories passed. 0.9768732654949122 %\n",
            "1057 stories passed. 0.9777983348751156 %\n",
            "1058 stories passed. 0.9787234042553191 %\n",
            "1059 stories passed. 0.9796484736355227 %\n",
            "1060 stories passed. 0.9805735430157262 %\n",
            "1061 stories passed. 0.9814986123959297 %\n",
            "1062 stories passed. 0.9824236817761333 %\n",
            "1063 stories passed. 0.9833487511563367 %\n",
            "1064 stories passed. 0.9842738205365402 %\n",
            "1065 stories passed. 0.9851988899167438 %\n",
            "1066 stories passed. 0.9861239592969473 %\n",
            "1067 stories passed. 0.9870490286771508 %\n",
            "1068 stories passed. 0.9879740980573543 %\n",
            "1069 stories passed. 0.9888991674375578 %\n",
            "1070 stories passed. 0.9898242368177613 %\n",
            "1071 stories passed. 0.9907493061979649 %\n",
            "1072 stories passed. 0.9916743755781684 %\n",
            "1073 stories passed. 0.9925994449583718 %\n",
            "1074 stories passed. 0.9935245143385754 %\n",
            "1075 stories passed. 0.9944495837187789 %\n",
            "1076 stories passed. 0.9953746530989824 %\n",
            "1077 stories passed. 0.996299722479186 %\n",
            "1078 stories passed. 0.9972247918593895 %\n",
            "1079 stories passed. 0.9981498612395929 %\n",
            "1080 stories passed. 0.9990749306197965 %\n",
            "1081 stories passed. 1.0 %\n",
            "ROUGE F1 (mean): 0.13965212316517933\n",
            "ROUGE F1 (best): 0.22383663762216474\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412,
          "referenced_widgets": [
            "e394c4a84d1b4c39a3b7ea2f01b12b31",
            "04f7e967d30a4da0ae657efb80767758",
            "54532c6863134a99a255b67c56a8ffba",
            "7d586fb4a4ed447ea5918153aaa39d45",
            "f55cbecc9a0c4e69b4d81eab97e5bfe0",
            "3b13e0c18aab4addb4b30e8910229484",
            "fd434e3d141b40c68b319e22180d6fb3",
            "a170a629b6be4caf93cfca15c5593a22"
          ]
        },
        "id": "b5q_nfGlez3O",
        "outputId": "942ab008-5ec6-420e-938e-980c4348de44"
      },
      "source": [
        "# collect test data from all_stories\n",
        "source_text_test = []\n",
        "target_text_test = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    \n",
        "    for j in range(len(highlights)):\n",
        "        source_text_test.append(story)\n",
        "        target_text_test.append(highlights[j])\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "print('')\n",
        "\n",
        "# format test data as a dataset and tokenize\n",
        "test_df = pd.DataFrame(list(zip(source_text_test, target_text_test)),columns =['source', 'target'])\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_tokenized = test_dataset.map(tokenize, batched=True, batch_size=512)\n",
        "print(test_dataset)\n",
        "\n",
        "# get loss\n",
        "eval_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=8,\n",
        "    # remove_unused_columns=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    output_dir = 'output'\n",
        ")\n",
        "trainer = Trainer(model=baseline_model, args=eval_args)\n",
        "\n",
        "trainer.evaluate(test_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.0003030856450398763 minutes\n",
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 2720\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e394c4a84d1b4c39a3b7ea2f01b12b31",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=6.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [340/340 00:49]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.08352790772914886,\n",
              " 'eval_mem_cpu_alloc_delta': 120849,\n",
              " 'eval_mem_cpu_peaked_delta': 305263,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 1279395840,\n",
              " 'eval_runtime': 50.0694,\n",
              " 'eval_samples_per_second': 54.325,\n",
              " 'init_mem_cpu_alloc_delta': 50734,\n",
              " 'init_mem_cpu_peaked_delta': 18306,\n",
              " 'init_mem_gpu_alloc_delta': 242026496,\n",
              " 'init_mem_gpu_peaked_delta': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdksd2SmxBxJ"
      },
      "source": [
        "## C. Na&iuml;ve Exclusion\n",
        "\n",
        "Let's just leave those pesky undesirable outcomes out of the training! Let's see what that buys us! (Likely not a lot).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MCB4DFUZAsw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsEaa10RZl7S"
      },
      "source": [
        "### Data Formatting\n",
        "\n",
        "Right now, we have a text file for each story. T5 requires a single matrix (a dataset object is perfect) with source/target columns. For this model, the input is the story, and the target is now a paragraph composed of all highlights but the first. The first highlight is not fed to the model at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPsx_MbMx-FP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dafbe44-e4b1-4f7d-844c-3c7ea980196c"
      },
      "source": [
        "### GET TRAIN DATA ###\n",
        "\n",
        "source_text_train = []\n",
        "target_text_train = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(train_files)):\n",
        "# for i in range(10):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[train_files[i],'story']\n",
        "    highlights = all_stories.loc[train_files[i],'highlights']\n",
        "\n",
        "    # format data as story/joined highlights pairs(don't use highlight zero)\n",
        "    source_text_train.append(story)\n",
        "    target_text_train.append('. '.join(highlights[1:])+'.')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "print('')\n",
        "\n",
        "# format as a dataset\n",
        "train_df = pd.DataFrame(list(zip(source_text_train, target_text_train)),columns =['source', 'target'])\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "print(train_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.025903729597727458 minutes\n",
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 232583\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0ZNOjHtyYGK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d916b0cf-8cc9-47d6-a9bb-17b7b100804a"
      },
      "source": [
        "### GET VALIDATION DATA ###\n",
        "\n",
        "source_text_val = []\n",
        "target_text_val = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(valid_files)):\n",
        "# for i in range(10):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/joined highlights pairs(don't use highlight zero)\n",
        "    source_text_val.append(story)\n",
        "    target_text_val.append('. '.join(highlights[1:])+'.') \n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "print('')\n",
        "\n",
        "# format as a dataset\n",
        "val_df = pd.DataFrame(list(zip(source_text_val, target_text_val)),columns =['source', 'target'])\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.00044585466384887694 minutes\n",
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 1872\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_STY2u1zPRM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d8e3ba9-e50a-4aa1-bbca-356aff0cc699"
      },
      "source": [
        "print('='*60+'\\nEXAMPLE INPUT TEXT\\n'+'='*60)\n",
        "print(source_text_train[0])\n",
        "print('='*60+'\\nEXAMPLE TARGET TEXT\\n'+'='*60)\n",
        "print(target_text_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXAMPLE INPUT TEXT\n",
            "============================================================\n",
            "It 's official : U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria .\n",
            "\n",
            "Obama sent a letter to the heads of the House and Senate on Saturday night , hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons .\n",
            "\n",
            "The proposed legislation from Obama asks Congress to approve the use of military force `` to deter , disrupt , prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction . ''\n",
            "\n",
            "It 's a step that is set to turn an international crisis into a fierce domestic political battle .\n",
            "\n",
            "There are key questions looming over the debate : What did U.N. weapons inspectors find in Syria ? What happens if Congress votes no ? And how will the Syrian government react ?\n",
            "\n",
            "In a televised address from the White House Rose Garden earlier Saturday , the president said he would take his case to Congress , not because he has to -- but because he wants to .\n",
            "\n",
            "`` While I believe I have the authority to carry out this military action without specific congressional authorization , I know that the country will be stronger if we take this course , and our actions will be even more effective , '' he said . `` We should have this debate , because the issues are too big for business as usual . ''\n",
            "\n",
            "Obama said top congressional leaders had agreed to schedule a debate when the body returns to Washington on September 9 . The Senate Foreign Relations Committee will hold a hearing over the matter on Tuesday , Sen. Robert Menendez said .\n",
            "\n",
            "Transcript : Read Obama 's full remarks\n",
            "\n",
            "Syrian crisis : Latest developments\n",
            "\n",
            "U.N. inspectors leave Syria\n",
            "\n",
            "Obama 's remarks came shortly after U.N. inspectors left Syria , carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb .\n",
            "\n",
            "`` The aim of the game here , the mandate , is very clear -- and that is to ascertain whether chemical weapons were used -- and not by whom , '' U.N. spokesman Martin Nesirky told reporters on Saturday .\n",
            "\n",
            "But who used the weapons in the reported toxic gas attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisis .\n",
            "\n",
            "Top U.S. officials have said there 's no doubt that the Syrian government was behind it , while Syrian officials have denied responsibility and blamed jihadists fighting with the rebels .\n",
            "\n",
            "British and U.S. intelligence reports say the attack involved chemical weapons , but U.N. officials have stressed the importance of waiting for an official report from inspectors .\n",
            "\n",
            "The inspectors will share their findings with U.N. Secretary-General Ban Ki-moon Ban , who has said he wants to wait until the U.N. team 's final report is completed before presenting it to the U.N. Security Council .\n",
            "\n",
            "The Organization for the Prohibition of Chemical Weapons , which nine of the inspectors belong to , said Saturday that it could take up to three weeks to analyze the evidence they collected .\n",
            "\n",
            "`` It needs time to be able to analyze the information and the samples , '' Nesirky said .\n",
            "\n",
            "He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria , and that `` a military solution is not an option . ''\n",
            "\n",
            "Bergen : Syria is a problem from hell for the U.S.\n",
            "\n",
            "Obama : ` This menace must be confronted '\n",
            "\n",
            "Obama 's senior advisers have debated the next steps to take , and the president 's comments Saturday came amid mounting political pressure over the situation in Syria . Some U.S. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire .\n",
            "\n",
            "Some global leaders have expressed support , but the British Parliament 's vote against military action earlier this week was a blow to Obama 's hopes of getting strong backing from key NATO allies .\n",
            "\n",
            "On Saturday , Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad . Any military attack would not be open-ended or include U.S. ground forces , he said .\n",
            "\n",
            "Syria 's alleged use of chemical weapons earlier this month `` is an assault on human dignity , '' the president said .\n",
            "\n",
            "A failure to respond with force , Obama argued , `` could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm . In a world with many dangers , this menace must be confronted . ''\n",
            "\n",
            "Syria missile strike : What would happen next ?\n",
            "\n",
            "Map : U.S. and allied assets around Syria\n",
            "\n",
            "Obama decision came Friday night\n",
            "\n",
            "On Friday night , the president made a last-minute decision to consult lawmakers .\n",
            "\n",
            "What will happen if they vote no ?\n",
            "\n",
            "It 's unclear . A senior administration official told CNN that Obama has the authority to act without Congress -- even if Congress rejects his request for authorization to use force .\n",
            "\n",
            "Obama on Saturday continued to shore up support for a strike on the al-Assad government .\n",
            "\n",
            "He spoke by phone with French President Francois Hollande before his Rose Garden speech .\n",
            "\n",
            "`` The two leaders agreed that the international community must deliver a resolute message to the Assad regime -- and others who would consider using chemical weapons -- that these crimes are unacceptable and those who violate this international norm will be held accountable by the world , '' the White House said .\n",
            "\n",
            "Meanwhile , as uncertainty loomed over how Congress would weigh in , U.S. military officials said they remained at the ready .\n",
            "\n",
            "5 key assertions : U.S. intelligence report on Syria\n",
            "\n",
            "Syria : Who wants what after chemical weapons horror\n",
            "\n",
            "Reactions mixed to Obama 's speech\n",
            "\n",
            "A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama 's announcement .\n",
            "\n",
            "`` Our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way , '' said spokesman Louay Safi . `` So we are quite concerned . ''\n",
            "\n",
            "Some members of Congress applauded Obama 's decision .\n",
            "\n",
            "House Speaker John Boehner , Majority Leader Eric Cantor , Majority Whip Kevin McCarthy and Conference Chair Cathy McMorris Rodgers issued a statement Saturday praising the president .\n",
            "\n",
            "`` Under the Constitution , the responsibility to declare war lies with Congress , '' the Republican lawmakers said . `` We are glad the president is seeking authorization for any military action in Syria in response to serious , substantive questions being raised . ''\n",
            "\n",
            "More than 160 legislators , including 63 of Obama 's fellow Democrats , had signed letters calling for either a vote or at least a `` full debate '' before any U.S. action .\n",
            "\n",
            "British Prime Minister David Cameron , whose own attempt to get lawmakers in his country to support military action in Syria failed earlier this week , responded to Obama 's speech in a Twitter post Saturday .\n",
            "\n",
            "`` I understand and support Barack Obama 's position on Syria , '' Cameron said .\n",
            "\n",
            "An influential lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theory .\n",
            "\n",
            "`` The main reason Obama is turning to the Congress : the military operation did not get enough support either in the world , among allies of the US or in the United States itself , '' Alexei Pushkov , chairman of the international-affairs committee of the Russian State Duma , said in a Twitter post .\n",
            "\n",
            "In the United States , scattered groups of anti-war protesters around the country took to the streets Saturday .\n",
            "\n",
            "`` Like many other Americans ... we 're just tired of the United States getting involved and invading and bombing other countries , '' said Robin Rosecrans , who was among hundreds at a Los Angeles demonstration .\n",
            "\n",
            "What do Syria 's neighbors think ?\n",
            "\n",
            "Why Russia , China , Iran stand by Assad\n",
            "\n",
            "Syria 's government unfazed\n",
            "\n",
            "After Obama 's speech , a military and political analyst on Syrian state TV said Obama is `` embarrassed '' that Russia opposes military action against Syria , is `` crying for help '' for someone to come to his rescue and is facing two defeats -- on the political and military levels .\n",
            "\n",
            "Syria 's prime minister appeared unfazed by the saber-rattling .\n",
            "\n",
            "`` The Syrian Army 's status is on maximum readiness and fingers are on the trigger to confront all challenges , '' Wael Nader al-Halqi said during a meeting with a delegation of Syrian expatriates from Italy , according to a banner on Syria State TV that was broadcast prior to Obama 's address .\n",
            "\n",
            "An anchor on Syrian state television said Obama `` appeared to be preparing for an aggression on Syria based on repeated lies . ''\n",
            "\n",
            "A top Syrian diplomat told the state television network that Obama was facing pressure to take military action from Israel , Turkey , some Arabs and right-wing extremists in the United States .\n",
            "\n",
            "`` I think he has done well by doing what Cameron did in terms of taking the issue to Parliament , '' said Bashar Jaafari , Syria 's ambassador to the United Nations .\n",
            "\n",
            "Both Obama and Cameron , he said , `` climbed to the top of the tree and do n't know how to get down . ''\n",
            "\n",
            "The Syrian government has denied that it used chemical weapons in the August 21 attack , saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it .\n",
            "\n",
            "British intelligence had put the number of people killed in the attack at more than 350 .\n",
            "\n",
            "On Saturday , Obama said `` all told , well over 1,000 people were murdered . '' U.S. Secretary of State John Kerry on Friday cited a death toll of 1,429 , more than 400 of them children . No explanation was offered for the discrepancy .\n",
            "\n",
            "Iran : U.S. military action in Syria would spark ` disaster '\n",
            "\n",
            "Opinion : Why strikes in Syria are a bad idea\n",
            "============================================================\n",
            "EXAMPLE TARGET TEXT\n",
            "============================================================\n",
            "Obama sends a letter to the heads of the House and Senate\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMrPL8sFzVxL"
      },
      "source": [
        "### Tokenize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZtIks2gzUyZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184,
          "referenced_widgets": [
            "c8d7312d1e7a43b09ec65df299a0175f",
            "2d9a4d69ec084aa0a554ee781ce48a6a",
            "5c0d9930cee74690aab5fd3db5756f79",
            "61c950f1154449569c60355d5ca2c350",
            "e58c221536b84dfda7d6d6acb5c7c61c",
            "f64d5b6474434116a4a9de9b896d8d6e",
            "a00fbc894a1549739460f4a54b95e7d5",
            "22d96da79ed14450948b049aaf3d2470",
            "c5927c9fb75a424ba97f455ef50c2544",
            "1ef00c7b46f949da91bbdea0c302ee1b",
            "15c8239898d144038ecf91399c15ae06",
            "13a64b6ed19844aeabe040c68a6ca0a9",
            "7ae3b2394e984365a08327875d498d19",
            "d861c56a497c4f1ea45ee8473612599b",
            "82472882aee44715af066d8a9d249107",
            "3fcf316b569440cba19cd5c8f549a182"
          ]
        },
        "outputId": "82aac155-4271-4e58-e6c9-47461c9ac31d"
      },
      "source": [
        "train_tokenized = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
        "val_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "print(val_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8d7312d1e7a43b09ec65df299a0175f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=455.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c5927c9fb75a424ba97f455ef50c2544",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset({\n",
            "    features: ['attention_mask', 'input_ids', 'labels', 'source', 'target'],\n",
            "    num_rows: 1872\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QgBiiFcznZa"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBpeTt59zowB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b2fdeb28-7765-4254-964e-65cc790dcb80"
      },
      "source": [
        "output_dir = 'naive_exclusion_model'\n",
        "\n",
        "# training arguments to feed to Trainer object\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_dir, # trained model will be saved here\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size = 8, # number of examples per batch\n",
        "    per_device_eval_batch_size = 8, # number of examples per batch\n",
        "    eval_accumulation_steps = 1,\n",
        "    prediction_loss_only = True,\n",
        "    learning_rate = 0.001,\n",
        "    evaluation_strategy = 'steps',\n",
        "    save_steps = 10,\n",
        "    save_total_limit = 1,\n",
        "    remove_unused_columns = True,\n",
        "    run_name = 'run_name',\n",
        "    logging_steps = 500, # print loss after this many steps\n",
        "    eval_steps = 500, # calculate loss after this many steps\n",
        "    logging_first_step = False,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"loss\", \n",
        "    greater_is_better = False\n",
        ")\n",
        "\n",
        "# create Trainer to feed the train/dev data\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_tokenized,\n",
        "    eval_dataset = val_tokenized\n",
        ")\n",
        "\n",
        "# train the model and save it to our directory\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir + '/model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='29073' max='29073' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [29073/29073 4:17:56, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.175300</td>\n",
              "      <td>0.100025</td>\n",
              "      <td>38.042500</td>\n",
              "      <td>49.208000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.099000</td>\n",
              "      <td>0.099280</td>\n",
              "      <td>37.590000</td>\n",
              "      <td>49.801000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.098100</td>\n",
              "      <td>0.098426</td>\n",
              "      <td>37.599100</td>\n",
              "      <td>49.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.097300</td>\n",
              "      <td>0.097800</td>\n",
              "      <td>37.357400</td>\n",
              "      <td>50.111000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.097300</td>\n",
              "      <td>0.098307</td>\n",
              "      <td>37.561300</td>\n",
              "      <td>49.838000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.098000</td>\n",
              "      <td>0.096842</td>\n",
              "      <td>37.492400</td>\n",
              "      <td>49.930000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.097000</td>\n",
              "      <td>0.096864</td>\n",
              "      <td>37.420800</td>\n",
              "      <td>50.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.095800</td>\n",
              "      <td>0.097174</td>\n",
              "      <td>37.539100</td>\n",
              "      <td>49.868000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.096200</td>\n",
              "      <td>0.096590</td>\n",
              "      <td>37.471500</td>\n",
              "      <td>49.958000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.095800</td>\n",
              "      <td>0.096404</td>\n",
              "      <td>37.419900</td>\n",
              "      <td>50.027000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.095900</td>\n",
              "      <td>0.096190</td>\n",
              "      <td>37.413100</td>\n",
              "      <td>50.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.095400</td>\n",
              "      <td>0.095695</td>\n",
              "      <td>37.466800</td>\n",
              "      <td>49.964000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.093700</td>\n",
              "      <td>0.095681</td>\n",
              "      <td>36.875500</td>\n",
              "      <td>50.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.096600</td>\n",
              "      <td>0.095851</td>\n",
              "      <td>35.926600</td>\n",
              "      <td>52.106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.094000</td>\n",
              "      <td>0.095578</td>\n",
              "      <td>35.884100</td>\n",
              "      <td>52.168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.093700</td>\n",
              "      <td>0.095834</td>\n",
              "      <td>35.932300</td>\n",
              "      <td>52.098000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.094600</td>\n",
              "      <td>0.095302</td>\n",
              "      <td>35.812800</td>\n",
              "      <td>52.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.092100</td>\n",
              "      <td>0.094903</td>\n",
              "      <td>35.863600</td>\n",
              "      <td>52.198000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.093700</td>\n",
              "      <td>0.094191</td>\n",
              "      <td>36.334300</td>\n",
              "      <td>51.522000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.094000</td>\n",
              "      <td>0.094460</td>\n",
              "      <td>35.916200</td>\n",
              "      <td>52.121000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.093900</td>\n",
              "      <td>0.094496</td>\n",
              "      <td>35.882900</td>\n",
              "      <td>52.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.093400</td>\n",
              "      <td>0.093748</td>\n",
              "      <td>35.883900</td>\n",
              "      <td>52.168000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.092300</td>\n",
              "      <td>0.093659</td>\n",
              "      <td>35.906100</td>\n",
              "      <td>52.136000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.093600</td>\n",
              "      <td>0.093797</td>\n",
              "      <td>35.966300</td>\n",
              "      <td>52.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.092400</td>\n",
              "      <td>0.093567</td>\n",
              "      <td>35.910200</td>\n",
              "      <td>52.130000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.092200</td>\n",
              "      <td>0.093529</td>\n",
              "      <td>36.157100</td>\n",
              "      <td>51.774000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.092200</td>\n",
              "      <td>0.092698</td>\n",
              "      <td>35.974600</td>\n",
              "      <td>52.037000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.091800</td>\n",
              "      <td>0.093093</td>\n",
              "      <td>36.307800</td>\n",
              "      <td>51.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.091000</td>\n",
              "      <td>0.092784</td>\n",
              "      <td>36.056100</td>\n",
              "      <td>51.919000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.091800</td>\n",
              "      <td>0.092476</td>\n",
              "      <td>35.958000</td>\n",
              "      <td>52.061000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.092435</td>\n",
              "      <td>35.973400</td>\n",
              "      <td>52.038000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.090900</td>\n",
              "      <td>0.092840</td>\n",
              "      <td>35.939500</td>\n",
              "      <td>52.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.090600</td>\n",
              "      <td>0.092104</td>\n",
              "      <td>36.053200</td>\n",
              "      <td>51.923000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.090900</td>\n",
              "      <td>0.091675</td>\n",
              "      <td>36.035200</td>\n",
              "      <td>51.949000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.091000</td>\n",
              "      <td>0.091577</td>\n",
              "      <td>35.981900</td>\n",
              "      <td>52.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.091530</td>\n",
              "      <td>35.954100</td>\n",
              "      <td>52.066000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.090100</td>\n",
              "      <td>0.091746</td>\n",
              "      <td>35.960800</td>\n",
              "      <td>52.057000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.088400</td>\n",
              "      <td>0.090861</td>\n",
              "      <td>35.949100</td>\n",
              "      <td>52.074000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.089800</td>\n",
              "      <td>0.091216</td>\n",
              "      <td>35.975100</td>\n",
              "      <td>52.036000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.087600</td>\n",
              "      <td>0.090632</td>\n",
              "      <td>36.124900</td>\n",
              "      <td>51.820000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.089800</td>\n",
              "      <td>0.090699</td>\n",
              "      <td>36.240700</td>\n",
              "      <td>51.655000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.089800</td>\n",
              "      <td>0.090730</td>\n",
              "      <td>36.250000</td>\n",
              "      <td>51.641000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.088900</td>\n",
              "      <td>0.090661</td>\n",
              "      <td>36.309100</td>\n",
              "      <td>51.557000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.088600</td>\n",
              "      <td>0.090347</td>\n",
              "      <td>36.184100</td>\n",
              "      <td>51.735000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.088400</td>\n",
              "      <td>0.090077</td>\n",
              "      <td>36.167700</td>\n",
              "      <td>51.759000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.087300</td>\n",
              "      <td>0.090122</td>\n",
              "      <td>34.731600</td>\n",
              "      <td>53.899000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.086900</td>\n",
              "      <td>0.089977</td>\n",
              "      <td>35.962400</td>\n",
              "      <td>52.054000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.088400</td>\n",
              "      <td>0.090036</td>\n",
              "      <td>36.007400</td>\n",
              "      <td>51.989000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.087800</td>\n",
              "      <td>0.089699</td>\n",
              "      <td>36.019700</td>\n",
              "      <td>51.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.088300</td>\n",
              "      <td>0.089573</td>\n",
              "      <td>36.032800</td>\n",
              "      <td>51.953000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.088500</td>\n",
              "      <td>0.089316</td>\n",
              "      <td>35.991000</td>\n",
              "      <td>52.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.087600</td>\n",
              "      <td>0.089258</td>\n",
              "      <td>36.048300</td>\n",
              "      <td>51.930000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.087800</td>\n",
              "      <td>0.089218</td>\n",
              "      <td>36.042700</td>\n",
              "      <td>51.938000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.087000</td>\n",
              "      <td>0.089077</td>\n",
              "      <td>35.960100</td>\n",
              "      <td>52.058000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.086000</td>\n",
              "      <td>0.088948</td>\n",
              "      <td>35.964800</td>\n",
              "      <td>52.051000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.087800</td>\n",
              "      <td>0.088846</td>\n",
              "      <td>36.017700</td>\n",
              "      <td>51.974000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.087200</td>\n",
              "      <td>0.088868</td>\n",
              "      <td>35.968100</td>\n",
              "      <td>52.046000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.085900</td>\n",
              "      <td>0.088858</td>\n",
              "      <td>34.743300</td>\n",
              "      <td>53.881000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZhNR8_40prL"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "33a842d8670448d3b8fd2b530a3ec309",
            "8d09abd5d3b4416e973f5fa681db03c2",
            "03cfe8b25a794d6b822b6cb9d739c750",
            "73f3689fa5614b5a8a78c1d17bbf9225",
            "1713acc87e2f43f8b4bb9dfa995e629e",
            "887380724a9545c7a80577afc2712f8f",
            "39baf294d185498497a4384f2958e1be",
            "9306ab0e198e49b4b8a5f35824500e5e"
          ]
        },
        "id": "o-VNpijS64xZ",
        "outputId": "429b540e-b6d6-4446-bb99-e5ccc1e23fc5"
      },
      "source": [
        "# load our model\n",
        "naive_model = T5ForConditionalGeneration.from_pretrained('naive_exclusion_model/model')\n",
        "\n",
        "# collect test data in the format required for this next model\n",
        "source_text_test = []\n",
        "target_text_test = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "\n",
        "    # format data as story/joined highlights pairs(don't use highlight zero)\n",
        "    source_text_test.append(story)\n",
        "    target_text_test.append('. '.join(highlights[1:])+'.')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format test data as dataset\n",
        "test_df = pd.DataFrame(list(zip(source_text_test, target_text_test)),columns =['source', 'target'])\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_tokenized = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
        "print(test_dataset)\n",
        "\n",
        "# evaluate the model on the test dataset\n",
        "eval_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=8,\n",
        "    # remove_unused_columns=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    output_dir = 'output'\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=naive_model, args=eval_args)\n",
        "\n",
        "trainer.evaluate(test_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.0002849419911702474 minutes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33a842d8670448d3b8fd2b530a3ec309",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 1639\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='205' max='205' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [205/205 00:30]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.08876967430114746,\n",
              " 'eval_mem_cpu_alloc_delta': -2612924416,\n",
              " 'eval_mem_cpu_peaked_delta': 2612924416,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 1279395840,\n",
              " 'eval_runtime': 30.4047,\n",
              " 'eval_samples_per_second': 53.906,\n",
              " 'init_mem_cpu_alloc_delta': 0,\n",
              " 'init_mem_cpu_peaked_delta': 0,\n",
              " 'init_mem_gpu_alloc_delta': 242026496,\n",
              " 'init_mem_gpu_peaked_delta': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fy96ysF0qzp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9dc248db-31d2-4292-fd61-a618e50c8650"
      },
      "source": [
        "naive_model = T5ForConditionalGeneration.from_pretrained('naive_exclusion_model/model')\n",
        "\n",
        "# for scoring outputs\n",
        "rouge = Rouge()\n",
        "\n",
        "mean_rouge = []\n",
        "max_rouge = []\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "\n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "\n",
        "    # encode the input\n",
        "    encoded = tokenizer.encode('summarize: ' + story, return_tensors='pt')\n",
        "\n",
        "    # generate the output\n",
        "    output = naive_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
        "                             min_length=30, max_length=300, early_stopping=True)\n",
        "    summary = tokenizer.decode(output[0])\n",
        "    \n",
        "    # get ROUGE scores between the output and highlights\n",
        "    scores = [rouge.get_scores(summary,highlight)[0]['rouge-1']['f'] for highlight in highlights]\n",
        "\n",
        "    mean_rouge.append(np.mean(scores))\n",
        "    max_rouge.append(max(scores))\n",
        "\n",
        "    if 1%1==0:\n",
        "      print(i+1,'stories passed')\n",
        "\n",
        "print('ROUGE F1 (mean):',np.mean(mean_rouge))\n",
        "print('ROUGE F1 (best):',np.mean(max_rouge))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 stories passed\n",
            "2 stories passed\n",
            "3 stories passed\n",
            "4 stories passed\n",
            "5 stories passed\n",
            "6 stories passed\n",
            "7 stories passed\n",
            "8 stories passed\n",
            "9 stories passed\n",
            "10 stories passed\n",
            "11 stories passed\n",
            "12 stories passed\n",
            "13 stories passed\n",
            "14 stories passed\n",
            "15 stories passed\n",
            "16 stories passed\n",
            "17 stories passed\n",
            "18 stories passed\n",
            "19 stories passed\n",
            "20 stories passed\n",
            "21 stories passed\n",
            "22 stories passed\n",
            "23 stories passed\n",
            "24 stories passed\n",
            "25 stories passed\n",
            "26 stories passed\n",
            "27 stories passed\n",
            "28 stories passed\n",
            "29 stories passed\n",
            "30 stories passed\n",
            "31 stories passed\n",
            "32 stories passed\n",
            "33 stories passed\n",
            "34 stories passed\n",
            "35 stories passed\n",
            "36 stories passed\n",
            "37 stories passed\n",
            "38 stories passed\n",
            "39 stories passed\n",
            "40 stories passed\n",
            "41 stories passed\n",
            "42 stories passed\n",
            "43 stories passed\n",
            "44 stories passed\n",
            "45 stories passed\n",
            "46 stories passed\n",
            "47 stories passed\n",
            "48 stories passed\n",
            "49 stories passed\n",
            "50 stories passed\n",
            "51 stories passed\n",
            "52 stories passed\n",
            "53 stories passed\n",
            "54 stories passed\n",
            "55 stories passed\n",
            "56 stories passed\n",
            "57 stories passed\n",
            "58 stories passed\n",
            "59 stories passed\n",
            "60 stories passed\n",
            "61 stories passed\n",
            "62 stories passed\n",
            "63 stories passed\n",
            "64 stories passed\n",
            "65 stories passed\n",
            "66 stories passed\n",
            "67 stories passed\n",
            "68 stories passed\n",
            "69 stories passed\n",
            "70 stories passed\n",
            "71 stories passed\n",
            "72 stories passed\n",
            "73 stories passed\n",
            "74 stories passed\n",
            "75 stories passed\n",
            "76 stories passed\n",
            "77 stories passed\n",
            "78 stories passed\n",
            "79 stories passed\n",
            "80 stories passed\n",
            "81 stories passed\n",
            "82 stories passed\n",
            "83 stories passed\n",
            "84 stories passed\n",
            "85 stories passed\n",
            "86 stories passed\n",
            "87 stories passed\n",
            "88 stories passed\n",
            "89 stories passed\n",
            "90 stories passed\n",
            "91 stories passed\n",
            "92 stories passed\n",
            "93 stories passed\n",
            "94 stories passed\n",
            "95 stories passed\n",
            "96 stories passed\n",
            "97 stories passed\n",
            "98 stories passed\n",
            "99 stories passed\n",
            "100 stories passed\n",
            "101 stories passed\n",
            "102 stories passed\n",
            "103 stories passed\n",
            "104 stories passed\n",
            "105 stories passed\n",
            "106 stories passed\n",
            "107 stories passed\n",
            "108 stories passed\n",
            "109 stories passed\n",
            "110 stories passed\n",
            "111 stories passed\n",
            "112 stories passed\n",
            "113 stories passed\n",
            "114 stories passed\n",
            "115 stories passed\n",
            "116 stories passed\n",
            "117 stories passed\n",
            "118 stories passed\n",
            "119 stories passed\n",
            "120 stories passed\n",
            "121 stories passed\n",
            "122 stories passed\n",
            "123 stories passed\n",
            "124 stories passed\n",
            "125 stories passed\n",
            "126 stories passed\n",
            "127 stories passed\n",
            "128 stories passed\n",
            "129 stories passed\n",
            "130 stories passed\n",
            "131 stories passed\n",
            "132 stories passed\n",
            "133 stories passed\n",
            "134 stories passed\n",
            "135 stories passed\n",
            "136 stories passed\n",
            "137 stories passed\n",
            "138 stories passed\n",
            "139 stories passed\n",
            "140 stories passed\n",
            "141 stories passed\n",
            "142 stories passed\n",
            "143 stories passed\n",
            "144 stories passed\n",
            "145 stories passed\n",
            "146 stories passed\n",
            "147 stories passed\n",
            "148 stories passed\n",
            "149 stories passed\n",
            "150 stories passed\n",
            "151 stories passed\n",
            "152 stories passed\n",
            "153 stories passed\n",
            "154 stories passed\n",
            "155 stories passed\n",
            "156 stories passed\n",
            "157 stories passed\n",
            "158 stories passed\n",
            "159 stories passed\n",
            "160 stories passed\n",
            "161 stories passed\n",
            "162 stories passed\n",
            "163 stories passed\n",
            "164 stories passed\n",
            "165 stories passed\n",
            "166 stories passed\n",
            "167 stories passed\n",
            "168 stories passed\n",
            "169 stories passed\n",
            "170 stories passed\n",
            "171 stories passed\n",
            "172 stories passed\n",
            "173 stories passed\n",
            "174 stories passed\n",
            "175 stories passed\n",
            "176 stories passed\n",
            "177 stories passed\n",
            "178 stories passed\n",
            "179 stories passed\n",
            "180 stories passed\n",
            "181 stories passed\n",
            "182 stories passed\n",
            "183 stories passed\n",
            "184 stories passed\n",
            "185 stories passed\n",
            "186 stories passed\n",
            "187 stories passed\n",
            "188 stories passed\n",
            "189 stories passed\n",
            "190 stories passed\n",
            "191 stories passed\n",
            "192 stories passed\n",
            "193 stories passed\n",
            "194 stories passed\n",
            "195 stories passed\n",
            "196 stories passed\n",
            "197 stories passed\n",
            "198 stories passed\n",
            "199 stories passed\n",
            "200 stories passed\n",
            "201 stories passed\n",
            "202 stories passed\n",
            "203 stories passed\n",
            "204 stories passed\n",
            "205 stories passed\n",
            "206 stories passed\n",
            "207 stories passed\n",
            "208 stories passed\n",
            "209 stories passed\n",
            "210 stories passed\n",
            "211 stories passed\n",
            "212 stories passed\n",
            "213 stories passed\n",
            "214 stories passed\n",
            "215 stories passed\n",
            "216 stories passed\n",
            "217 stories passed\n",
            "218 stories passed\n",
            "219 stories passed\n",
            "220 stories passed\n",
            "221 stories passed\n",
            "222 stories passed\n",
            "223 stories passed\n",
            "224 stories passed\n",
            "225 stories passed\n",
            "226 stories passed\n",
            "227 stories passed\n",
            "228 stories passed\n",
            "229 stories passed\n",
            "230 stories passed\n",
            "231 stories passed\n",
            "232 stories passed\n",
            "233 stories passed\n",
            "234 stories passed\n",
            "235 stories passed\n",
            "236 stories passed\n",
            "237 stories passed\n",
            "238 stories passed\n",
            "239 stories passed\n",
            "240 stories passed\n",
            "241 stories passed\n",
            "242 stories passed\n",
            "243 stories passed\n",
            "244 stories passed\n",
            "245 stories passed\n",
            "246 stories passed\n",
            "247 stories passed\n",
            "248 stories passed\n",
            "249 stories passed\n",
            "250 stories passed\n",
            "251 stories passed\n",
            "252 stories passed\n",
            "253 stories passed\n",
            "254 stories passed\n",
            "255 stories passed\n",
            "256 stories passed\n",
            "257 stories passed\n",
            "258 stories passed\n",
            "259 stories passed\n",
            "260 stories passed\n",
            "261 stories passed\n",
            "262 stories passed\n",
            "263 stories passed\n",
            "264 stories passed\n",
            "265 stories passed\n",
            "266 stories passed\n",
            "267 stories passed\n",
            "268 stories passed\n",
            "269 stories passed\n",
            "270 stories passed\n",
            "271 stories passed\n",
            "272 stories passed\n",
            "273 stories passed\n",
            "274 stories passed\n",
            "275 stories passed\n",
            "276 stories passed\n",
            "277 stories passed\n",
            "278 stories passed\n",
            "279 stories passed\n",
            "280 stories passed\n",
            "281 stories passed\n",
            "282 stories passed\n",
            "283 stories passed\n",
            "284 stories passed\n",
            "285 stories passed\n",
            "286 stories passed\n",
            "287 stories passed\n",
            "288 stories passed\n",
            "289 stories passed\n",
            "290 stories passed\n",
            "291 stories passed\n",
            "292 stories passed\n",
            "293 stories passed\n",
            "294 stories passed\n",
            "295 stories passed\n",
            "296 stories passed\n",
            "297 stories passed\n",
            "298 stories passed\n",
            "299 stories passed\n",
            "300 stories passed\n",
            "301 stories passed\n",
            "302 stories passed\n",
            "303 stories passed\n",
            "304 stories passed\n",
            "305 stories passed\n",
            "306 stories passed\n",
            "307 stories passed\n",
            "308 stories passed\n",
            "309 stories passed\n",
            "310 stories passed\n",
            "311 stories passed\n",
            "312 stories passed\n",
            "313 stories passed\n",
            "314 stories passed\n",
            "315 stories passed\n",
            "316 stories passed\n",
            "317 stories passed\n",
            "318 stories passed\n",
            "319 stories passed\n",
            "320 stories passed\n",
            "321 stories passed\n",
            "322 stories passed\n",
            "323 stories passed\n",
            "324 stories passed\n",
            "325 stories passed\n",
            "326 stories passed\n",
            "327 stories passed\n",
            "328 stories passed\n",
            "329 stories passed\n",
            "330 stories passed\n",
            "331 stories passed\n",
            "332 stories passed\n",
            "333 stories passed\n",
            "334 stories passed\n",
            "335 stories passed\n",
            "336 stories passed\n",
            "337 stories passed\n",
            "338 stories passed\n",
            "339 stories passed\n",
            "340 stories passed\n",
            "341 stories passed\n",
            "342 stories passed\n",
            "343 stories passed\n",
            "344 stories passed\n",
            "345 stories passed\n",
            "346 stories passed\n",
            "347 stories passed\n",
            "348 stories passed\n",
            "349 stories passed\n",
            "350 stories passed\n",
            "351 stories passed\n",
            "352 stories passed\n",
            "353 stories passed\n",
            "354 stories passed\n",
            "355 stories passed\n",
            "356 stories passed\n",
            "357 stories passed\n",
            "358 stories passed\n",
            "359 stories passed\n",
            "360 stories passed\n",
            "361 stories passed\n",
            "362 stories passed\n",
            "363 stories passed\n",
            "364 stories passed\n",
            "365 stories passed\n",
            "366 stories passed\n",
            "367 stories passed\n",
            "368 stories passed\n",
            "369 stories passed\n",
            "370 stories passed\n",
            "371 stories passed\n",
            "372 stories passed\n",
            "373 stories passed\n",
            "374 stories passed\n",
            "375 stories passed\n",
            "376 stories passed\n",
            "377 stories passed\n",
            "378 stories passed\n",
            "379 stories passed\n",
            "380 stories passed\n",
            "381 stories passed\n",
            "382 stories passed\n",
            "383 stories passed\n",
            "384 stories passed\n",
            "385 stories passed\n",
            "386 stories passed\n",
            "387 stories passed\n",
            "388 stories passed\n",
            "389 stories passed\n",
            "390 stories passed\n",
            "391 stories passed\n",
            "392 stories passed\n",
            "393 stories passed\n",
            "394 stories passed\n",
            "395 stories passed\n",
            "396 stories passed\n",
            "397 stories passed\n",
            "398 stories passed\n",
            "399 stories passed\n",
            "400 stories passed\n",
            "401 stories passed\n",
            "402 stories passed\n",
            "403 stories passed\n",
            "404 stories passed\n",
            "405 stories passed\n",
            "406 stories passed\n",
            "407 stories passed\n",
            "408 stories passed\n",
            "409 stories passed\n",
            "410 stories passed\n",
            "411 stories passed\n",
            "412 stories passed\n",
            "413 stories passed\n",
            "414 stories passed\n",
            "415 stories passed\n",
            "416 stories passed\n",
            "417 stories passed\n",
            "418 stories passed\n",
            "419 stories passed\n",
            "420 stories passed\n",
            "421 stories passed\n",
            "422 stories passed\n",
            "423 stories passed\n",
            "424 stories passed\n",
            "425 stories passed\n",
            "426 stories passed\n",
            "427 stories passed\n",
            "428 stories passed\n",
            "429 stories passed\n",
            "430 stories passed\n",
            "431 stories passed\n",
            "432 stories passed\n",
            "433 stories passed\n",
            "434 stories passed\n",
            "435 stories passed\n",
            "436 stories passed\n",
            "437 stories passed\n",
            "438 stories passed\n",
            "439 stories passed\n",
            "440 stories passed\n",
            "441 stories passed\n",
            "442 stories passed\n",
            "443 stories passed\n",
            "444 stories passed\n",
            "445 stories passed\n",
            "446 stories passed\n",
            "447 stories passed\n",
            "448 stories passed\n",
            "449 stories passed\n",
            "450 stories passed\n",
            "451 stories passed\n",
            "452 stories passed\n",
            "453 stories passed\n",
            "454 stories passed\n",
            "455 stories passed\n",
            "456 stories passed\n",
            "457 stories passed\n",
            "458 stories passed\n",
            "459 stories passed\n",
            "460 stories passed\n",
            "461 stories passed\n",
            "462 stories passed\n",
            "463 stories passed\n",
            "464 stories passed\n",
            "465 stories passed\n",
            "466 stories passed\n",
            "467 stories passed\n",
            "468 stories passed\n",
            "469 stories passed\n",
            "470 stories passed\n",
            "471 stories passed\n",
            "472 stories passed\n",
            "473 stories passed\n",
            "474 stories passed\n",
            "475 stories passed\n",
            "476 stories passed\n",
            "477 stories passed\n",
            "478 stories passed\n",
            "479 stories passed\n",
            "480 stories passed\n",
            "481 stories passed\n",
            "482 stories passed\n",
            "483 stories passed\n",
            "484 stories passed\n",
            "485 stories passed\n",
            "486 stories passed\n",
            "487 stories passed\n",
            "488 stories passed\n",
            "489 stories passed\n",
            "490 stories passed\n",
            "491 stories passed\n",
            "492 stories passed\n",
            "493 stories passed\n",
            "494 stories passed\n",
            "495 stories passed\n",
            "496 stories passed\n",
            "497 stories passed\n",
            "498 stories passed\n",
            "499 stories passed\n",
            "500 stories passed\n",
            "501 stories passed\n",
            "502 stories passed\n",
            "503 stories passed\n",
            "504 stories passed\n",
            "505 stories passed\n",
            "506 stories passed\n",
            "507 stories passed\n",
            "508 stories passed\n",
            "509 stories passed\n",
            "510 stories passed\n",
            "511 stories passed\n",
            "512 stories passed\n",
            "513 stories passed\n",
            "514 stories passed\n",
            "515 stories passed\n",
            "516 stories passed\n",
            "517 stories passed\n",
            "518 stories passed\n",
            "519 stories passed\n",
            "520 stories passed\n",
            "521 stories passed\n",
            "522 stories passed\n",
            "523 stories passed\n",
            "524 stories passed\n",
            "525 stories passed\n",
            "526 stories passed\n",
            "527 stories passed\n",
            "528 stories passed\n",
            "529 stories passed\n",
            "530 stories passed\n",
            "531 stories passed\n",
            "532 stories passed\n",
            "533 stories passed\n",
            "534 stories passed\n",
            "535 stories passed\n",
            "536 stories passed\n",
            "537 stories passed\n",
            "538 stories passed\n",
            "539 stories passed\n",
            "540 stories passed\n",
            "541 stories passed\n",
            "542 stories passed\n",
            "543 stories passed\n",
            "544 stories passed\n",
            "545 stories passed\n",
            "546 stories passed\n",
            "547 stories passed\n",
            "548 stories passed\n",
            "549 stories passed\n",
            "550 stories passed\n",
            "551 stories passed\n",
            "552 stories passed\n",
            "553 stories passed\n",
            "554 stories passed\n",
            "555 stories passed\n",
            "556 stories passed\n",
            "557 stories passed\n",
            "558 stories passed\n",
            "559 stories passed\n",
            "560 stories passed\n",
            "561 stories passed\n",
            "562 stories passed\n",
            "563 stories passed\n",
            "564 stories passed\n",
            "565 stories passed\n",
            "566 stories passed\n",
            "567 stories passed\n",
            "568 stories passed\n",
            "569 stories passed\n",
            "570 stories passed\n",
            "571 stories passed\n",
            "572 stories passed\n",
            "573 stories passed\n",
            "574 stories passed\n",
            "575 stories passed\n",
            "576 stories passed\n",
            "577 stories passed\n",
            "578 stories passed\n",
            "579 stories passed\n",
            "580 stories passed\n",
            "581 stories passed\n",
            "582 stories passed\n",
            "583 stories passed\n",
            "584 stories passed\n",
            "585 stories passed\n",
            "586 stories passed\n",
            "587 stories passed\n",
            "588 stories passed\n",
            "589 stories passed\n",
            "590 stories passed\n",
            "591 stories passed\n",
            "592 stories passed\n",
            "593 stories passed\n",
            "594 stories passed\n",
            "595 stories passed\n",
            "596 stories passed\n",
            "597 stories passed\n",
            "598 stories passed\n",
            "599 stories passed\n",
            "600 stories passed\n",
            "601 stories passed\n",
            "602 stories passed\n",
            "603 stories passed\n",
            "604 stories passed\n",
            "605 stories passed\n",
            "606 stories passed\n",
            "607 stories passed\n",
            "608 stories passed\n",
            "609 stories passed\n",
            "610 stories passed\n",
            "611 stories passed\n",
            "612 stories passed\n",
            "613 stories passed\n",
            "614 stories passed\n",
            "615 stories passed\n",
            "616 stories passed\n",
            "617 stories passed\n",
            "618 stories passed\n",
            "619 stories passed\n",
            "620 stories passed\n",
            "621 stories passed\n",
            "622 stories passed\n",
            "623 stories passed\n",
            "624 stories passed\n",
            "625 stories passed\n",
            "626 stories passed\n",
            "627 stories passed\n",
            "628 stories passed\n",
            "629 stories passed\n",
            "630 stories passed\n",
            "631 stories passed\n",
            "632 stories passed\n",
            "633 stories passed\n",
            "634 stories passed\n",
            "635 stories passed\n",
            "636 stories passed\n",
            "637 stories passed\n",
            "638 stories passed\n",
            "639 stories passed\n",
            "640 stories passed\n",
            "641 stories passed\n",
            "642 stories passed\n",
            "643 stories passed\n",
            "644 stories passed\n",
            "645 stories passed\n",
            "646 stories passed\n",
            "647 stories passed\n",
            "648 stories passed\n",
            "649 stories passed\n",
            "650 stories passed\n",
            "651 stories passed\n",
            "652 stories passed\n",
            "653 stories passed\n",
            "654 stories passed\n",
            "655 stories passed\n",
            "656 stories passed\n",
            "657 stories passed\n",
            "658 stories passed\n",
            "659 stories passed\n",
            "660 stories passed\n",
            "661 stories passed\n",
            "662 stories passed\n",
            "663 stories passed\n",
            "664 stories passed\n",
            "665 stories passed\n",
            "666 stories passed\n",
            "667 stories passed\n",
            "668 stories passed\n",
            "669 stories passed\n",
            "670 stories passed\n",
            "671 stories passed\n",
            "672 stories passed\n",
            "673 stories passed\n",
            "674 stories passed\n",
            "675 stories passed\n",
            "676 stories passed\n",
            "677 stories passed\n",
            "678 stories passed\n",
            "679 stories passed\n",
            "680 stories passed\n",
            "681 stories passed\n",
            "682 stories passed\n",
            "683 stories passed\n",
            "684 stories passed\n",
            "685 stories passed\n",
            "686 stories passed\n",
            "687 stories passed\n",
            "688 stories passed\n",
            "689 stories passed\n",
            "690 stories passed\n",
            "691 stories passed\n",
            "692 stories passed\n",
            "693 stories passed\n",
            "694 stories passed\n",
            "695 stories passed\n",
            "696 stories passed\n",
            "697 stories passed\n",
            "698 stories passed\n",
            "699 stories passed\n",
            "700 stories passed\n",
            "701 stories passed\n",
            "702 stories passed\n",
            "703 stories passed\n",
            "704 stories passed\n",
            "705 stories passed\n",
            "706 stories passed\n",
            "707 stories passed\n",
            "708 stories passed\n",
            "709 stories passed\n",
            "710 stories passed\n",
            "711 stories passed\n",
            "712 stories passed\n",
            "713 stories passed\n",
            "714 stories passed\n",
            "715 stories passed\n",
            "716 stories passed\n",
            "717 stories passed\n",
            "718 stories passed\n",
            "719 stories passed\n",
            "720 stories passed\n",
            "721 stories passed\n",
            "722 stories passed\n",
            "723 stories passed\n",
            "724 stories passed\n",
            "725 stories passed\n",
            "726 stories passed\n",
            "727 stories passed\n",
            "728 stories passed\n",
            "729 stories passed\n",
            "730 stories passed\n",
            "731 stories passed\n",
            "732 stories passed\n",
            "733 stories passed\n",
            "734 stories passed\n",
            "735 stories passed\n",
            "736 stories passed\n",
            "737 stories passed\n",
            "738 stories passed\n",
            "739 stories passed\n",
            "740 stories passed\n",
            "741 stories passed\n",
            "742 stories passed\n",
            "743 stories passed\n",
            "744 stories passed\n",
            "745 stories passed\n",
            "746 stories passed\n",
            "747 stories passed\n",
            "748 stories passed\n",
            "749 stories passed\n",
            "750 stories passed\n",
            "751 stories passed\n",
            "752 stories passed\n",
            "753 stories passed\n",
            "754 stories passed\n",
            "755 stories passed\n",
            "756 stories passed\n",
            "757 stories passed\n",
            "758 stories passed\n",
            "759 stories passed\n",
            "760 stories passed\n",
            "761 stories passed\n",
            "762 stories passed\n",
            "763 stories passed\n",
            "764 stories passed\n",
            "765 stories passed\n",
            "766 stories passed\n",
            "767 stories passed\n",
            "768 stories passed\n",
            "769 stories passed\n",
            "770 stories passed\n",
            "771 stories passed\n",
            "772 stories passed\n",
            "773 stories passed\n",
            "774 stories passed\n",
            "775 stories passed\n",
            "776 stories passed\n",
            "777 stories passed\n",
            "778 stories passed\n",
            "779 stories passed\n",
            "780 stories passed\n",
            "781 stories passed\n",
            "782 stories passed\n",
            "783 stories passed\n",
            "784 stories passed\n",
            "785 stories passed\n",
            "786 stories passed\n",
            "787 stories passed\n",
            "788 stories passed\n",
            "789 stories passed\n",
            "790 stories passed\n",
            "791 stories passed\n",
            "792 stories passed\n",
            "793 stories passed\n",
            "794 stories passed\n",
            "795 stories passed\n",
            "796 stories passed\n",
            "797 stories passed\n",
            "798 stories passed\n",
            "799 stories passed\n",
            "800 stories passed\n",
            "801 stories passed\n",
            "802 stories passed\n",
            "803 stories passed\n",
            "804 stories passed\n",
            "805 stories passed\n",
            "806 stories passed\n",
            "807 stories passed\n",
            "808 stories passed\n",
            "809 stories passed\n",
            "810 stories passed\n",
            "811 stories passed\n",
            "812 stories passed\n",
            "813 stories passed\n",
            "814 stories passed\n",
            "815 stories passed\n",
            "816 stories passed\n",
            "817 stories passed\n",
            "818 stories passed\n",
            "819 stories passed\n",
            "820 stories passed\n",
            "821 stories passed\n",
            "822 stories passed\n",
            "823 stories passed\n",
            "824 stories passed\n",
            "825 stories passed\n",
            "826 stories passed\n",
            "827 stories passed\n",
            "828 stories passed\n",
            "829 stories passed\n",
            "830 stories passed\n",
            "831 stories passed\n",
            "832 stories passed\n",
            "833 stories passed\n",
            "834 stories passed\n",
            "835 stories passed\n",
            "836 stories passed\n",
            "837 stories passed\n",
            "838 stories passed\n",
            "839 stories passed\n",
            "840 stories passed\n",
            "841 stories passed\n",
            "842 stories passed\n",
            "843 stories passed\n",
            "844 stories passed\n",
            "845 stories passed\n",
            "846 stories passed\n",
            "847 stories passed\n",
            "848 stories passed\n",
            "849 stories passed\n",
            "850 stories passed\n",
            "851 stories passed\n",
            "852 stories passed\n",
            "853 stories passed\n",
            "854 stories passed\n",
            "855 stories passed\n",
            "856 stories passed\n",
            "857 stories passed\n",
            "858 stories passed\n",
            "859 stories passed\n",
            "860 stories passed\n",
            "861 stories passed\n",
            "862 stories passed\n",
            "863 stories passed\n",
            "864 stories passed\n",
            "865 stories passed\n",
            "866 stories passed\n",
            "867 stories passed\n",
            "868 stories passed\n",
            "869 stories passed\n",
            "870 stories passed\n",
            "871 stories passed\n",
            "872 stories passed\n",
            "873 stories passed\n",
            "874 stories passed\n",
            "875 stories passed\n",
            "876 stories passed\n",
            "877 stories passed\n",
            "878 stories passed\n",
            "879 stories passed\n",
            "880 stories passed\n",
            "881 stories passed\n",
            "882 stories passed\n",
            "883 stories passed\n",
            "884 stories passed\n",
            "885 stories passed\n",
            "886 stories passed\n",
            "887 stories passed\n",
            "888 stories passed\n",
            "889 stories passed\n",
            "890 stories passed\n",
            "891 stories passed\n",
            "892 stories passed\n",
            "893 stories passed\n",
            "894 stories passed\n",
            "895 stories passed\n",
            "896 stories passed\n",
            "897 stories passed\n",
            "898 stories passed\n",
            "899 stories passed\n",
            "900 stories passed\n",
            "901 stories passed\n",
            "902 stories passed\n",
            "903 stories passed\n",
            "904 stories passed\n",
            "905 stories passed\n",
            "906 stories passed\n",
            "907 stories passed\n",
            "908 stories passed\n",
            "909 stories passed\n",
            "910 stories passed\n",
            "911 stories passed\n",
            "912 stories passed\n",
            "913 stories passed\n",
            "914 stories passed\n",
            "915 stories passed\n",
            "916 stories passed\n",
            "917 stories passed\n",
            "918 stories passed\n",
            "919 stories passed\n",
            "920 stories passed\n",
            "921 stories passed\n",
            "922 stories passed\n",
            "923 stories passed\n",
            "924 stories passed\n",
            "925 stories passed\n",
            "926 stories passed\n",
            "927 stories passed\n",
            "928 stories passed\n",
            "929 stories passed\n",
            "930 stories passed\n",
            "931 stories passed\n",
            "932 stories passed\n",
            "933 stories passed\n",
            "934 stories passed\n",
            "935 stories passed\n",
            "936 stories passed\n",
            "937 stories passed\n",
            "938 stories passed\n",
            "939 stories passed\n",
            "940 stories passed\n",
            "941 stories passed\n",
            "942 stories passed\n",
            "943 stories passed\n",
            "944 stories passed\n",
            "945 stories passed\n",
            "946 stories passed\n",
            "947 stories passed\n",
            "948 stories passed\n",
            "949 stories passed\n",
            "950 stories passed\n",
            "951 stories passed\n",
            "952 stories passed\n",
            "953 stories passed\n",
            "954 stories passed\n",
            "955 stories passed\n",
            "956 stories passed\n",
            "957 stories passed\n",
            "958 stories passed\n",
            "959 stories passed\n",
            "960 stories passed\n",
            "961 stories passed\n",
            "962 stories passed\n",
            "963 stories passed\n",
            "964 stories passed\n",
            "965 stories passed\n",
            "966 stories passed\n",
            "967 stories passed\n",
            "968 stories passed\n",
            "969 stories passed\n",
            "970 stories passed\n",
            "971 stories passed\n",
            "972 stories passed\n",
            "973 stories passed\n",
            "974 stories passed\n",
            "975 stories passed\n",
            "976 stories passed\n",
            "977 stories passed\n",
            "978 stories passed\n",
            "979 stories passed\n",
            "980 stories passed\n",
            "981 stories passed\n",
            "982 stories passed\n",
            "983 stories passed\n",
            "984 stories passed\n",
            "985 stories passed\n",
            "986 stories passed\n",
            "987 stories passed\n",
            "988 stories passed\n",
            "989 stories passed\n",
            "990 stories passed\n",
            "991 stories passed\n",
            "992 stories passed\n",
            "993 stories passed\n",
            "994 stories passed\n",
            "995 stories passed\n",
            "996 stories passed\n",
            "997 stories passed\n",
            "998 stories passed\n",
            "999 stories passed\n",
            "1000 stories passed\n",
            "1001 stories passed\n",
            "1002 stories passed\n",
            "1003 stories passed\n",
            "1004 stories passed\n",
            "1005 stories passed\n",
            "1006 stories passed\n",
            "1007 stories passed\n",
            "1008 stories passed\n",
            "1009 stories passed\n",
            "1010 stories passed\n",
            "1011 stories passed\n",
            "1012 stories passed\n",
            "1013 stories passed\n",
            "1014 stories passed\n",
            "1015 stories passed\n",
            "1016 stories passed\n",
            "1017 stories passed\n",
            "1018 stories passed\n",
            "1019 stories passed\n",
            "1020 stories passed\n",
            "1021 stories passed\n",
            "1022 stories passed\n",
            "1023 stories passed\n",
            "1024 stories passed\n",
            "1025 stories passed\n",
            "1026 stories passed\n",
            "1027 stories passed\n",
            "1028 stories passed\n",
            "1029 stories passed\n",
            "1030 stories passed\n",
            "1031 stories passed\n",
            "1032 stories passed\n",
            "1033 stories passed\n",
            "1034 stories passed\n",
            "1035 stories passed\n",
            "1036 stories passed\n",
            "1037 stories passed\n",
            "1038 stories passed\n",
            "1039 stories passed\n",
            "1040 stories passed\n",
            "1041 stories passed\n",
            "1042 stories passed\n",
            "1043 stories passed\n",
            "1044 stories passed\n",
            "1045 stories passed\n",
            "1046 stories passed\n",
            "1047 stories passed\n",
            "1048 stories passed\n",
            "1049 stories passed\n",
            "1050 stories passed\n",
            "1051 stories passed\n",
            "1052 stories passed\n",
            "1053 stories passed\n",
            "1054 stories passed\n",
            "1055 stories passed\n",
            "1056 stories passed\n",
            "1057 stories passed\n",
            "1058 stories passed\n",
            "1059 stories passed\n",
            "1060 stories passed\n",
            "1061 stories passed\n",
            "1062 stories passed\n",
            "1063 stories passed\n",
            "1064 stories passed\n",
            "1065 stories passed\n",
            "1066 stories passed\n",
            "1067 stories passed\n",
            "1068 stories passed\n",
            "1069 stories passed\n",
            "1070 stories passed\n",
            "1071 stories passed\n",
            "1072 stories passed\n",
            "1073 stories passed\n",
            "1074 stories passed\n",
            "1075 stories passed\n",
            "1076 stories passed\n",
            "1077 stories passed\n",
            "1078 stories passed\n",
            "1079 stories passed\n",
            "1080 stories passed\n",
            "1081 stories passed\n",
            "ROUGE F1 (mean): 0.1240785892931836\n",
            "ROUGE F1 (best): 0.19601828999873142\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTF32RsZzqay"
      },
      "source": [
        "### How did Training affect the loss?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtRymR6qZl7W"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2L2GzF-fsg8"
      },
      "source": [
        "### ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42jbpyOkfsg9",
        "outputId": "3a39d1b3-391d-4b6f-baab-f4d8f9ae3929"
      },
      "source": [
        "naive_model = T5ForConditionalGeneration.from_pretrained('naive_exclusion_model/model')\n",
        "\n",
        "# for scoring outputs\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge_score = []\n",
        "\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "\n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    highlights[1:]\n",
        "\n",
        "    # encode the input\n",
        "    encoded = tokenizer.encode('summarize: ' + story, return_tensors='pt')\n",
        "\n",
        "    # generate the output\n",
        "    output = naive_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
        "                             min_length=30, max_length=300, early_stopping=True)\n",
        "    summary = tokenizer.decode(output[0])\n",
        "    \n",
        "    # get ROUGE scores between the output and highlights\n",
        "    score = rouge.get_scores(summary,'. '.join(highlights[1:])+'.')[0]['rouge-1']['f']\n",
        "\n",
        "    rouge_score.append(np.mean(score))\n",
        "    \n",
        "\n",
        "    if 1%1==0:\n",
        "      print(i+1,'stories passed, ROUGE F1 (mean):', np.mean(rouge_score))\n",
        "\n",
        "print('ROUGE F1 (mean):',np.mean(rouge_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 stories passed, ROUGE F1 (mean): 0.3157894687396123\n",
            "2 stories passed, ROUGE F1 (mean): 0.23632610442747637\n",
            "3 stories passed, ROUGE F1 (mean): 0.20516978242255043\n",
            "4 stories passed, ROUGE F1 (mean): 0.2421126296965745\n",
            "5 stories passed, ROUGE F1 (mean): 0.21869010275899572\n",
            "6 stories passed, ROUGE F1 (mean): 0.21696397370107282\n",
            "7 stories passed, ROUGE F1 (mean): 0.1934879167912639\n",
            "8 stories passed, ROUGE F1 (mean): 0.18319081548710905\n",
            "9 stories passed, ROUGE F1 (mean): 0.19061405768993028\n",
            "10 stories passed, ROUGE F1 (mean): 0.20791628778870583\n",
            "11 stories passed, ROUGE F1 (mean): 0.20987918814633758\n",
            "12 stories passed, ROUGE F1 (mean): 0.19963563235011636\n",
            "13 stories passed, ROUGE F1 (mean): 0.18812519875491512\n",
            "14 stories passed, ROUGE F1 (mean): 0.17961379259618543\n",
            "15 stories passed, ROUGE F1 (mean): 0.1958446676283579\n",
            "16 stories passed, ROUGE F1 (mean): 0.18658056607165358\n",
            "17 stories passed, ROUGE F1 (mean): 0.18847288542756044\n",
            "18 stories passed, ROUGE F1 (mean): 0.19265418394876946\n",
            "19 stories passed, ROUGE F1 (mean): 0.1849624703294226\n",
            "20 stories passed, ROUGE F1 (mean): 0.18084255170447022\n",
            "21 stories passed, ROUGE F1 (mean): 0.18669754750590925\n",
            "22 stories passed, ROUGE F1 (mean): 0.1911983081392729\n",
            "23 stories passed, ROUGE F1 (mean): 0.1846969322513486\n",
            "24 stories passed, ROUGE F1 (mean): 0.18383182762997366\n",
            "25 stories passed, ROUGE F1 (mean): 0.17825633211450312\n",
            "26 stories passed, ROUGE F1 (mean): 0.1803189367638403\n",
            "27 stories passed, ROUGE F1 (mean): 0.17648946029263957\n",
            "28 stories passed, ROUGE F1 (mean): 0.18392252884014465\n",
            "29 stories passed, ROUGE F1 (mean): 0.180453935729996\n",
            "30 stories passed, ROUGE F1 (mean): 0.17582769326845218\n",
            "31 stories passed, ROUGE F1 (mean): 0.17694700351230686\n",
            "32 stories passed, ROUGE F1 (mean): 0.18034598092486864\n",
            "33 stories passed, ROUGE F1 (mean): 0.17834415455621153\n",
            "34 stories passed, ROUGE F1 (mean): 0.1764924032788012\n",
            "35 stories passed, ROUGE F1 (mean): 0.17313043531228378\n",
            "36 stories passed, ROUGE F1 (mean): 0.16990855803478838\n",
            "37 stories passed, ROUGE F1 (mean): 0.16743620154395342\n",
            "38 stories passed, ROUGE F1 (mean): 0.1654970908465848\n",
            "39 stories passed, ROUGE F1 (mean): 0.16512391907172144\n",
            "40 stories passed, ROUGE F1 (mean): 0.17570170332330212\n",
            "41 stories passed, ROUGE F1 (mean): 0.17851163948397142\n",
            "42 stories passed, ROUGE F1 (mean): 0.176331755623497\n",
            "43 stories passed, ROUGE F1 (mean): 0.17975495681322978\n",
            "44 stories passed, ROUGE F1 (mean): 0.18216312326555517\n",
            "45 stories passed, ROUGE F1 (mean): 0.18255949820436507\n",
            "46 stories passed, ROUGE F1 (mean): 0.188351239281555\n",
            "47 stories passed, ROUGE F1 (mean): 0.18927746936061277\n",
            "48 stories passed, ROUGE F1 (mean): 0.1885893969846104\n",
            "49 stories passed, ROUGE F1 (mean): 0.19083262271433563\n",
            "50 stories passed, ROUGE F1 (mean): 0.18775671091190077\n",
            "51 stories passed, ROUGE F1 (mean): 0.1850555988508831\n",
            "52 stories passed, ROUGE F1 (mean): 0.18332833908055174\n",
            "53 stories passed, ROUGE F1 (mean): 0.18356890676708368\n",
            "54 stories passed, ROUGE F1 (mean): 0.18109540840935992\n",
            "55 stories passed, ROUGE F1 (mean): 0.18118542837701904\n",
            "56 stories passed, ROUGE F1 (mean): 0.1793505344551165\n",
            "57 stories passed, ROUGE F1 (mean): 0.17862387647690023\n",
            "58 stories passed, ROUGE F1 (mean): 0.17554415446867783\n",
            "59 stories passed, ROUGE F1 (mean): 0.17503416101083918\n",
            "60 stories passed, ROUGE F1 (mean): 0.17964380663143548\n",
            "61 stories passed, ROUGE F1 (mean): 0.18489554743440376\n",
            "62 stories passed, ROUGE F1 (mean): 0.184914111297641\n",
            "63 stories passed, ROUGE F1 (mean): 0.18381046844021154\n",
            "64 stories passed, ROUGE F1 (mean): 0.18365582109838524\n",
            "65 stories passed, ROUGE F1 (mean): 0.1825720014228075\n",
            "66 stories passed, ROUGE F1 (mean): 0.18475319242266763\n",
            "67 stories passed, ROUGE F1 (mean): 0.18360923587014938\n",
            "68 stories passed, ROUGE F1 (mean): 0.18267380586158835\n",
            "69 stories passed, ROUGE F1 (mean): 0.1818667089942555\n",
            "70 stories passed, ROUGE F1 (mean): 0.18201586582772725\n",
            "71 stories passed, ROUGE F1 (mean): 0.18057902258384378\n",
            "72 stories passed, ROUGE F1 (mean): 0.17807098060351262\n",
            "73 stories passed, ROUGE F1 (mean): 0.17745813605217076\n",
            "74 stories passed, ROUGE F1 (mean): 0.1777246895327677\n",
            "75 stories passed, ROUGE F1 (mean): 0.17910502694062508\n",
            "76 stories passed, ROUGE F1 (mean): 0.1811343466957923\n",
            "77 stories passed, ROUGE F1 (mean): 0.18069180736901236\n",
            "78 stories passed, ROUGE F1 (mean): 0.18135676028170736\n",
            "79 stories passed, ROUGE F1 (mean): 0.18029605408969124\n",
            "80 stories passed, ROUGE F1 (mean): 0.17967278813946802\n",
            "81 stories passed, ROUGE F1 (mean): 0.1791772583962643\n",
            "82 stories passed, ROUGE F1 (mean): 0.17745236309946089\n",
            "83 stories passed, ROUGE F1 (mean): 0.1753143828211541\n",
            "84 stories passed, ROUGE F1 (mean): 0.17455005811441918\n",
            "85 stories passed, ROUGE F1 (mean): 0.17308476325792602\n",
            "86 stories passed, ROUGE F1 (mean): 0.17347792353770208\n",
            "87 stories passed, ROUGE F1 (mean): 0.17446391584555593\n",
            "88 stories passed, ROUGE F1 (mean): 0.17461205312287917\n",
            "89 stories passed, ROUGE F1 (mean): 0.17265011994172322\n",
            "90 stories passed, ROUGE F1 (mean): 0.17355365471373774\n",
            "91 stories passed, ROUGE F1 (mean): 0.1751786537438514\n",
            "92 stories passed, ROUGE F1 (mean): 0.1759919291926652\n",
            "93 stories passed, ROUGE F1 (mean): 0.17550973141739407\n",
            "94 stories passed, ROUGE F1 (mean): 0.1756218247859155\n",
            "95 stories passed, ROUGE F1 (mean): 0.17606982466517643\n",
            "96 stories passed, ROUGE F1 (mean): 0.17527743061449752\n",
            "97 stories passed, ROUGE F1 (mean): 0.17399912768733586\n",
            "98 stories passed, ROUGE F1 (mean): 0.17418594972495755\n",
            "99 stories passed, ROUGE F1 (mean): 0.1747447602501868\n",
            "100 stories passed, ROUGE F1 (mean): 0.17461021582432384\n",
            "101 stories passed, ROUGE F1 (mean): 0.17408883957397372\n",
            "102 stories passed, ROUGE F1 (mean): 0.1732164625021956\n",
            "103 stories passed, ROUGE F1 (mean): 0.17257497114004083\n",
            "104 stories passed, ROUGE F1 (mean): 0.17134294680538475\n",
            "105 stories passed, ROUGE F1 (mean): 0.17147477760052326\n",
            "106 stories passed, ROUGE F1 (mean): 0.17246957283133219\n",
            "107 stories passed, ROUGE F1 (mean): 0.17184147545703155\n",
            "108 stories passed, ROUGE F1 (mean): 0.1712791572233632\n",
            "109 stories passed, ROUGE F1 (mean): 0.17223863343860663\n",
            "110 stories passed, ROUGE F1 (mean): 0.17306517214746242\n",
            "111 stories passed, ROUGE F1 (mean): 0.17345392026458692\n",
            "112 stories passed, ROUGE F1 (mean): 0.17259203769176018\n",
            "113 stories passed, ROUGE F1 (mean): 0.17213734813288642\n",
            "114 stories passed, ROUGE F1 (mean): 0.17235299689123174\n",
            "115 stories passed, ROUGE F1 (mean): 0.17191904888255474\n",
            "116 stories passed, ROUGE F1 (mean): 0.17136063339429908\n",
            "117 stories passed, ROUGE F1 (mean): 0.17056636616618312\n",
            "118 stories passed, ROUGE F1 (mean): 0.17046605928242387\n",
            "119 stories passed, ROUGE F1 (mean): 0.16903357138929423\n",
            "120 stories passed, ROUGE F1 (mean): 0.16876132188985518\n",
            "121 stories passed, ROUGE F1 (mean): 0.1686889142302035\n",
            "122 stories passed, ROUGE F1 (mean): 0.16864994297619856\n",
            "123 stories passed, ROUGE F1 (mean): 0.1682950653507331\n",
            "124 stories passed, ROUGE F1 (mean): 0.1681180201404388\n",
            "125 stories passed, ROUGE F1 (mean): 0.1677282998308279\n",
            "126 stories passed, ROUGE F1 (mean): 0.16719077360449197\n",
            "127 stories passed, ROUGE F1 (mean): 0.16635152386572788\n",
            "128 stories passed, ROUGE F1 (mean): 0.16539912476886018\n",
            "129 stories passed, ROUGE F1 (mean): 0.16411696101096204\n",
            "130 stories passed, ROUGE F1 (mean): 0.16340397336032828\n",
            "131 stories passed, ROUGE F1 (mean): 0.1635879124569634\n",
            "132 stories passed, ROUGE F1 (mean): 0.16270097086964788\n",
            "133 stories passed, ROUGE F1 (mean): 0.16331150819306406\n",
            "134 stories passed, ROUGE F1 (mean): 0.16250735926479906\n",
            "135 stories passed, ROUGE F1 (mean): 0.16159408757611027\n",
            "136 stories passed, ROUGE F1 (mean): 0.16143188752342588\n",
            "137 stories passed, ROUGE F1 (mean): 0.16173814984474674\n",
            "138 stories passed, ROUGE F1 (mean): 0.16118284715008183\n",
            "139 stories passed, ROUGE F1 (mean): 0.1618218194371848\n",
            "140 stories passed, ROUGE F1 (mean): 0.1613685253752856\n",
            "141 stories passed, ROUGE F1 (mean): 0.16108372814747113\n",
            "142 stories passed, ROUGE F1 (mean): 0.1606906255053773\n",
            "143 stories passed, ROUGE F1 (mean): 0.16096551620567534\n",
            "144 stories passed, ROUGE F1 (mean): 0.16168208163904987\n",
            "145 stories passed, ROUGE F1 (mean): 0.16116673292348951\n",
            "146 stories passed, ROUGE F1 (mean): 0.16201979832312205\n",
            "147 stories passed, ROUGE F1 (mean): 0.16313891947294393\n",
            "148 stories passed, ROUGE F1 (mean): 0.1639222359948569\n",
            "149 stories passed, ROUGE F1 (mean): 0.16463598116192169\n",
            "150 stories passed, ROUGE F1 (mean): 0.1645081048905502\n",
            "151 stories passed, ROUGE F1 (mean): 0.16380820700661336\n",
            "152 stories passed, ROUGE F1 (mean): 0.16338841614110935\n",
            "153 stories passed, ROUGE F1 (mean): 0.16339198480189687\n",
            "154 stories passed, ROUGE F1 (mean): 0.1630626605589406\n",
            "155 stories passed, ROUGE F1 (mean): 0.16336887765729147\n",
            "156 stories passed, ROUGE F1 (mean): 0.16267776659097377\n",
            "157 stories passed, ROUGE F1 (mean): 0.16237653629671284\n",
            "158 stories passed, ROUGE F1 (mean): 0.1622694350572159\n",
            "159 stories passed, ROUGE F1 (mean): 0.16446340783113977\n",
            "160 stories passed, ROUGE F1 (mean): 0.16420081762809272\n",
            "161 stories passed, ROUGE F1 (mean): 0.16485317754492546\n",
            "162 stories passed, ROUGE F1 (mean): 0.16522445419595988\n",
            "163 stories passed, ROUGE F1 (mean): 0.164517555676046\n",
            "164 stories passed, ROUGE F1 (mean): 0.16435544271203284\n",
            "165 stories passed, ROUGE F1 (mean): 0.16483754569273001\n",
            "166 stories passed, ROUGE F1 (mean): 0.16453963644428696\n",
            "167 stories passed, ROUGE F1 (mean): 0.16433968374692567\n",
            "168 stories passed, ROUGE F1 (mean): 0.16479004274481301\n",
            "169 stories passed, ROUGE F1 (mean): 0.16623560997470824\n",
            "170 stories passed, ROUGE F1 (mean): 0.16811943863085324\n",
            "171 stories passed, ROUGE F1 (mean): 0.16938549676530895\n",
            "172 stories passed, ROUGE F1 (mean): 0.1687757911166904\n",
            "173 stories passed, ROUGE F1 (mean): 0.16942276197844394\n",
            "174 stories passed, ROUGE F1 (mean): 0.1686701112405212\n",
            "175 stories passed, ROUGE F1 (mean): 0.1686133114833839\n",
            "176 stories passed, ROUGE F1 (mean): 0.1689664700873228\n",
            "177 stories passed, ROUGE F1 (mean): 0.16843035482875593\n",
            "178 stories passed, ROUGE F1 (mean): 0.16957895593821548\n",
            "179 stories passed, ROUGE F1 (mean): 0.16949106316117146\n",
            "180 stories passed, ROUGE F1 (mean): 0.16920304088759675\n",
            "181 stories passed, ROUGE F1 (mean): 0.16911819618543425\n",
            "182 stories passed, ROUGE F1 (mean): 0.16928787640309673\n",
            "183 stories passed, ROUGE F1 (mean): 0.1698877774239725\n",
            "184 stories passed, ROUGE F1 (mean): 0.16966573653096906\n",
            "185 stories passed, ROUGE F1 (mean): 0.16989522556208006\n",
            "186 stories passed, ROUGE F1 (mean): 0.16984896261639085\n",
            "187 stories passed, ROUGE F1 (mean): 0.16928568540496142\n",
            "188 stories passed, ROUGE F1 (mean): 0.1695135339248264\n",
            "189 stories passed, ROUGE F1 (mean): 0.16939093035260266\n",
            "190 stories passed, ROUGE F1 (mean): 0.1687186973595645\n",
            "191 stories passed, ROUGE F1 (mean): 0.16866202738658653\n",
            "192 stories passed, ROUGE F1 (mean): 0.16868937641100853\n",
            "193 stories passed, ROUGE F1 (mean): 0.16893132063070196\n",
            "194 stories passed, ROUGE F1 (mean): 0.17008859535916307\n",
            "195 stories passed, ROUGE F1 (mean): 0.17007104698410186\n",
            "196 stories passed, ROUGE F1 (mean): 0.171904417970554\n",
            "197 stories passed, ROUGE F1 (mean): 0.17122335921994764\n",
            "198 stories passed, ROUGE F1 (mean): 0.1712982236012876\n",
            "199 stories passed, ROUGE F1 (mean): 0.1714843296580092\n",
            "200 stories passed, ROUGE F1 (mean): 0.17087081042625696\n",
            "201 stories passed, ROUGE F1 (mean): 0.17115787815434236\n",
            "202 stories passed, ROUGE F1 (mean): 0.17053558440417155\n",
            "203 stories passed, ROUGE F1 (mean): 0.1707325830618424\n",
            "204 stories passed, ROUGE F1 (mean): 0.17054925338529864\n",
            "205 stories passed, ROUGE F1 (mean): 0.1697173058078094\n",
            "206 stories passed, ROUGE F1 (mean): 0.1697923925771643\n",
            "207 stories passed, ROUGE F1 (mean): 0.16981229908093348\n",
            "208 stories passed, ROUGE F1 (mean): 0.16938835845632733\n",
            "209 stories passed, ROUGE F1 (mean): 0.16997828459927286\n",
            "210 stories passed, ROUGE F1 (mean): 0.1699410649510889\n",
            "211 stories passed, ROUGE F1 (mean): 0.16976756856967246\n",
            "212 stories passed, ROUGE F1 (mean): 0.1714978411759221\n",
            "213 stories passed, ROUGE F1 (mean): 0.17138821818679784\n",
            "214 stories passed, ROUGE F1 (mean): 0.17132906835303546\n",
            "215 stories passed, ROUGE F1 (mean): 0.17161385471523058\n",
            "216 stories passed, ROUGE F1 (mean): 0.17155420795146328\n",
            "217 stories passed, ROUGE F1 (mean): 0.1724564785776205\n",
            "218 stories passed, ROUGE F1 (mean): 0.17266883415756207\n",
            "219 stories passed, ROUGE F1 (mean): 0.17284169935278484\n",
            "220 stories passed, ROUGE F1 (mean): 0.17296514615385292\n",
            "221 stories passed, ROUGE F1 (mean): 0.17327471248503382\n",
            "222 stories passed, ROUGE F1 (mean): 0.17324494649038152\n",
            "223 stories passed, ROUGE F1 (mean): 0.1740778137918327\n",
            "224 stories passed, ROUGE F1 (mean): 0.17374710924400366\n",
            "225 stories passed, ROUGE F1 (mean): 0.17350823318670586\n",
            "226 stories passed, ROUGE F1 (mean): 0.17366232652902286\n",
            "227 stories passed, ROUGE F1 (mean): 0.17317699495203945\n",
            "228 stories passed, ROUGE F1 (mean): 0.1735139379365715\n",
            "229 stories passed, ROUGE F1 (mean): 0.17360829605426664\n",
            "230 stories passed, ROUGE F1 (mean): 0.17329562916088903\n",
            "231 stories passed, ROUGE F1 (mean): 0.17342590706716188\n",
            "232 stories passed, ROUGE F1 (mean): 0.17331695119009266\n",
            "233 stories passed, ROUGE F1 (mean): 0.17379934316526\n",
            "234 stories passed, ROUGE F1 (mean): 0.17374037159221187\n",
            "235 stories passed, ROUGE F1 (mean): 0.17365571532925006\n",
            "236 stories passed, ROUGE F1 (mean): 0.1733050940209492\n",
            "237 stories passed, ROUGE F1 (mean): 0.17341773073559502\n",
            "238 stories passed, ROUGE F1 (mean): 0.1729291808269306\n",
            "239 stories passed, ROUGE F1 (mean): 0.17369330789991053\n",
            "240 stories passed, ROUGE F1 (mean): 0.17318888400936494\n",
            "241 stories passed, ROUGE F1 (mean): 0.17286543673701024\n",
            "242 stories passed, ROUGE F1 (mean): 0.17266764565559586\n",
            "243 stories passed, ROUGE F1 (mean): 0.17382057791090313\n",
            "244 stories passed, ROUGE F1 (mean): 0.17467778340629606\n",
            "245 stories passed, ROUGE F1 (mean): 0.1747349322099399\n",
            "246 stories passed, ROUGE F1 (mean): 0.17540847121767295\n",
            "247 stories passed, ROUGE F1 (mean): 0.17517461932714012\n",
            "248 stories passed, ROUGE F1 (mean): 0.1744682700556597\n",
            "249 stories passed, ROUGE F1 (mean): 0.17399070893540047\n",
            "250 stories passed, ROUGE F1 (mean): 0.1737653343166831\n",
            "251 stories passed, ROUGE F1 (mean): 0.17332599855643066\n",
            "252 stories passed, ROUGE F1 (mean): 0.1734521977693432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gnbi99J6fLmL"
      },
      "source": [
        "## D. Repatriated Highlight\n",
        "\n",
        "Let's put that unwanted highlight back on the end of the input story. This includes the data within the process, but not as an outcome. Can the model learn from this?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bGew_0OaCY8"
      },
      "source": [
        "### Data Formatting\n",
        "\n",
        "Right now, we have a text file for each story. T5 requires a single matrix (a dataset object is perfect) with source/target columns. For this model, the input is the story and one highlight, and the target is now a paragraph composed of the joined remaining highlights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "eDJ6NE6Hf2OO",
        "outputId": "d261ecff-3b40-4b62-dba0-237897abe59c"
      },
      "source": [
        "# collect train data in the format required for this next model\n",
        "source_text_train = []\n",
        "target_text_train = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(train_files)):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[train_files[i],'story']\n",
        "    highlights = all_stories.loc[train_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/single highlight pairs\n",
        "    for j in range(len(highlights)):\n",
        "        source_text_train.append('\\n\\n@highlight\\n\\n '.join([story]+[highlights[j]]))\n",
        "        target_text_train.append('. '.join(highlights[:j]+highlights[j+1:]+[' ']))\n",
        "    \n",
        "    if (i+1)%100 == 0:\n",
        "        print(i+1, \"passed\", end = ', ')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format as a dataset\n",
        "train_df = pd.DataFrame(list(zip(source_text_train, target_text_train)),columns =['source', 'target'])\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "print(train_dataset)\n",
        "train_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 passed, 200 passed, 300 passed, 400 passed, 500 passed, 600 passed, 700 passed, 800 passed, 900 passed, 1000 passed, 1100 passed, 1200 passed, 1300 passed, 1400 passed, 1500 passed, 1600 passed, 1700 passed, 1800 passed, 1900 passed, 2000 passed, 2100 passed, 2200 passed, 2300 passed, 2400 passed, 2500 passed, 2600 passed, 2700 passed, 2800 passed, 2900 passed, 3000 passed, 3100 passed, 3200 passed, 3300 passed, 3400 passed, 3500 passed, 3600 passed, 3700 passed, 3800 passed, 3900 passed, 4000 passed, 4100 passed, 4200 passed, 4300 passed, 4400 passed, 4500 passed, 4600 passed, 4700 passed, 4800 passed, 4900 passed, 5000 passed, 5100 passed, 5200 passed, 5300 passed, 5400 passed, 5500 passed, 5600 passed, 5700 passed, 5800 passed, 5900 passed, 6000 passed, 6100 passed, 6200 passed, 6300 passed, 6400 passed, 6500 passed, 6600 passed, 6700 passed, 6800 passed, 6900 passed, 7000 passed, 7100 passed, 7200 passed, 7300 passed, 7400 passed, 7500 passed, 7600 passed, 7700 passed, 7800 passed, 7900 passed, 8000 passed, 8100 passed, 8200 passed, 8300 passed, 8400 passed, 8500 passed, 8600 passed, 8700 passed, 8800 passed, 8900 passed, 9000 passed, 9100 passed, 9200 passed, 9300 passed, 9400 passed, 9500 passed, 9600 passed, 9700 passed, 9800 passed, 9900 passed, 10000 passed, 10100 passed, 10200 passed, 10300 passed, 10400 passed, 10500 passed, 10600 passed, 10700 passed, 10800 passed, 10900 passed, 11000 passed, 11100 passed, 11200 passed, 11300 passed, 11400 passed, 11500 passed, 11600 passed, 11700 passed, 11800 passed, 11900 passed, 12000 passed, 12100 passed, 12200 passed, 12300 passed, 12400 passed, 12500 passed, 12600 passed, 12700 passed, 12800 passed, 12900 passed, 13000 passed, 13100 passed, 13200 passed, 13300 passed, 13400 passed, 13500 passed, 13600 passed, 13700 passed, 13800 passed, 13900 passed, 14000 passed, 14100 passed, 14200 passed, 14300 passed, 14400 passed, 14500 passed, 14600 passed, 14700 passed, 14800 passed, 14900 passed, 15000 passed, 15100 passed, 15200 passed, 15300 passed, 15400 passed, 15500 passed, 15600 passed, 15700 passed, 15800 passed, 15900 passed, 16000 passed, 16100 passed, 16200 passed, 16300 passed, 16400 passed, 16500 passed, 16600 passed, 16700 passed, 16800 passed, 16900 passed, 17000 passed, 17100 passed, 17200 passed, 17300 passed, 17400 passed, 17500 passed, 17600 passed, 17700 passed, 17800 passed, 17900 passed, 18000 passed, 18100 passed, 18200 passed, 18300 passed, 18400 passed, 18500 passed, 18600 passed, 18700 passed, 18800 passed, 18900 passed, 19000 passed, 19100 passed, 19200 passed, 19300 passed, 19400 passed, 19500 passed, 19600 passed, 19700 passed, 19800 passed, 19900 passed, 20000 passed, 20100 passed, 20200 passed, 20300 passed, 20400 passed, 20500 passed, 20600 passed, 20700 passed, 20800 passed, 20900 passed, 21000 passed, 21100 passed, 21200 passed, 21300 passed, 21400 passed, 21500 passed, 21600 passed, 21700 passed, 21800 passed, 21900 passed, 22000 passed, 22100 passed, 22200 passed, 22300 passed, 22400 passed, 22500 passed, 22600 passed, 22700 passed, 22800 passed, 22900 passed, 23000 passed, 23100 passed, 23200 passed, 23300 passed, 23400 passed, 23500 passed, 23600 passed, 23700 passed, 23800 passed, 23900 passed, 24000 passed, 24100 passed, 24200 passed, 24300 passed, 24400 passed, 24500 passed, 24600 passed, 24700 passed, 24800 passed, 24900 passed, 25000 passed, 25100 passed, 25200 passed, 25300 passed, 25400 passed, 25500 passed, 25600 passed, 25700 passed, 25800 passed, 25900 passed, 26000 passed, 26100 passed, 26200 passed, 26300 passed, 26400 passed, 26500 passed, 26600 passed, 26700 passed, 26800 passed, 26900 passed, 27000 passed, 27100 passed, 27200 passed, 27300 passed, 27400 passed, 27500 passed, 27600 passed, 27700 passed, 27800 passed, 27900 passed, 28000 passed, 28100 passed, 28200 passed, 28300 passed, 28400 passed, 28500 passed, 28600 passed, 28700 passed, 28800 passed, 28900 passed, 29000 passed, 29100 passed, 29200 passed, 29300 passed, 29400 passed, 29500 passed, 29600 passed, 29700 passed, 29800 passed, 29900 passed, 30000 passed, 30100 passed, 30200 passed, 30300 passed, 30400 passed, 30500 passed, 30600 passed, 30700 passed, 30800 passed, 30900 passed, 31000 passed, 31100 passed, 31200 passed, 31300 passed, 31400 passed, 31500 passed, 31600 passed, 31700 passed, 31800 passed, 31900 passed, 32000 passed, 32100 passed, 32200 passed, 32300 passed, 32400 passed, 32500 passed, 32600 passed, 32700 passed, 32800 passed, 32900 passed, 33000 passed, 33100 passed, 33200 passed, 33300 passed, 33400 passed, 33500 passed, 33600 passed, 33700 passed, 33800 passed, 33900 passed, 34000 passed, 34100 passed, 34200 passed, 34300 passed, 34400 passed, 34500 passed, 34600 passed, 34700 passed, 34800 passed, 34900 passed, 35000 passed, 35100 passed, 35200 passed, 35300 passed, 35400 passed, 35500 passed, 35600 passed, 35700 passed, 35800 passed, 35900 passed, 36000 passed, 36100 passed, 36200 passed, 36300 passed, 36400 passed, 36500 passed, 36600 passed, 36700 passed, 36800 passed, 36900 passed, 37000 passed, 37100 passed, 37200 passed, 37300 passed, 37400 passed, 37500 passed, 37600 passed, 37700 passed, 37800 passed, 37900 passed, 38000 passed, 38100 passed, 38200 passed, 38300 passed, 38400 passed, 38500 passed, 38600 passed, 38700 passed, 38800 passed, 38900 passed, 39000 passed, 39100 passed, 39200 passed, 39300 passed, 39400 passed, 39500 passed, 39600 passed, 39700 passed, 39800 passed, 39900 passed, 40000 passed, 40100 passed, 40200 passed, 40300 passed, 40400 passed, 40500 passed, 40600 passed, 40700 passed, 40800 passed, 40900 passed, 41000 passed, 41100 passed, 41200 passed, 41300 passed, 41400 passed, 41500 passed, 41600 passed, 41700 passed, 41800 passed, 41900 passed, 42000 passed, 42100 passed, 42200 passed, 42300 passed, 42400 passed, 42500 passed, 42600 passed, 42700 passed, 42800 passed, 42900 passed, 43000 passed, 43100 passed, 43200 passed, 43300 passed, 43400 passed, 43500 passed, 43600 passed, 43700 passed, 43800 passed, 43900 passed, 44000 passed, 44100 passed, 44200 passed, 44300 passed, 44400 passed, 44500 passed, 44600 passed, 44700 passed, 44800 passed, 44900 passed, 45000 passed, 45100 passed, 45200 passed, 45300 passed, 45400 passed, 45500 passed, 45600 passed, 45700 passed, 45800 passed, 45900 passed, 46000 passed, 46100 passed, 46200 passed, 46300 passed, 46400 passed, 46500 passed, 46600 passed, 46700 passed, 46800 passed, 46900 passed, 47000 passed, 47100 passed, 47200 passed, 47300 passed, 47400 passed, 47500 passed, 47600 passed, 47700 passed, 47800 passed, 47900 passed, 48000 passed, 48100 passed, 48200 passed, 48300 passed, 48400 passed, 48500 passed, 48600 passed, 48700 passed, 48800 passed, 48900 passed, 49000 passed, 49100 passed, 49200 passed, 49300 passed, 49400 passed, 49500 passed, 49600 passed, 49700 passed, 49800 passed, 49900 passed, 50000 passed, 50100 passed, 50200 passed, 50300 passed, 50400 passed, 50500 passed, 50600 passed, 50700 passed, 50800 passed, 50900 passed, 51000 passed, 51100 passed, 51200 passed, 51300 passed, 51400 passed, 51500 passed, 51600 passed, 51700 passed, 51800 passed, 51900 passed, 52000 passed, 52100 passed, 52200 passed, 52300 passed, 52400 passed, 52500 passed, 52600 passed, 52700 passed, 52800 passed, 52900 passed, 53000 passed, 53100 passed, 53200 passed, 53300 passed, 53400 passed, 53500 passed, 53600 passed, 53700 passed, 53800 passed, 53900 passed, 54000 passed, 54100 passed, 54200 passed, 54300 passed, 54400 passed, 54500 passed, 54600 passed, 54700 passed, 54800 passed, 54900 passed, 55000 passed, 55100 passed, 55200 passed, 55300 passed, 55400 passed, 55500 passed, 55600 passed, 55700 passed, 55800 passed, 55900 passed, 56000 passed, 56100 passed, 56200 passed, 56300 passed, 56400 passed, 56500 passed, 56600 passed, 56700 passed, 56800 passed, 56900 passed, 57000 passed, 57100 passed, 57200 passed, 57300 passed, 57400 passed, 57500 passed, 57600 passed, 57700 passed, 57800 passed, 57900 passed, 58000 passed, 58100 passed, 58200 passed, 58300 passed, 58400 passed, 58500 passed, 58600 passed, 58700 passed, 58800 passed, 58900 passed, 59000 passed, 59100 passed, 59200 passed, 59300 passed, 59400 passed, 59500 passed, 59600 passed, 59700 passed, 59800 passed, 59900 passed, 60000 passed, 60100 passed, 60200 passed, 60300 passed, 60400 passed, 60500 passed, 60600 passed, 60700 passed, 60800 passed, 60900 passed, 61000 passed, 61100 passed, 61200 passed, 61300 passed, 61400 passed, 61500 passed, 61600 passed, 61700 passed, 61800 passed, 61900 passed, 62000 passed, 62100 passed, 62200 passed, 62300 passed, 62400 passed, 62500 passed, 62600 passed, 62700 passed, 62800 passed, 62900 passed, 63000 passed, 63100 passed, 63200 passed, 63300 passed, 63400 passed, 63500 passed, 63600 passed, 63700 passed, 63800 passed, 63900 passed, 64000 passed, 64100 passed, 64200 passed, 64300 passed, 64400 passed, 64500 passed, 64600 passed, 64700 passed, 64800 passed, 64900 passed, 65000 passed, 65100 passed, 65200 passed, 65300 passed, 65400 passed, 65500 passed, 65600 passed, 65700 passed, 65800 passed, 65900 passed, 66000 passed, 66100 passed, 66200 passed, 66300 passed, 66400 passed, 66500 passed, 66600 passed, 66700 passed, 66800 passed, 66900 passed, 67000 passed, 67100 passed, 67200 passed, 67300 passed, 67400 passed, 67500 passed, 67600 passed, 67700 passed, 67800 passed, 67900 passed, 68000 passed, 68100 passed, 68200 passed, 68300 passed, 68400 passed, 68500 passed, 68600 passed, 68700 passed, 68800 passed, 68900 passed, 69000 passed, 69100 passed, 69200 passed, 69300 passed, 69400 passed, 69500 passed, 69600 passed, 69700 passed, 69800 passed, 69900 passed, 70000 passed, 70100 passed, 70200 passed, 70300 passed, 70400 passed, 70500 passed, 70600 passed, 70700 passed, 70800 passed, 70900 passed, 71000 passed, 71100 passed, 71200 passed, 71300 passed, 71400 passed, 71500 passed, 71600 passed, 71700 passed, 71800 passed, 71900 passed, 72000 passed, 72100 passed, 72200 passed, 72300 passed, 72400 passed, 72500 passed, 72600 passed, 72700 passed, 72800 passed, 72900 passed, 73000 passed, 73100 passed, 73200 passed, 73300 passed, 73400 passed, 73500 passed, 73600 passed, 73700 passed, 73800 passed, 73900 passed, 74000 passed, 74100 passed, 74200 passed, 74300 passed, 74400 passed, 74500 passed, 74600 passed, 74700 passed, 74800 passed, 74900 passed, 75000 passed, 75100 passed, 75200 passed, 75300 passed, 75400 passed, 75500 passed, 75600 passed, 75700 passed, 75800 passed, 75900 passed, 76000 passed, 76100 passed, 76200 passed, 76300 passed, 76400 passed, 76500 passed, 76600 passed, 76700 passed, 76800 passed, 76900 passed, 77000 passed, 77100 passed, 77200 passed, 77300 passed, 77400 passed, 77500 passed, 77600 passed, 77700 passed, 77800 passed, 77900 passed, 78000 passed, 78100 passed, 78200 passed, 78300 passed, 78400 passed, 78500 passed, 78600 passed, 78700 passed, 78800 passed, 78900 passed, 79000 passed, 79100 passed, 79200 passed, 79300 passed, 79400 passed, 79500 passed, 79600 passed, 79700 passed, 79800 passed, 79900 passed, 80000 passed, 80100 passed, 80200 passed, 80300 passed, 80400 passed, 80500 passed, 80600 passed, 80700 passed, 80800 passed, 80900 passed, 81000 passed, 81100 passed, 81200 passed, 81300 passed, 81400 passed, 81500 passed, 81600 passed, 81700 passed, 81800 passed, 81900 passed, 82000 passed, 82100 passed, 82200 passed, 82300 passed, 82400 passed, 82500 passed, 82600 passed, 82700 passed, 82800 passed, 82900 passed, 83000 passed, 83100 passed, 83200 passed, 83300 passed, 83400 passed, 83500 passed, 83600 passed, 83700 passed, 83800 passed, 83900 passed, 84000 passed, 84100 passed, 84200 passed, 84300 passed, 84400 passed, 84500 passed, 84600 passed, 84700 passed, 84800 passed, 84900 passed, 85000 passed, 85100 passed, 85200 passed, 85300 passed, 85400 passed, 85500 passed, 85600 passed, 85700 passed, 85800 passed, 85900 passed, 86000 passed, 86100 passed, 86200 passed, 86300 passed, 86400 passed, 86500 passed, 86600 passed, 86700 passed, 86800 passed, 86900 passed, 87000 passed, 87100 passed, 87200 passed, 87300 passed, 87400 passed, 87500 passed, 87600 passed, 87700 passed, 87800 passed, 87900 passed, 88000 passed, 88100 passed, 88200 passed, 88300 passed, 88400 passed, 88500 passed, 88600 passed, 88700 passed, 88800 passed, 88900 passed, 89000 passed, 89100 passed, 89200 passed, 89300 passed, 89400 passed, 89500 passed, 89600 passed, 89700 passed, 89800 passed, 89900 passed, \n",
            "\n",
            "time: 0.06575791438420614 minutes\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 322487\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Obama sends a letter to the heads of the House...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Syrian official : Obama climbed to the top of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Syrian official : Obama climbed to the top of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>It 's official : U.S. President Barack Obama w...</td>\n",
              "      <td>Syrian official : Obama climbed to the top of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-LRB- CNN -RRB- -- Usain Bolt rounded off the ...</td>\n",
              "      <td>Anchors Jamaica to 4x100m relay victory. Eight...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              source                                             target\n",
              "0  It 's official : U.S. President Barack Obama w...  Obama sends a letter to the heads of the House...\n",
              "1  It 's official : U.S. President Barack Obama w...  Syrian official : Obama climbed to the top of ...\n",
              "2  It 's official : U.S. President Barack Obama w...  Syrian official : Obama climbed to the top of ...\n",
              "3  It 's official : U.S. President Barack Obama w...  Syrian official : Obama climbed to the top of ...\n",
              "4  -LRB- CNN -RRB- -- Usain Bolt rounded off the ...  Anchors Jamaica to 4x100m relay victory. Eight..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P5-chQWf24k",
        "outputId": "6ef0399a-4a49-4e41-d86f-d2d860240fb7"
      },
      "source": [
        "# collect val data in the format required for this next model\n",
        "source_text_val = []\n",
        "target_text_val = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(valid_files)):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    for j in range(len(highlights)):\n",
        "        source_text_val.append('\\n\\n@highlight\\n\\n '.join([story]+[highlights[j]]))\n",
        "        target_text_val.append('. '.join(highlights[:j]+highlights[j+1:]+[' ']))\n",
        "    \n",
        "    if (i+1)%100 == 0:\n",
        "        print(i+1, \"passed\", end = ', ')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format as a dataset\n",
        "val_df = pd.DataFrame(list(zip(source_text_val, target_text_val)),columns =['source', 'target'])\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "print(val_dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100 passed, 200 passed, 300 passed, 400 passed, 500 passed, 600 passed, 700 passed, 800 passed, 900 passed, 1000 passed, 1100 passed, \n",
            "\n",
            "time: 0.0007175048192342123 minutes\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 3054\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5UU8C2ggYsQ",
        "outputId": "4f085eab-ec1a-4357-cc75-88f0addc763e"
      },
      "source": [
        "print('='*60+'\\nEXAMPLE INPUT TEXT\\n'+'='*60)\n",
        "print(source_text_train[0])\n",
        "print('='*60+'\\nEXAMPLE TARGET TEXT\\n'+'='*60)\n",
        "print(target_text_train[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EXAMPLE INPUT TEXT\n",
            "============================================================\n",
            "It 's official : U.S. President Barack Obama wants lawmakers to weigh in on whether to use military force in Syria .\n",
            "\n",
            "Obama sent a letter to the heads of the House and Senate on Saturday night , hours after announcing that he believes military action against Syrian targets is the right step to take over the alleged use of chemical weapons .\n",
            "\n",
            "The proposed legislation from Obama asks Congress to approve the use of military force `` to deter , disrupt , prevent and degrade the potential for future uses of chemical weapons or other weapons of mass destruction . ''\n",
            "\n",
            "It 's a step that is set to turn an international crisis into a fierce domestic political battle .\n",
            "\n",
            "There are key questions looming over the debate : What did U.N. weapons inspectors find in Syria ? What happens if Congress votes no ? And how will the Syrian government react ?\n",
            "\n",
            "In a televised address from the White House Rose Garden earlier Saturday , the president said he would take his case to Congress , not because he has to -- but because he wants to .\n",
            "\n",
            "`` While I believe I have the authority to carry out this military action without specific congressional authorization , I know that the country will be stronger if we take this course , and our actions will be even more effective , '' he said . `` We should have this debate , because the issues are too big for business as usual . ''\n",
            "\n",
            "Obama said top congressional leaders had agreed to schedule a debate when the body returns to Washington on September 9 . The Senate Foreign Relations Committee will hold a hearing over the matter on Tuesday , Sen. Robert Menendez said .\n",
            "\n",
            "Transcript : Read Obama 's full remarks\n",
            "\n",
            "Syrian crisis : Latest developments\n",
            "\n",
            "U.N. inspectors leave Syria\n",
            "\n",
            "Obama 's remarks came shortly after U.N. inspectors left Syria , carrying evidence that will determine whether chemical weapons were used in an attack early last week in a Damascus suburb .\n",
            "\n",
            "`` The aim of the game here , the mandate , is very clear -- and that is to ascertain whether chemical weapons were used -- and not by whom , '' U.N. spokesman Martin Nesirky told reporters on Saturday .\n",
            "\n",
            "But who used the weapons in the reported toxic gas attack in a Damascus suburb on August 21 has been a key point of global debate over the Syrian crisis .\n",
            "\n",
            "Top U.S. officials have said there 's no doubt that the Syrian government was behind it , while Syrian officials have denied responsibility and blamed jihadists fighting with the rebels .\n",
            "\n",
            "British and U.S. intelligence reports say the attack involved chemical weapons , but U.N. officials have stressed the importance of waiting for an official report from inspectors .\n",
            "\n",
            "The inspectors will share their findings with U.N. Secretary-General Ban Ki-moon Ban , who has said he wants to wait until the U.N. team 's final report is completed before presenting it to the U.N. Security Council .\n",
            "\n",
            "The Organization for the Prohibition of Chemical Weapons , which nine of the inspectors belong to , said Saturday that it could take up to three weeks to analyze the evidence they collected .\n",
            "\n",
            "`` It needs time to be able to analyze the information and the samples , '' Nesirky said .\n",
            "\n",
            "He noted that Ban has repeatedly said there is no alternative to a political solution to the crisis in Syria , and that `` a military solution is not an option . ''\n",
            "\n",
            "Bergen : Syria is a problem from hell for the U.S.\n",
            "\n",
            "Obama : ` This menace must be confronted '\n",
            "\n",
            "Obama 's senior advisers have debated the next steps to take , and the president 's comments Saturday came amid mounting political pressure over the situation in Syria . Some U.S. lawmakers have called for immediate action while others warn of stepping into what could become a quagmire .\n",
            "\n",
            "Some global leaders have expressed support , but the British Parliament 's vote against military action earlier this week was a blow to Obama 's hopes of getting strong backing from key NATO allies .\n",
            "\n",
            "On Saturday , Obama proposed what he said would be a limited military action against Syrian President Bashar al-Assad . Any military attack would not be open-ended or include U.S. ground forces , he said .\n",
            "\n",
            "Syria 's alleged use of chemical weapons earlier this month `` is an assault on human dignity , '' the president said .\n",
            "\n",
            "A failure to respond with force , Obama argued , `` could lead to escalating use of chemical weapons or their proliferation to terrorist groups who would do our people harm . In a world with many dangers , this menace must be confronted . ''\n",
            "\n",
            "Syria missile strike : What would happen next ?\n",
            "\n",
            "Map : U.S. and allied assets around Syria\n",
            "\n",
            "Obama decision came Friday night\n",
            "\n",
            "On Friday night , the president made a last-minute decision to consult lawmakers .\n",
            "\n",
            "What will happen if they vote no ?\n",
            "\n",
            "It 's unclear . A senior administration official told CNN that Obama has the authority to act without Congress -- even if Congress rejects his request for authorization to use force .\n",
            "\n",
            "Obama on Saturday continued to shore up support for a strike on the al-Assad government .\n",
            "\n",
            "He spoke by phone with French President Francois Hollande before his Rose Garden speech .\n",
            "\n",
            "`` The two leaders agreed that the international community must deliver a resolute message to the Assad regime -- and others who would consider using chemical weapons -- that these crimes are unacceptable and those who violate this international norm will be held accountable by the world , '' the White House said .\n",
            "\n",
            "Meanwhile , as uncertainty loomed over how Congress would weigh in , U.S. military officials said they remained at the ready .\n",
            "\n",
            "5 key assertions : U.S. intelligence report on Syria\n",
            "\n",
            "Syria : Who wants what after chemical weapons horror\n",
            "\n",
            "Reactions mixed to Obama 's speech\n",
            "\n",
            "A spokesman for the Syrian National Coalition said that the opposition group was disappointed by Obama 's announcement .\n",
            "\n",
            "`` Our fear now is that the lack of action could embolden the regime and they repeat his attacks in a more serious way , '' said spokesman Louay Safi . `` So we are quite concerned . ''\n",
            "\n",
            "Some members of Congress applauded Obama 's decision .\n",
            "\n",
            "House Speaker John Boehner , Majority Leader Eric Cantor , Majority Whip Kevin McCarthy and Conference Chair Cathy McMorris Rodgers issued a statement Saturday praising the president .\n",
            "\n",
            "`` Under the Constitution , the responsibility to declare war lies with Congress , '' the Republican lawmakers said . `` We are glad the president is seeking authorization for any military action in Syria in response to serious , substantive questions being raised . ''\n",
            "\n",
            "More than 160 legislators , including 63 of Obama 's fellow Democrats , had signed letters calling for either a vote or at least a `` full debate '' before any U.S. action .\n",
            "\n",
            "British Prime Minister David Cameron , whose own attempt to get lawmakers in his country to support military action in Syria failed earlier this week , responded to Obama 's speech in a Twitter post Saturday .\n",
            "\n",
            "`` I understand and support Barack Obama 's position on Syria , '' Cameron said .\n",
            "\n",
            "An influential lawmaker in Russia -- which has stood by Syria and criticized the United States -- had his own theory .\n",
            "\n",
            "`` The main reason Obama is turning to the Congress : the military operation did not get enough support either in the world , among allies of the US or in the United States itself , '' Alexei Pushkov , chairman of the international-affairs committee of the Russian State Duma , said in a Twitter post .\n",
            "\n",
            "In the United States , scattered groups of anti-war protesters around the country took to the streets Saturday .\n",
            "\n",
            "`` Like many other Americans ... we 're just tired of the United States getting involved and invading and bombing other countries , '' said Robin Rosecrans , who was among hundreds at a Los Angeles demonstration .\n",
            "\n",
            "What do Syria 's neighbors think ?\n",
            "\n",
            "Why Russia , China , Iran stand by Assad\n",
            "\n",
            "Syria 's government unfazed\n",
            "\n",
            "After Obama 's speech , a military and political analyst on Syrian state TV said Obama is `` embarrassed '' that Russia opposes military action against Syria , is `` crying for help '' for someone to come to his rescue and is facing two defeats -- on the political and military levels .\n",
            "\n",
            "Syria 's prime minister appeared unfazed by the saber-rattling .\n",
            "\n",
            "`` The Syrian Army 's status is on maximum readiness and fingers are on the trigger to confront all challenges , '' Wael Nader al-Halqi said during a meeting with a delegation of Syrian expatriates from Italy , according to a banner on Syria State TV that was broadcast prior to Obama 's address .\n",
            "\n",
            "An anchor on Syrian state television said Obama `` appeared to be preparing for an aggression on Syria based on repeated lies . ''\n",
            "\n",
            "A top Syrian diplomat told the state television network that Obama was facing pressure to take military action from Israel , Turkey , some Arabs and right-wing extremists in the United States .\n",
            "\n",
            "`` I think he has done well by doing what Cameron did in terms of taking the issue to Parliament , '' said Bashar Jaafari , Syria 's ambassador to the United Nations .\n",
            "\n",
            "Both Obama and Cameron , he said , `` climbed to the top of the tree and do n't know how to get down . ''\n",
            "\n",
            "The Syrian government has denied that it used chemical weapons in the August 21 attack , saying that jihadists fighting with the rebels used them in an effort to turn global sentiments against it .\n",
            "\n",
            "British intelligence had put the number of people killed in the attack at more than 350 .\n",
            "\n",
            "On Saturday , Obama said `` all told , well over 1,000 people were murdered . '' U.S. Secretary of State John Kerry on Friday cited a death toll of 1,429 , more than 400 of them children . No explanation was offered for the discrepancy .\n",
            "\n",
            "Iran : U.S. military action in Syria would spark ` disaster '\n",
            "\n",
            "Opinion : Why strikes in Syria are a bad idea\n",
            "\n",
            "@highlight\n",
            "\n",
            " Syrian official : Obama climbed to the top of the tree , `` does n't know how to get down ''\n",
            "============================================================\n",
            "EXAMPLE TARGET TEXT\n",
            "============================================================\n",
            "Obama sends a letter to the heads of the House and Senate. Obama to seek congressional approval on military action against Syria. Aim is to determine whether CW were used , not by whom , says U.N. spokesman.  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GM0YEiJRgniX"
      },
      "source": [
        "### Tokenize"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184,
          "referenced_widgets": [
            "e6551e8ae4f542e39bb1841a3df4289c",
            "bb82538a63e24dcea110650c47f45442",
            "2636714e2c0742769f88dcd500b591c6",
            "bcfba8465f434c709eafdcff8653d8cf",
            "cf3572056e724d61b8fe278c34e61cd6",
            "58882dbfb96444c2a02b1c3296339347",
            "2030914dfe7e406898a0f5785adc072d",
            "4e3c59a1fcf749498053774e9f08f202",
            "19e2deb800664bf78666c478ff3fb9a0",
            "6766a437be27417bb68d1f6f06856aad",
            "19140313bca44d528977f769cd7ab357",
            "320ff669467d4d7d8837e8086044c79c",
            "5fe775ee22ae451ab0c16f145cd3c622",
            "17559f9c1ac24aefa255001547bff8bd",
            "6429722109fb41b3807ff6c8cbb91635",
            "2d090dba847448f38f9690f421113612"
          ]
        },
        "id": "RkBDj8M1gpa3",
        "outputId": "8ee14947-5e87-4328-da73-9ddcdb45824f"
      },
      "source": [
        "train_tokenized = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
        "val_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "print(val_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6551e8ae4f542e39bb1841a3df4289c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=630.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "19e2deb800664bf78666c478ff3fb9a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset({\n",
            "    features: ['attention_mask', 'input_ids', 'labels', 'source', 'target'],\n",
            "    num_rows: 3054\n",
            "})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULH3KF9DgqMM"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7FvOUN5ag2Ew",
        "outputId": "930c63b8-ef3b-4fae-97e4-0bf3164f3ee5"
      },
      "source": [
        "# training this one on 2 epochs takes about 15 hours\n",
        "start = time.time()\n",
        "\n",
        "output_dir = 'exclude_highlights_model'\n",
        "\n",
        "# training arguments to feed to Trainer object\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = output_dir, # trained model will be saved here\n",
        "    num_train_epochs = 2,\n",
        "    per_device_train_batch_size = 8, # number of examples per batch\n",
        "    per_device_eval_batch_size = 8, # number of examples per batch\n",
        "    eval_accumulation_steps = 1,\n",
        "    prediction_loss_only = True,\n",
        "    learning_rate = 0.001,\n",
        "    evaluation_strategy = 'steps',\n",
        "    save_steps = 10,\n",
        "    save_total_limit = 1,\n",
        "    remove_unused_columns = True,\n",
        "    run_name = 'run_name',\n",
        "    logging_steps = 500, # print loss after this many steps\n",
        "    eval_steps = 500, # calculate loss after this many steps\n",
        "    logging_first_step = False,\n",
        "    load_best_model_at_end = True,\n",
        "    metric_for_best_model = \"loss\", \n",
        "    greater_is_better = False\n",
        ")\n",
        "\n",
        "# create Trainer to feed the train/dev data\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_tokenized,\n",
        "    eval_dataset = val_tokenized\n",
        ")\n",
        "\n",
        "# train the model and save it to our directory\n",
        "trainer.train()\n",
        "trainer.save_model(output_dir + '/model')\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60, 'minutes')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='72583' max='80622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [72583/80622 13:32:04 < 1:29:56, 1.49 it/s, Epoch 1.80/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.236200</td>\n",
              "      <td>0.162231</td>\n",
              "      <td>62.833300</td>\n",
              "      <td>48.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.245600</td>\n",
              "      <td>0.158533</td>\n",
              "      <td>62.289200</td>\n",
              "      <td>49.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.241600</td>\n",
              "      <td>0.158035</td>\n",
              "      <td>62.804400</td>\n",
              "      <td>48.627000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.237600</td>\n",
              "      <td>0.157259</td>\n",
              "      <td>62.861700</td>\n",
              "      <td>48.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.237800</td>\n",
              "      <td>0.155415</td>\n",
              "      <td>62.741700</td>\n",
              "      <td>48.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.235600</td>\n",
              "      <td>0.155988</td>\n",
              "      <td>62.769900</td>\n",
              "      <td>48.654000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.235300</td>\n",
              "      <td>0.155270</td>\n",
              "      <td>62.789000</td>\n",
              "      <td>48.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.234200</td>\n",
              "      <td>0.154215</td>\n",
              "      <td>63.701600</td>\n",
              "      <td>47.942000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.235000</td>\n",
              "      <td>0.155944</td>\n",
              "      <td>62.922800</td>\n",
              "      <td>48.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>0.155442</td>\n",
              "      <td>62.928300</td>\n",
              "      <td>48.531000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.153418</td>\n",
              "      <td>62.936800</td>\n",
              "      <td>48.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.233100</td>\n",
              "      <td>0.152663</td>\n",
              "      <td>62.863900</td>\n",
              "      <td>48.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.231900</td>\n",
              "      <td>0.153689</td>\n",
              "      <td>63.780300</td>\n",
              "      <td>47.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.231200</td>\n",
              "      <td>0.153285</td>\n",
              "      <td>63.117600</td>\n",
              "      <td>48.386000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.152401</td>\n",
              "      <td>62.868100</td>\n",
              "      <td>48.578000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.230500</td>\n",
              "      <td>0.154449</td>\n",
              "      <td>62.501400</td>\n",
              "      <td>48.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.154772</td>\n",
              "      <td>61.670800</td>\n",
              "      <td>49.521000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.152490</td>\n",
              "      <td>61.671000</td>\n",
              "      <td>49.521000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.152547</td>\n",
              "      <td>62.474600</td>\n",
              "      <td>48.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.227600</td>\n",
              "      <td>0.153690</td>\n",
              "      <td>61.732900</td>\n",
              "      <td>49.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.224200</td>\n",
              "      <td>0.152178</td>\n",
              "      <td>61.808900</td>\n",
              "      <td>49.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.227100</td>\n",
              "      <td>0.152827</td>\n",
              "      <td>61.414800</td>\n",
              "      <td>49.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.224000</td>\n",
              "      <td>0.152351</td>\n",
              "      <td>61.423500</td>\n",
              "      <td>49.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.225300</td>\n",
              "      <td>0.150647</td>\n",
              "      <td>61.341500</td>\n",
              "      <td>49.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.223100</td>\n",
              "      <td>0.151268</td>\n",
              "      <td>61.388600</td>\n",
              "      <td>49.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>0.150695</td>\n",
              "      <td>61.418100</td>\n",
              "      <td>49.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.223900</td>\n",
              "      <td>0.150777</td>\n",
              "      <td>61.692500</td>\n",
              "      <td>49.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.150068</td>\n",
              "      <td>61.552200</td>\n",
              "      <td>49.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.220700</td>\n",
              "      <td>0.150723</td>\n",
              "      <td>61.602300</td>\n",
              "      <td>49.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.150885</td>\n",
              "      <td>61.893500</td>\n",
              "      <td>49.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.220900</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>61.600600</td>\n",
              "      <td>49.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.222100</td>\n",
              "      <td>0.150836</td>\n",
              "      <td>61.623700</td>\n",
              "      <td>49.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.150737</td>\n",
              "      <td>62.354800</td>\n",
              "      <td>48.978000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.219900</td>\n",
              "      <td>0.149742</td>\n",
              "      <td>61.835800</td>\n",
              "      <td>49.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.149997</td>\n",
              "      <td>61.893600</td>\n",
              "      <td>49.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.221300</td>\n",
              "      <td>0.150722</td>\n",
              "      <td>62.126200</td>\n",
              "      <td>49.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.222200</td>\n",
              "      <td>0.150486</td>\n",
              "      <td>62.016400</td>\n",
              "      <td>49.245000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.218900</td>\n",
              "      <td>0.149428</td>\n",
              "      <td>61.823000</td>\n",
              "      <td>49.399000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.217900</td>\n",
              "      <td>0.149347</td>\n",
              "      <td>61.555600</td>\n",
              "      <td>49.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.215200</td>\n",
              "      <td>0.149564</td>\n",
              "      <td>61.806400</td>\n",
              "      <td>49.412000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.218700</td>\n",
              "      <td>0.148446</td>\n",
              "      <td>61.759000</td>\n",
              "      <td>49.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.217600</td>\n",
              "      <td>0.148917</td>\n",
              "      <td>62.153200</td>\n",
              "      <td>49.137000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.220900</td>\n",
              "      <td>0.148894</td>\n",
              "      <td>61.949800</td>\n",
              "      <td>49.298000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>0.148455</td>\n",
              "      <td>62.037100</td>\n",
              "      <td>49.229000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>0.148681</td>\n",
              "      <td>61.905800</td>\n",
              "      <td>49.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>0.148834</td>\n",
              "      <td>62.084200</td>\n",
              "      <td>49.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.216100</td>\n",
              "      <td>0.148107</td>\n",
              "      <td>62.204800</td>\n",
              "      <td>49.096000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.216300</td>\n",
              "      <td>0.147616</td>\n",
              "      <td>61.813800</td>\n",
              "      <td>49.406000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.212300</td>\n",
              "      <td>0.148385</td>\n",
              "      <td>63.092900</td>\n",
              "      <td>48.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.214100</td>\n",
              "      <td>0.147897</td>\n",
              "      <td>62.216100</td>\n",
              "      <td>49.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.215300</td>\n",
              "      <td>0.147586</td>\n",
              "      <td>62.861200</td>\n",
              "      <td>48.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.213200</td>\n",
              "      <td>0.147526</td>\n",
              "      <td>62.996900</td>\n",
              "      <td>48.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.147377</td>\n",
              "      <td>62.988600</td>\n",
              "      <td>48.485000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.215400</td>\n",
              "      <td>0.148235</td>\n",
              "      <td>62.872300</td>\n",
              "      <td>48.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.215400</td>\n",
              "      <td>0.147941</td>\n",
              "      <td>63.005800</td>\n",
              "      <td>48.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.213300</td>\n",
              "      <td>0.147397</td>\n",
              "      <td>62.290300</td>\n",
              "      <td>49.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.213200</td>\n",
              "      <td>0.146013</td>\n",
              "      <td>61.664800</td>\n",
              "      <td>49.526000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.214200</td>\n",
              "      <td>0.147023</td>\n",
              "      <td>61.512300</td>\n",
              "      <td>49.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.147914</td>\n",
              "      <td>61.487600</td>\n",
              "      <td>49.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.214800</td>\n",
              "      <td>0.146741</td>\n",
              "      <td>61.817000</td>\n",
              "      <td>49.404000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.212900</td>\n",
              "      <td>0.146957</td>\n",
              "      <td>61.771600</td>\n",
              "      <td>49.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.211200</td>\n",
              "      <td>0.146969</td>\n",
              "      <td>61.623500</td>\n",
              "      <td>49.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.146415</td>\n",
              "      <td>61.602200</td>\n",
              "      <td>49.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.211000</td>\n",
              "      <td>0.145922</td>\n",
              "      <td>61.963100</td>\n",
              "      <td>49.287000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.212400</td>\n",
              "      <td>0.146003</td>\n",
              "      <td>61.763700</td>\n",
              "      <td>49.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.210700</td>\n",
              "      <td>0.145638</td>\n",
              "      <td>61.893700</td>\n",
              "      <td>49.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.210800</td>\n",
              "      <td>0.145352</td>\n",
              "      <td>61.939400</td>\n",
              "      <td>49.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.207300</td>\n",
              "      <td>0.145926</td>\n",
              "      <td>62.364600</td>\n",
              "      <td>48.970000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.209700</td>\n",
              "      <td>0.145065</td>\n",
              "      <td>61.089200</td>\n",
              "      <td>49.993000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.208100</td>\n",
              "      <td>0.146048</td>\n",
              "      <td>62.096700</td>\n",
              "      <td>49.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.206900</td>\n",
              "      <td>0.145184</td>\n",
              "      <td>61.984400</td>\n",
              "      <td>49.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.208300</td>\n",
              "      <td>0.144818</td>\n",
              "      <td>61.978800</td>\n",
              "      <td>49.275000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.206500</td>\n",
              "      <td>0.146101</td>\n",
              "      <td>62.083500</td>\n",
              "      <td>49.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.206900</td>\n",
              "      <td>0.146314</td>\n",
              "      <td>61.931200</td>\n",
              "      <td>49.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.210700</td>\n",
              "      <td>0.145239</td>\n",
              "      <td>61.898000</td>\n",
              "      <td>49.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.205600</td>\n",
              "      <td>0.144952</td>\n",
              "      <td>62.129400</td>\n",
              "      <td>49.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.144559</td>\n",
              "      <td>62.102200</td>\n",
              "      <td>49.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.207200</td>\n",
              "      <td>0.143776</td>\n",
              "      <td>62.317900</td>\n",
              "      <td>49.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.207000</td>\n",
              "      <td>0.144248</td>\n",
              "      <td>62.531300</td>\n",
              "      <td>48.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>0.143938</td>\n",
              "      <td>62.343500</td>\n",
              "      <td>48.987000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>0.202200</td>\n",
              "      <td>0.144140</td>\n",
              "      <td>62.866800</td>\n",
              "      <td>48.579000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.199400</td>\n",
              "      <td>0.144126</td>\n",
              "      <td>62.513200</td>\n",
              "      <td>48.854000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.143201</td>\n",
              "      <td>62.622600</td>\n",
              "      <td>48.768000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.143595</td>\n",
              "      <td>62.526500</td>\n",
              "      <td>48.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>0.197300</td>\n",
              "      <td>0.144442</td>\n",
              "      <td>62.568300</td>\n",
              "      <td>48.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>0.142639</td>\n",
              "      <td>62.552100</td>\n",
              "      <td>48.823000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.143262</td>\n",
              "      <td>62.643600</td>\n",
              "      <td>48.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>0.197500</td>\n",
              "      <td>0.143761</td>\n",
              "      <td>62.844200</td>\n",
              "      <td>48.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.143036</td>\n",
              "      <td>63.213700</td>\n",
              "      <td>48.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>0.198400</td>\n",
              "      <td>0.142640</td>\n",
              "      <td>62.897000</td>\n",
              "      <td>48.556000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.143455</td>\n",
              "      <td>63.078600</td>\n",
              "      <td>48.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.143582</td>\n",
              "      <td>62.736700</td>\n",
              "      <td>48.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>0.198100</td>\n",
              "      <td>0.143374</td>\n",
              "      <td>62.621300</td>\n",
              "      <td>48.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>0.198500</td>\n",
              "      <td>0.143040</td>\n",
              "      <td>62.665300</td>\n",
              "      <td>48.735000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>0.197800</td>\n",
              "      <td>0.142843</td>\n",
              "      <td>63.009300</td>\n",
              "      <td>48.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>0.196300</td>\n",
              "      <td>0.142727</td>\n",
              "      <td>62.815300</td>\n",
              "      <td>48.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>0.196400</td>\n",
              "      <td>0.143252</td>\n",
              "      <td>62.795800</td>\n",
              "      <td>48.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>62.631500</td>\n",
              "      <td>48.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.141620</td>\n",
              "      <td>62.816200</td>\n",
              "      <td>48.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.194600</td>\n",
              "      <td>0.142339</td>\n",
              "      <td>63.395500</td>\n",
              "      <td>48.174000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>0.197100</td>\n",
              "      <td>0.142367</td>\n",
              "      <td>63.696100</td>\n",
              "      <td>47.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>0.193200</td>\n",
              "      <td>0.142282</td>\n",
              "      <td>63.560500</td>\n",
              "      <td>48.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.142321</td>\n",
              "      <td>64.006700</td>\n",
              "      <td>47.714000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>0.196500</td>\n",
              "      <td>0.142329</td>\n",
              "      <td>63.696500</td>\n",
              "      <td>47.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.141508</td>\n",
              "      <td>63.604600</td>\n",
              "      <td>48.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>0.196900</td>\n",
              "      <td>0.142121</td>\n",
              "      <td>63.710000</td>\n",
              "      <td>47.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>0.194100</td>\n",
              "      <td>0.141545</td>\n",
              "      <td>63.263500</td>\n",
              "      <td>48.274000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>0.193900</td>\n",
              "      <td>0.142275</td>\n",
              "      <td>63.297500</td>\n",
              "      <td>48.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>0.192900</td>\n",
              "      <td>0.141615</td>\n",
              "      <td>63.246300</td>\n",
              "      <td>48.287000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>0.192600</td>\n",
              "      <td>0.141590</td>\n",
              "      <td>63.594200</td>\n",
              "      <td>48.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>0.199700</td>\n",
              "      <td>0.141151</td>\n",
              "      <td>63.319000</td>\n",
              "      <td>48.232000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.195100</td>\n",
              "      <td>0.142147</td>\n",
              "      <td>63.869600</td>\n",
              "      <td>47.816000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>0.141429</td>\n",
              "      <td>63.412600</td>\n",
              "      <td>48.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>0.191800</td>\n",
              "      <td>0.142418</td>\n",
              "      <td>63.420200</td>\n",
              "      <td>48.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>0.142257</td>\n",
              "      <td>63.639500</td>\n",
              "      <td>47.989000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.141311</td>\n",
              "      <td>63.699500</td>\n",
              "      <td>47.944000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>0.194300</td>\n",
              "      <td>0.141598</td>\n",
              "      <td>63.550000</td>\n",
              "      <td>48.057000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.140850</td>\n",
              "      <td>63.937700</td>\n",
              "      <td>47.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.140775</td>\n",
              "      <td>63.662700</td>\n",
              "      <td>47.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>0.194300</td>\n",
              "      <td>0.140906</td>\n",
              "      <td>63.229600</td>\n",
              "      <td>48.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.141052</td>\n",
              "      <td>63.228100</td>\n",
              "      <td>48.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>0.191700</td>\n",
              "      <td>0.141102</td>\n",
              "      <td>63.322100</td>\n",
              "      <td>48.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.140829</td>\n",
              "      <td>63.286100</td>\n",
              "      <td>48.257000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>0.191500</td>\n",
              "      <td>0.140905</td>\n",
              "      <td>63.303800</td>\n",
              "      <td>48.244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62500</td>\n",
              "      <td>0.195100</td>\n",
              "      <td>0.140658</td>\n",
              "      <td>63.216200</td>\n",
              "      <td>48.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>0.140720</td>\n",
              "      <td>63.314300</td>\n",
              "      <td>48.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63500</td>\n",
              "      <td>0.190900</td>\n",
              "      <td>0.140677</td>\n",
              "      <td>63.352400</td>\n",
              "      <td>48.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>0.192600</td>\n",
              "      <td>0.141548</td>\n",
              "      <td>63.586300</td>\n",
              "      <td>48.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64500</td>\n",
              "      <td>0.193500</td>\n",
              "      <td>0.140982</td>\n",
              "      <td>63.837100</td>\n",
              "      <td>47.841000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>0.190800</td>\n",
              "      <td>0.140289</td>\n",
              "      <td>64.028900</td>\n",
              "      <td>47.697000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65500</td>\n",
              "      <td>0.188800</td>\n",
              "      <td>0.140623</td>\n",
              "      <td>63.907000</td>\n",
              "      <td>47.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>0.193300</td>\n",
              "      <td>0.140990</td>\n",
              "      <td>63.584800</td>\n",
              "      <td>48.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66500</td>\n",
              "      <td>0.189300</td>\n",
              "      <td>0.140515</td>\n",
              "      <td>62.458500</td>\n",
              "      <td>48.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.140523</td>\n",
              "      <td>63.747500</td>\n",
              "      <td>47.908000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.140268</td>\n",
              "      <td>62.900500</td>\n",
              "      <td>48.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>0.191300</td>\n",
              "      <td>0.140592</td>\n",
              "      <td>62.708000</td>\n",
              "      <td>48.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68500</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.140701</td>\n",
              "      <td>63.509300</td>\n",
              "      <td>48.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>0.188600</td>\n",
              "      <td>0.140485</td>\n",
              "      <td>63.057200</td>\n",
              "      <td>48.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69500</td>\n",
              "      <td>0.189700</td>\n",
              "      <td>0.140534</td>\n",
              "      <td>63.380400</td>\n",
              "      <td>48.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>0.188300</td>\n",
              "      <td>0.140844</td>\n",
              "      <td>63.390800</td>\n",
              "      <td>48.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70500</td>\n",
              "      <td>0.189600</td>\n",
              "      <td>0.140254</td>\n",
              "      <td>63.233900</td>\n",
              "      <td>48.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71000</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>0.140047</td>\n",
              "      <td>63.274000</td>\n",
              "      <td>48.266000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71500</td>\n",
              "      <td>0.188900</td>\n",
              "      <td>0.140085</td>\n",
              "      <td>63.344700</td>\n",
              "      <td>48.212000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72000</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>63.170300</td>\n",
              "      <td>48.346000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72500</td>\n",
              "      <td>0.189900</td>\n",
              "      <td>0.140022</td>\n",
              "      <td>63.109500</td>\n",
              "      <td>48.392000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='80622' max='80622' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [80622/80622 15:02:33, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.236200</td>\n",
              "      <td>0.162231</td>\n",
              "      <td>62.833300</td>\n",
              "      <td>48.605000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.245600</td>\n",
              "      <td>0.158533</td>\n",
              "      <td>62.289200</td>\n",
              "      <td>49.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.241600</td>\n",
              "      <td>0.158035</td>\n",
              "      <td>62.804400</td>\n",
              "      <td>48.627000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.237600</td>\n",
              "      <td>0.157259</td>\n",
              "      <td>62.861700</td>\n",
              "      <td>48.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.237800</td>\n",
              "      <td>0.155415</td>\n",
              "      <td>62.741700</td>\n",
              "      <td>48.676000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.235600</td>\n",
              "      <td>0.155988</td>\n",
              "      <td>62.769900</td>\n",
              "      <td>48.654000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.235300</td>\n",
              "      <td>0.155270</td>\n",
              "      <td>62.789000</td>\n",
              "      <td>48.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.234200</td>\n",
              "      <td>0.154215</td>\n",
              "      <td>63.701600</td>\n",
              "      <td>47.942000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.235000</td>\n",
              "      <td>0.155944</td>\n",
              "      <td>62.922800</td>\n",
              "      <td>48.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.232100</td>\n",
              "      <td>0.155442</td>\n",
              "      <td>62.928300</td>\n",
              "      <td>48.531000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.232300</td>\n",
              "      <td>0.153418</td>\n",
              "      <td>62.936800</td>\n",
              "      <td>48.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.233100</td>\n",
              "      <td>0.152663</td>\n",
              "      <td>62.863900</td>\n",
              "      <td>48.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6500</td>\n",
              "      <td>0.231900</td>\n",
              "      <td>0.153689</td>\n",
              "      <td>63.780300</td>\n",
              "      <td>47.883000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7000</td>\n",
              "      <td>0.231200</td>\n",
              "      <td>0.153285</td>\n",
              "      <td>63.117600</td>\n",
              "      <td>48.386000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7500</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.152401</td>\n",
              "      <td>62.868100</td>\n",
              "      <td>48.578000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8000</td>\n",
              "      <td>0.230500</td>\n",
              "      <td>0.154449</td>\n",
              "      <td>62.501400</td>\n",
              "      <td>48.863000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8500</td>\n",
              "      <td>0.231700</td>\n",
              "      <td>0.154772</td>\n",
              "      <td>61.670800</td>\n",
              "      <td>49.521000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9000</td>\n",
              "      <td>0.227800</td>\n",
              "      <td>0.152490</td>\n",
              "      <td>61.671000</td>\n",
              "      <td>49.521000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9500</td>\n",
              "      <td>0.230000</td>\n",
              "      <td>0.152547</td>\n",
              "      <td>62.474600</td>\n",
              "      <td>48.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10000</td>\n",
              "      <td>0.227600</td>\n",
              "      <td>0.153690</td>\n",
              "      <td>61.732900</td>\n",
              "      <td>49.471000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10500</td>\n",
              "      <td>0.224200</td>\n",
              "      <td>0.152178</td>\n",
              "      <td>61.808900</td>\n",
              "      <td>49.410000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11000</td>\n",
              "      <td>0.227100</td>\n",
              "      <td>0.152827</td>\n",
              "      <td>61.414800</td>\n",
              "      <td>49.727000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11500</td>\n",
              "      <td>0.224000</td>\n",
              "      <td>0.152351</td>\n",
              "      <td>61.423500</td>\n",
              "      <td>49.720000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12000</td>\n",
              "      <td>0.225300</td>\n",
              "      <td>0.150647</td>\n",
              "      <td>61.341500</td>\n",
              "      <td>49.787000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12500</td>\n",
              "      <td>0.223100</td>\n",
              "      <td>0.151268</td>\n",
              "      <td>61.388600</td>\n",
              "      <td>49.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13000</td>\n",
              "      <td>0.223600</td>\n",
              "      <td>0.150695</td>\n",
              "      <td>61.418100</td>\n",
              "      <td>49.725000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13500</td>\n",
              "      <td>0.223900</td>\n",
              "      <td>0.150777</td>\n",
              "      <td>61.692500</td>\n",
              "      <td>49.504000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14000</td>\n",
              "      <td>0.226000</td>\n",
              "      <td>0.150068</td>\n",
              "      <td>61.552200</td>\n",
              "      <td>49.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14500</td>\n",
              "      <td>0.220700</td>\n",
              "      <td>0.150723</td>\n",
              "      <td>61.602300</td>\n",
              "      <td>49.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15000</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.150885</td>\n",
              "      <td>61.893500</td>\n",
              "      <td>49.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15500</td>\n",
              "      <td>0.220900</td>\n",
              "      <td>0.151000</td>\n",
              "      <td>61.600600</td>\n",
              "      <td>49.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16000</td>\n",
              "      <td>0.222100</td>\n",
              "      <td>0.150836</td>\n",
              "      <td>61.623700</td>\n",
              "      <td>49.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16500</td>\n",
              "      <td>0.222000</td>\n",
              "      <td>0.150737</td>\n",
              "      <td>62.354800</td>\n",
              "      <td>48.978000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17000</td>\n",
              "      <td>0.219900</td>\n",
              "      <td>0.149742</td>\n",
              "      <td>61.835800</td>\n",
              "      <td>49.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17500</td>\n",
              "      <td>0.220000</td>\n",
              "      <td>0.149997</td>\n",
              "      <td>61.893600</td>\n",
              "      <td>49.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18000</td>\n",
              "      <td>0.221300</td>\n",
              "      <td>0.150722</td>\n",
              "      <td>62.126200</td>\n",
              "      <td>49.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18500</td>\n",
              "      <td>0.222200</td>\n",
              "      <td>0.150486</td>\n",
              "      <td>62.016400</td>\n",
              "      <td>49.245000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19000</td>\n",
              "      <td>0.218900</td>\n",
              "      <td>0.149428</td>\n",
              "      <td>61.823000</td>\n",
              "      <td>49.399000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19500</td>\n",
              "      <td>0.217900</td>\n",
              "      <td>0.149347</td>\n",
              "      <td>61.555600</td>\n",
              "      <td>49.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20000</td>\n",
              "      <td>0.215200</td>\n",
              "      <td>0.149564</td>\n",
              "      <td>61.806400</td>\n",
              "      <td>49.412000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20500</td>\n",
              "      <td>0.218700</td>\n",
              "      <td>0.148446</td>\n",
              "      <td>61.759000</td>\n",
              "      <td>49.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21000</td>\n",
              "      <td>0.217600</td>\n",
              "      <td>0.148917</td>\n",
              "      <td>62.153200</td>\n",
              "      <td>49.137000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21500</td>\n",
              "      <td>0.220900</td>\n",
              "      <td>0.148894</td>\n",
              "      <td>61.949800</td>\n",
              "      <td>49.298000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22000</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>0.148455</td>\n",
              "      <td>62.037100</td>\n",
              "      <td>49.229000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22500</td>\n",
              "      <td>0.217500</td>\n",
              "      <td>0.148681</td>\n",
              "      <td>61.905800</td>\n",
              "      <td>49.333000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23000</td>\n",
              "      <td>0.216700</td>\n",
              "      <td>0.148834</td>\n",
              "      <td>62.084200</td>\n",
              "      <td>49.191000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23500</td>\n",
              "      <td>0.216100</td>\n",
              "      <td>0.148107</td>\n",
              "      <td>62.204800</td>\n",
              "      <td>49.096000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24000</td>\n",
              "      <td>0.216300</td>\n",
              "      <td>0.147616</td>\n",
              "      <td>61.813800</td>\n",
              "      <td>49.406000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24500</td>\n",
              "      <td>0.212300</td>\n",
              "      <td>0.148385</td>\n",
              "      <td>63.092900</td>\n",
              "      <td>48.405000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25000</td>\n",
              "      <td>0.214100</td>\n",
              "      <td>0.147897</td>\n",
              "      <td>62.216100</td>\n",
              "      <td>49.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25500</td>\n",
              "      <td>0.215300</td>\n",
              "      <td>0.147586</td>\n",
              "      <td>62.861200</td>\n",
              "      <td>48.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26000</td>\n",
              "      <td>0.213200</td>\n",
              "      <td>0.147526</td>\n",
              "      <td>62.996900</td>\n",
              "      <td>48.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26500</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.147377</td>\n",
              "      <td>62.988600</td>\n",
              "      <td>48.485000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27000</td>\n",
              "      <td>0.215400</td>\n",
              "      <td>0.148235</td>\n",
              "      <td>62.872300</td>\n",
              "      <td>48.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27500</td>\n",
              "      <td>0.215400</td>\n",
              "      <td>0.147941</td>\n",
              "      <td>63.005800</td>\n",
              "      <td>48.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28000</td>\n",
              "      <td>0.213300</td>\n",
              "      <td>0.147397</td>\n",
              "      <td>62.290300</td>\n",
              "      <td>49.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28500</td>\n",
              "      <td>0.213200</td>\n",
              "      <td>0.146013</td>\n",
              "      <td>61.664800</td>\n",
              "      <td>49.526000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29000</td>\n",
              "      <td>0.214200</td>\n",
              "      <td>0.147023</td>\n",
              "      <td>61.512300</td>\n",
              "      <td>49.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29500</td>\n",
              "      <td>0.214500</td>\n",
              "      <td>0.147914</td>\n",
              "      <td>61.487600</td>\n",
              "      <td>49.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30000</td>\n",
              "      <td>0.214800</td>\n",
              "      <td>0.146741</td>\n",
              "      <td>61.817000</td>\n",
              "      <td>49.404000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30500</td>\n",
              "      <td>0.212900</td>\n",
              "      <td>0.146957</td>\n",
              "      <td>61.771600</td>\n",
              "      <td>49.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31000</td>\n",
              "      <td>0.211200</td>\n",
              "      <td>0.146969</td>\n",
              "      <td>61.623500</td>\n",
              "      <td>49.559000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31500</td>\n",
              "      <td>0.210000</td>\n",
              "      <td>0.146415</td>\n",
              "      <td>61.602200</td>\n",
              "      <td>49.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32000</td>\n",
              "      <td>0.211000</td>\n",
              "      <td>0.145922</td>\n",
              "      <td>61.963100</td>\n",
              "      <td>49.287000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32500</td>\n",
              "      <td>0.212400</td>\n",
              "      <td>0.146003</td>\n",
              "      <td>61.763700</td>\n",
              "      <td>49.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33000</td>\n",
              "      <td>0.210700</td>\n",
              "      <td>0.145638</td>\n",
              "      <td>61.893700</td>\n",
              "      <td>49.343000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33500</td>\n",
              "      <td>0.210800</td>\n",
              "      <td>0.145352</td>\n",
              "      <td>61.939400</td>\n",
              "      <td>49.306000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34000</td>\n",
              "      <td>0.207300</td>\n",
              "      <td>0.145926</td>\n",
              "      <td>62.364600</td>\n",
              "      <td>48.970000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34500</td>\n",
              "      <td>0.209700</td>\n",
              "      <td>0.145065</td>\n",
              "      <td>61.089200</td>\n",
              "      <td>49.993000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35000</td>\n",
              "      <td>0.208100</td>\n",
              "      <td>0.146048</td>\n",
              "      <td>62.096700</td>\n",
              "      <td>49.181000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35500</td>\n",
              "      <td>0.206900</td>\n",
              "      <td>0.145184</td>\n",
              "      <td>61.984400</td>\n",
              "      <td>49.270000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36000</td>\n",
              "      <td>0.208300</td>\n",
              "      <td>0.144818</td>\n",
              "      <td>61.978800</td>\n",
              "      <td>49.275000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36500</td>\n",
              "      <td>0.206500</td>\n",
              "      <td>0.146101</td>\n",
              "      <td>62.083500</td>\n",
              "      <td>49.192000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37000</td>\n",
              "      <td>0.206900</td>\n",
              "      <td>0.146314</td>\n",
              "      <td>61.931200</td>\n",
              "      <td>49.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37500</td>\n",
              "      <td>0.210700</td>\n",
              "      <td>0.145239</td>\n",
              "      <td>61.898000</td>\n",
              "      <td>49.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38000</td>\n",
              "      <td>0.205600</td>\n",
              "      <td>0.144952</td>\n",
              "      <td>62.129400</td>\n",
              "      <td>49.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38500</td>\n",
              "      <td>0.208600</td>\n",
              "      <td>0.144559</td>\n",
              "      <td>62.102200</td>\n",
              "      <td>49.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39000</td>\n",
              "      <td>0.207200</td>\n",
              "      <td>0.143776</td>\n",
              "      <td>62.317900</td>\n",
              "      <td>49.007000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39500</td>\n",
              "      <td>0.207000</td>\n",
              "      <td>0.144248</td>\n",
              "      <td>62.531300</td>\n",
              "      <td>48.840000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40000</td>\n",
              "      <td>0.206600</td>\n",
              "      <td>0.143938</td>\n",
              "      <td>62.343500</td>\n",
              "      <td>48.987000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40500</td>\n",
              "      <td>0.202200</td>\n",
              "      <td>0.144140</td>\n",
              "      <td>62.866800</td>\n",
              "      <td>48.579000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41000</td>\n",
              "      <td>0.199400</td>\n",
              "      <td>0.144126</td>\n",
              "      <td>62.513200</td>\n",
              "      <td>48.854000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41500</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.143201</td>\n",
              "      <td>62.622600</td>\n",
              "      <td>48.768000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42000</td>\n",
              "      <td>0.198000</td>\n",
              "      <td>0.143595</td>\n",
              "      <td>62.526500</td>\n",
              "      <td>48.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42500</td>\n",
              "      <td>0.197300</td>\n",
              "      <td>0.144442</td>\n",
              "      <td>62.568300</td>\n",
              "      <td>48.811000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43000</td>\n",
              "      <td>0.197600</td>\n",
              "      <td>0.142639</td>\n",
              "      <td>62.552100</td>\n",
              "      <td>48.823000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43500</td>\n",
              "      <td>0.198600</td>\n",
              "      <td>0.143262</td>\n",
              "      <td>62.643600</td>\n",
              "      <td>48.752000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44000</td>\n",
              "      <td>0.197500</td>\n",
              "      <td>0.143761</td>\n",
              "      <td>62.844200</td>\n",
              "      <td>48.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44500</td>\n",
              "      <td>0.197700</td>\n",
              "      <td>0.143036</td>\n",
              "      <td>63.213700</td>\n",
              "      <td>48.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45000</td>\n",
              "      <td>0.198400</td>\n",
              "      <td>0.142640</td>\n",
              "      <td>62.897000</td>\n",
              "      <td>48.556000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45500</td>\n",
              "      <td>0.197400</td>\n",
              "      <td>0.143455</td>\n",
              "      <td>63.078600</td>\n",
              "      <td>48.416000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46000</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.143582</td>\n",
              "      <td>62.736700</td>\n",
              "      <td>48.680000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46500</td>\n",
              "      <td>0.198100</td>\n",
              "      <td>0.143374</td>\n",
              "      <td>62.621300</td>\n",
              "      <td>48.769000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47000</td>\n",
              "      <td>0.198500</td>\n",
              "      <td>0.143040</td>\n",
              "      <td>62.665300</td>\n",
              "      <td>48.735000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47500</td>\n",
              "      <td>0.197800</td>\n",
              "      <td>0.142843</td>\n",
              "      <td>63.009300</td>\n",
              "      <td>48.469000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48000</td>\n",
              "      <td>0.196300</td>\n",
              "      <td>0.142727</td>\n",
              "      <td>62.815300</td>\n",
              "      <td>48.619000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48500</td>\n",
              "      <td>0.196400</td>\n",
              "      <td>0.143252</td>\n",
              "      <td>62.795800</td>\n",
              "      <td>48.634000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49000</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.142368</td>\n",
              "      <td>62.631500</td>\n",
              "      <td>48.761000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49500</td>\n",
              "      <td>0.196000</td>\n",
              "      <td>0.141620</td>\n",
              "      <td>62.816200</td>\n",
              "      <td>48.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50000</td>\n",
              "      <td>0.194600</td>\n",
              "      <td>0.142339</td>\n",
              "      <td>63.395500</td>\n",
              "      <td>48.174000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50500</td>\n",
              "      <td>0.197100</td>\n",
              "      <td>0.142367</td>\n",
              "      <td>63.696100</td>\n",
              "      <td>47.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51000</td>\n",
              "      <td>0.193200</td>\n",
              "      <td>0.142282</td>\n",
              "      <td>63.560500</td>\n",
              "      <td>48.049000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51500</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.142321</td>\n",
              "      <td>64.006700</td>\n",
              "      <td>47.714000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52000</td>\n",
              "      <td>0.196500</td>\n",
              "      <td>0.142329</td>\n",
              "      <td>63.696500</td>\n",
              "      <td>47.946000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52500</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.141508</td>\n",
              "      <td>63.604600</td>\n",
              "      <td>48.015000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53000</td>\n",
              "      <td>0.196900</td>\n",
              "      <td>0.142121</td>\n",
              "      <td>63.710000</td>\n",
              "      <td>47.936000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53500</td>\n",
              "      <td>0.194100</td>\n",
              "      <td>0.141545</td>\n",
              "      <td>63.263500</td>\n",
              "      <td>48.274000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54000</td>\n",
              "      <td>0.193900</td>\n",
              "      <td>0.142275</td>\n",
              "      <td>63.297500</td>\n",
              "      <td>48.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54500</td>\n",
              "      <td>0.192900</td>\n",
              "      <td>0.141615</td>\n",
              "      <td>63.246300</td>\n",
              "      <td>48.287000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55000</td>\n",
              "      <td>0.192600</td>\n",
              "      <td>0.141590</td>\n",
              "      <td>63.594200</td>\n",
              "      <td>48.023000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55500</td>\n",
              "      <td>0.199700</td>\n",
              "      <td>0.141151</td>\n",
              "      <td>63.319000</td>\n",
              "      <td>48.232000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56000</td>\n",
              "      <td>0.195100</td>\n",
              "      <td>0.142147</td>\n",
              "      <td>63.869600</td>\n",
              "      <td>47.816000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56500</td>\n",
              "      <td>0.194000</td>\n",
              "      <td>0.141429</td>\n",
              "      <td>63.412600</td>\n",
              "      <td>48.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57000</td>\n",
              "      <td>0.191800</td>\n",
              "      <td>0.142418</td>\n",
              "      <td>63.420200</td>\n",
              "      <td>48.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57500</td>\n",
              "      <td>0.194700</td>\n",
              "      <td>0.142257</td>\n",
              "      <td>63.639500</td>\n",
              "      <td>47.989000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58000</td>\n",
              "      <td>0.193600</td>\n",
              "      <td>0.141311</td>\n",
              "      <td>63.699500</td>\n",
              "      <td>47.944000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58500</td>\n",
              "      <td>0.194300</td>\n",
              "      <td>0.141598</td>\n",
              "      <td>63.550000</td>\n",
              "      <td>48.057000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59000</td>\n",
              "      <td>0.194900</td>\n",
              "      <td>0.140850</td>\n",
              "      <td>63.937700</td>\n",
              "      <td>47.765000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.140775</td>\n",
              "      <td>63.662700</td>\n",
              "      <td>47.972000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60000</td>\n",
              "      <td>0.194300</td>\n",
              "      <td>0.140906</td>\n",
              "      <td>63.229600</td>\n",
              "      <td>48.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.141052</td>\n",
              "      <td>63.228100</td>\n",
              "      <td>48.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61000</td>\n",
              "      <td>0.191700</td>\n",
              "      <td>0.141102</td>\n",
              "      <td>63.322100</td>\n",
              "      <td>48.230000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.140829</td>\n",
              "      <td>63.286100</td>\n",
              "      <td>48.257000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62000</td>\n",
              "      <td>0.191500</td>\n",
              "      <td>0.140905</td>\n",
              "      <td>63.303800</td>\n",
              "      <td>48.244000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62500</td>\n",
              "      <td>0.195100</td>\n",
              "      <td>0.140658</td>\n",
              "      <td>63.216200</td>\n",
              "      <td>48.310000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63000</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>0.140720</td>\n",
              "      <td>63.314300</td>\n",
              "      <td>48.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63500</td>\n",
              "      <td>0.190900</td>\n",
              "      <td>0.140677</td>\n",
              "      <td>63.352400</td>\n",
              "      <td>48.207000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64000</td>\n",
              "      <td>0.192600</td>\n",
              "      <td>0.141548</td>\n",
              "      <td>63.586300</td>\n",
              "      <td>48.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64500</td>\n",
              "      <td>0.193500</td>\n",
              "      <td>0.140982</td>\n",
              "      <td>63.837100</td>\n",
              "      <td>47.841000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65000</td>\n",
              "      <td>0.190800</td>\n",
              "      <td>0.140289</td>\n",
              "      <td>64.028900</td>\n",
              "      <td>47.697000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65500</td>\n",
              "      <td>0.188800</td>\n",
              "      <td>0.140623</td>\n",
              "      <td>63.907000</td>\n",
              "      <td>47.788000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66000</td>\n",
              "      <td>0.193300</td>\n",
              "      <td>0.140990</td>\n",
              "      <td>63.584800</td>\n",
              "      <td>48.030000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66500</td>\n",
              "      <td>0.189300</td>\n",
              "      <td>0.140515</td>\n",
              "      <td>62.458500</td>\n",
              "      <td>48.896000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67000</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.140523</td>\n",
              "      <td>63.747500</td>\n",
              "      <td>47.908000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67500</td>\n",
              "      <td>0.191100</td>\n",
              "      <td>0.140268</td>\n",
              "      <td>62.900500</td>\n",
              "      <td>48.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68000</td>\n",
              "      <td>0.191300</td>\n",
              "      <td>0.140592</td>\n",
              "      <td>62.708000</td>\n",
              "      <td>48.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68500</td>\n",
              "      <td>0.190400</td>\n",
              "      <td>0.140701</td>\n",
              "      <td>63.509300</td>\n",
              "      <td>48.087000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69000</td>\n",
              "      <td>0.188600</td>\n",
              "      <td>0.140485</td>\n",
              "      <td>63.057200</td>\n",
              "      <td>48.432000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69500</td>\n",
              "      <td>0.189700</td>\n",
              "      <td>0.140534</td>\n",
              "      <td>63.380400</td>\n",
              "      <td>48.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70000</td>\n",
              "      <td>0.188300</td>\n",
              "      <td>0.140844</td>\n",
              "      <td>63.390800</td>\n",
              "      <td>48.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70500</td>\n",
              "      <td>0.189600</td>\n",
              "      <td>0.140254</td>\n",
              "      <td>63.233900</td>\n",
              "      <td>48.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71000</td>\n",
              "      <td>0.191400</td>\n",
              "      <td>0.140047</td>\n",
              "      <td>63.274000</td>\n",
              "      <td>48.266000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71500</td>\n",
              "      <td>0.188900</td>\n",
              "      <td>0.140085</td>\n",
              "      <td>63.344700</td>\n",
              "      <td>48.212000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72000</td>\n",
              "      <td>0.188200</td>\n",
              "      <td>0.139963</td>\n",
              "      <td>63.170300</td>\n",
              "      <td>48.346000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72500</td>\n",
              "      <td>0.189900</td>\n",
              "      <td>0.140022</td>\n",
              "      <td>63.109500</td>\n",
              "      <td>48.392000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73000</td>\n",
              "      <td>0.188400</td>\n",
              "      <td>0.140008</td>\n",
              "      <td>63.330700</td>\n",
              "      <td>48.223000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73500</td>\n",
              "      <td>0.189500</td>\n",
              "      <td>0.140023</td>\n",
              "      <td>63.360200</td>\n",
              "      <td>48.201000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74000</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>0.139636</td>\n",
              "      <td>63.153400</td>\n",
              "      <td>48.358000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74500</td>\n",
              "      <td>0.188100</td>\n",
              "      <td>0.139889</td>\n",
              "      <td>63.281900</td>\n",
              "      <td>48.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75000</td>\n",
              "      <td>0.189200</td>\n",
              "      <td>0.139854</td>\n",
              "      <td>63.070900</td>\n",
              "      <td>48.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75500</td>\n",
              "      <td>0.187100</td>\n",
              "      <td>0.139862</td>\n",
              "      <td>63.071000</td>\n",
              "      <td>48.422000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76000</td>\n",
              "      <td>0.189500</td>\n",
              "      <td>0.139758</td>\n",
              "      <td>63.484700</td>\n",
              "      <td>48.106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76500</td>\n",
              "      <td>0.189000</td>\n",
              "      <td>0.139684</td>\n",
              "      <td>62.070100</td>\n",
              "      <td>49.202000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77000</td>\n",
              "      <td>0.186300</td>\n",
              "      <td>0.139738</td>\n",
              "      <td>63.055000</td>\n",
              "      <td>48.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77500</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.139692</td>\n",
              "      <td>63.058200</td>\n",
              "      <td>48.431000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78000</td>\n",
              "      <td>0.188000</td>\n",
              "      <td>0.139588</td>\n",
              "      <td>63.215300</td>\n",
              "      <td>48.311000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78500</td>\n",
              "      <td>0.187400</td>\n",
              "      <td>0.139521</td>\n",
              "      <td>63.406400</td>\n",
              "      <td>48.165000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79000</td>\n",
              "      <td>0.186600</td>\n",
              "      <td>0.139659</td>\n",
              "      <td>63.408000</td>\n",
              "      <td>48.164000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79500</td>\n",
              "      <td>0.188400</td>\n",
              "      <td>0.139604</td>\n",
              "      <td>63.260100</td>\n",
              "      <td>48.277000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80000</td>\n",
              "      <td>0.188100</td>\n",
              "      <td>0.139567</td>\n",
              "      <td>63.399800</td>\n",
              "      <td>48.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80500</td>\n",
              "      <td>0.185800</td>\n",
              "      <td>0.139590</td>\n",
              "      <td>62.957200</td>\n",
              "      <td>48.509000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 902.6242098172505 minutes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4SWf1TU4uVa"
      },
      "source": [
        "trainer.save_model(output_dir + '/model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ua3CxHWhTHs"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fiji78NTg3Fl"
      },
      "source": [
        "### How did Training affect the loss?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "a7697eb0b3914e1c8129f7061f06efc3",
            "8fdd34a6af3441ba84ab539e336de41e",
            "e40e8e6d37784493b2d30575cd8e3a5c",
            "375ce9b6c6f44467845b335ab41dccbc",
            "4435dd2817c34c12b262d8893fe392a0",
            "e9d639f9ca98474d9482924a69c761a0",
            "520139b274f44475ade626e2fbca8a08",
            "9658c9f96e764f8fa0bbeec4162495d8"
          ]
        },
        "id": "jMVyUKP8Md1T",
        "outputId": "bebf4bbe-b4d9-4eb6-edbe-87adc8960c2f"
      },
      "source": [
        "# load our model\n",
        "excluded_hl_model = T5ForConditionalGeneration.from_pretrained('exclude_highlights_model/model')\n",
        "\n",
        "# collect test data in the format required for this next model\n",
        "source_text_test = []\n",
        "target_text_test = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    \n",
        "    # # format data as story/joined highlights pairs\n",
        "    # source_text_test.append(story)\n",
        "    # target_text_test.append('. '.join(highlights)+'.')\n",
        "\n",
        "    for j in range(len(highlights)):\n",
        "        source_text_test.append('\\n\\n@highlight\\n\\n '.join([story]+[highlights[j]]))\n",
        "        target_text_test.append('. '.join(highlights[:j]+highlights[j+1:]+[' ']))\n",
        "\n",
        "# print the time this took in minutes\n",
        "print('\\n\\ntime:', (time.time()-start)/60,'minutes')\n",
        "\n",
        "# format test data as dataset\n",
        "test_df = pd.DataFrame(list(zip(source_text_test, target_text_test)),columns =['source', 'target'])\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "test_tokenized = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
        "print(test_dataset)\n",
        "\n",
        "# evaluate the model on the test dataset\n",
        "eval_args = TrainingArguments(\n",
        "    per_device_eval_batch_size=8,\n",
        "    # remove_unused_columns=True,\n",
        "    eval_accumulation_steps=1,\n",
        "    output_dir = 'output'\n",
        ")\n",
        "\n",
        "trainer = Trainer(model=excluded_hl_model, args=eval_args)\n",
        "\n",
        "trainer.evaluate(test_tokenized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "time: 0.000546574592590332 minutes\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7697eb0b3914e1c8129f7061f06efc3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Dataset({\n",
            "    features: ['source', 'target'],\n",
            "    num_rows: 2720\n",
            "})\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='340' max='340' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [340/340 00:51]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.13251729309558868,\n",
              " 'eval_mem_cpu_alloc_delta': 16900096,\n",
              " 'eval_mem_cpu_peaked_delta': 0,\n",
              " 'eval_mem_gpu_alloc_delta': 0,\n",
              " 'eval_mem_gpu_peaked_delta': 1279395840,\n",
              " 'eval_runtime': 51.9005,\n",
              " 'eval_samples_per_second': 52.408,\n",
              " 'init_mem_cpu_alloc_delta': 1463455744,\n",
              " 'init_mem_cpu_peaked_delta': 0,\n",
              " 'init_mem_gpu_alloc_delta': 242026496,\n",
              " 'init_mem_gpu_peaked_delta': 0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85sowUbdRFnl",
        "outputId": "5fb83624-2652-420c-af1c-339f95f444e4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2720"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sc40w0L9W8Vg"
      },
      "source": [
        "### ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3eENnkAfOOr",
        "outputId": "3a39d1b3-391d-4b6f-baab-f4d8f9ae3929"
      },
      "source": [
        "naive_model = T5ForConditionalGeneration.from_pretrained('naive_exclusion_model/model')\n",
        "\n",
        "# for scoring outputs\n",
        "rouge = Rouge()\n",
        "\n",
        "rouge_score = []\n",
        "\n",
        "\n",
        "for i in range(len(test_files)):\n",
        "# for i in range(5):\n",
        "\n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    highlights[1:]\n",
        "\n",
        "    # encode the input\n",
        "    encoded = tokenizer.encode('summarize: ' + story, return_tensors='pt')\n",
        "\n",
        "    # generate the output\n",
        "    output = naive_model.generate(encoded, num_beams=4, no_repeat_ngram_size=2,\n",
        "                             min_length=30, max_length=300, early_stopping=True)\n",
        "    summary = tokenizer.decode(output[0])\n",
        "    \n",
        "    # get ROUGE scores between the output and highlights\n",
        "    score = rouge.get_scores(summary,'. '.join(highlights[1:])+'.')[0]['rouge-1']['f']\n",
        "\n",
        "    rouge_score.append(np.mean(score))\n",
        "    \n",
        "\n",
        "    if 1%1==0:\n",
        "      print(i+1,'stories passed, ROUGE F1 (mean):', np.mean(rouge_score))\n",
        "\n",
        "print('ROUGE F1 (mean):',np.mean(rouge_score))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 stories passed, ROUGE F1 (mean): 0.3157894687396123\n",
            "2 stories passed, ROUGE F1 (mean): 0.23632610442747637\n",
            "3 stories passed, ROUGE F1 (mean): 0.20516978242255043\n",
            "4 stories passed, ROUGE F1 (mean): 0.2421126296965745\n",
            "5 stories passed, ROUGE F1 (mean): 0.21869010275899572\n",
            "6 stories passed, ROUGE F1 (mean): 0.21696397370107282\n",
            "7 stories passed, ROUGE F1 (mean): 0.1934879167912639\n",
            "8 stories passed, ROUGE F1 (mean): 0.18319081548710905\n",
            "9 stories passed, ROUGE F1 (mean): 0.19061405768993028\n",
            "10 stories passed, ROUGE F1 (mean): 0.20791628778870583\n",
            "11 stories passed, ROUGE F1 (mean): 0.20987918814633758\n",
            "12 stories passed, ROUGE F1 (mean): 0.19963563235011636\n",
            "13 stories passed, ROUGE F1 (mean): 0.18812519875491512\n",
            "14 stories passed, ROUGE F1 (mean): 0.17961379259618543\n",
            "15 stories passed, ROUGE F1 (mean): 0.1958446676283579\n",
            "16 stories passed, ROUGE F1 (mean): 0.18658056607165358\n",
            "17 stories passed, ROUGE F1 (mean): 0.18847288542756044\n",
            "18 stories passed, ROUGE F1 (mean): 0.19265418394876946\n",
            "19 stories passed, ROUGE F1 (mean): 0.1849624703294226\n",
            "20 stories passed, ROUGE F1 (mean): 0.18084255170447022\n",
            "21 stories passed, ROUGE F1 (mean): 0.18669754750590925\n",
            "22 stories passed, ROUGE F1 (mean): 0.1911983081392729\n",
            "23 stories passed, ROUGE F1 (mean): 0.1846969322513486\n",
            "24 stories passed, ROUGE F1 (mean): 0.18383182762997366\n",
            "25 stories passed, ROUGE F1 (mean): 0.17825633211450312\n",
            "26 stories passed, ROUGE F1 (mean): 0.1803189367638403\n",
            "27 stories passed, ROUGE F1 (mean): 0.17648946029263957\n",
            "28 stories passed, ROUGE F1 (mean): 0.18392252884014465\n",
            "29 stories passed, ROUGE F1 (mean): 0.180453935729996\n",
            "30 stories passed, ROUGE F1 (mean): 0.17582769326845218\n",
            "31 stories passed, ROUGE F1 (mean): 0.17694700351230686\n",
            "32 stories passed, ROUGE F1 (mean): 0.18034598092486864\n",
            "33 stories passed, ROUGE F1 (mean): 0.17834415455621153\n",
            "34 stories passed, ROUGE F1 (mean): 0.1764924032788012\n",
            "35 stories passed, ROUGE F1 (mean): 0.17313043531228378\n",
            "36 stories passed, ROUGE F1 (mean): 0.16990855803478838\n",
            "37 stories passed, ROUGE F1 (mean): 0.16743620154395342\n",
            "38 stories passed, ROUGE F1 (mean): 0.1654970908465848\n",
            "39 stories passed, ROUGE F1 (mean): 0.16512391907172144\n",
            "40 stories passed, ROUGE F1 (mean): 0.17570170332330212\n",
            "41 stories passed, ROUGE F1 (mean): 0.17851163948397142\n",
            "42 stories passed, ROUGE F1 (mean): 0.176331755623497\n",
            "43 stories passed, ROUGE F1 (mean): 0.17975495681322978\n",
            "44 stories passed, ROUGE F1 (mean): 0.18216312326555517\n",
            "45 stories passed, ROUGE F1 (mean): 0.18255949820436507\n",
            "46 stories passed, ROUGE F1 (mean): 0.188351239281555\n",
            "47 stories passed, ROUGE F1 (mean): 0.18927746936061277\n",
            "48 stories passed, ROUGE F1 (mean): 0.1885893969846104\n",
            "49 stories passed, ROUGE F1 (mean): 0.19083262271433563\n",
            "50 stories passed, ROUGE F1 (mean): 0.18775671091190077\n",
            "51 stories passed, ROUGE F1 (mean): 0.1850555988508831\n",
            "52 stories passed, ROUGE F1 (mean): 0.18332833908055174\n",
            "53 stories passed, ROUGE F1 (mean): 0.18356890676708368\n",
            "54 stories passed, ROUGE F1 (mean): 0.18109540840935992\n",
            "55 stories passed, ROUGE F1 (mean): 0.18118542837701904\n",
            "56 stories passed, ROUGE F1 (mean): 0.1793505344551165\n",
            "57 stories passed, ROUGE F1 (mean): 0.17862387647690023\n",
            "58 stories passed, ROUGE F1 (mean): 0.17554415446867783\n",
            "59 stories passed, ROUGE F1 (mean): 0.17503416101083918\n",
            "60 stories passed, ROUGE F1 (mean): 0.17964380663143548\n",
            "61 stories passed, ROUGE F1 (mean): 0.18489554743440376\n",
            "62 stories passed, ROUGE F1 (mean): 0.184914111297641\n",
            "63 stories passed, ROUGE F1 (mean): 0.18381046844021154\n",
            "64 stories passed, ROUGE F1 (mean): 0.18365582109838524\n",
            "65 stories passed, ROUGE F1 (mean): 0.1825720014228075\n",
            "66 stories passed, ROUGE F1 (mean): 0.18475319242266763\n",
            "67 stories passed, ROUGE F1 (mean): 0.18360923587014938\n",
            "68 stories passed, ROUGE F1 (mean): 0.18267380586158835\n",
            "69 stories passed, ROUGE F1 (mean): 0.1818667089942555\n",
            "70 stories passed, ROUGE F1 (mean): 0.18201586582772725\n",
            "71 stories passed, ROUGE F1 (mean): 0.18057902258384378\n",
            "72 stories passed, ROUGE F1 (mean): 0.17807098060351262\n",
            "73 stories passed, ROUGE F1 (mean): 0.17745813605217076\n",
            "74 stories passed, ROUGE F1 (mean): 0.1777246895327677\n",
            "75 stories passed, ROUGE F1 (mean): 0.17910502694062508\n",
            "76 stories passed, ROUGE F1 (mean): 0.1811343466957923\n",
            "77 stories passed, ROUGE F1 (mean): 0.18069180736901236\n",
            "78 stories passed, ROUGE F1 (mean): 0.18135676028170736\n",
            "79 stories passed, ROUGE F1 (mean): 0.18029605408969124\n",
            "80 stories passed, ROUGE F1 (mean): 0.17967278813946802\n",
            "81 stories passed, ROUGE F1 (mean): 0.1791772583962643\n",
            "82 stories passed, ROUGE F1 (mean): 0.17745236309946089\n",
            "83 stories passed, ROUGE F1 (mean): 0.1753143828211541\n",
            "84 stories passed, ROUGE F1 (mean): 0.17455005811441918\n",
            "85 stories passed, ROUGE F1 (mean): 0.17308476325792602\n",
            "86 stories passed, ROUGE F1 (mean): 0.17347792353770208\n",
            "87 stories passed, ROUGE F1 (mean): 0.17446391584555593\n",
            "88 stories passed, ROUGE F1 (mean): 0.17461205312287917\n",
            "89 stories passed, ROUGE F1 (mean): 0.17265011994172322\n",
            "90 stories passed, ROUGE F1 (mean): 0.17355365471373774\n",
            "91 stories passed, ROUGE F1 (mean): 0.1751786537438514\n",
            "92 stories passed, ROUGE F1 (mean): 0.1759919291926652\n",
            "93 stories passed, ROUGE F1 (mean): 0.17550973141739407\n",
            "94 stories passed, ROUGE F1 (mean): 0.1756218247859155\n",
            "95 stories passed, ROUGE F1 (mean): 0.17606982466517643\n",
            "96 stories passed, ROUGE F1 (mean): 0.17527743061449752\n",
            "97 stories passed, ROUGE F1 (mean): 0.17399912768733586\n",
            "98 stories passed, ROUGE F1 (mean): 0.17418594972495755\n",
            "99 stories passed, ROUGE F1 (mean): 0.1747447602501868\n",
            "100 stories passed, ROUGE F1 (mean): 0.17461021582432384\n",
            "101 stories passed, ROUGE F1 (mean): 0.17408883957397372\n",
            "102 stories passed, ROUGE F1 (mean): 0.1732164625021956\n",
            "103 stories passed, ROUGE F1 (mean): 0.17257497114004083\n",
            "104 stories passed, ROUGE F1 (mean): 0.17134294680538475\n",
            "105 stories passed, ROUGE F1 (mean): 0.17147477760052326\n",
            "106 stories passed, ROUGE F1 (mean): 0.17246957283133219\n",
            "107 stories passed, ROUGE F1 (mean): 0.17184147545703155\n",
            "108 stories passed, ROUGE F1 (mean): 0.1712791572233632\n",
            "109 stories passed, ROUGE F1 (mean): 0.17223863343860663\n",
            "110 stories passed, ROUGE F1 (mean): 0.17306517214746242\n",
            "111 stories passed, ROUGE F1 (mean): 0.17345392026458692\n",
            "112 stories passed, ROUGE F1 (mean): 0.17259203769176018\n",
            "113 stories passed, ROUGE F1 (mean): 0.17213734813288642\n",
            "114 stories passed, ROUGE F1 (mean): 0.17235299689123174\n",
            "115 stories passed, ROUGE F1 (mean): 0.17191904888255474\n",
            "116 stories passed, ROUGE F1 (mean): 0.17136063339429908\n",
            "117 stories passed, ROUGE F1 (mean): 0.17056636616618312\n",
            "118 stories passed, ROUGE F1 (mean): 0.17046605928242387\n",
            "119 stories passed, ROUGE F1 (mean): 0.16903357138929423\n",
            "120 stories passed, ROUGE F1 (mean): 0.16876132188985518\n",
            "121 stories passed, ROUGE F1 (mean): 0.1686889142302035\n",
            "122 stories passed, ROUGE F1 (mean): 0.16864994297619856\n",
            "123 stories passed, ROUGE F1 (mean): 0.1682950653507331\n",
            "124 stories passed, ROUGE F1 (mean): 0.1681180201404388\n",
            "125 stories passed, ROUGE F1 (mean): 0.1677282998308279\n",
            "126 stories passed, ROUGE F1 (mean): 0.16719077360449197\n",
            "127 stories passed, ROUGE F1 (mean): 0.16635152386572788\n",
            "128 stories passed, ROUGE F1 (mean): 0.16539912476886018\n",
            "129 stories passed, ROUGE F1 (mean): 0.16411696101096204\n",
            "130 stories passed, ROUGE F1 (mean): 0.16340397336032828\n",
            "131 stories passed, ROUGE F1 (mean): 0.1635879124569634\n",
            "132 stories passed, ROUGE F1 (mean): 0.16270097086964788\n",
            "133 stories passed, ROUGE F1 (mean): 0.16331150819306406\n",
            "134 stories passed, ROUGE F1 (mean): 0.16250735926479906\n",
            "135 stories passed, ROUGE F1 (mean): 0.16159408757611027\n",
            "136 stories passed, ROUGE F1 (mean): 0.16143188752342588\n",
            "137 stories passed, ROUGE F1 (mean): 0.16173814984474674\n",
            "138 stories passed, ROUGE F1 (mean): 0.16118284715008183\n",
            "139 stories passed, ROUGE F1 (mean): 0.1618218194371848\n",
            "140 stories passed, ROUGE F1 (mean): 0.1613685253752856\n",
            "141 stories passed, ROUGE F1 (mean): 0.16108372814747113\n",
            "142 stories passed, ROUGE F1 (mean): 0.1606906255053773\n",
            "143 stories passed, ROUGE F1 (mean): 0.16096551620567534\n",
            "144 stories passed, ROUGE F1 (mean): 0.16168208163904987\n",
            "145 stories passed, ROUGE F1 (mean): 0.16116673292348951\n",
            "146 stories passed, ROUGE F1 (mean): 0.16201979832312205\n",
            "147 stories passed, ROUGE F1 (mean): 0.16313891947294393\n",
            "148 stories passed, ROUGE F1 (mean): 0.1639222359948569\n",
            "149 stories passed, ROUGE F1 (mean): 0.16463598116192169\n",
            "150 stories passed, ROUGE F1 (mean): 0.1645081048905502\n",
            "151 stories passed, ROUGE F1 (mean): 0.16380820700661336\n",
            "152 stories passed, ROUGE F1 (mean): 0.16338841614110935\n",
            "153 stories passed, ROUGE F1 (mean): 0.16339198480189687\n",
            "154 stories passed, ROUGE F1 (mean): 0.1630626605589406\n",
            "155 stories passed, ROUGE F1 (mean): 0.16336887765729147\n",
            "156 stories passed, ROUGE F1 (mean): 0.16267776659097377\n",
            "157 stories passed, ROUGE F1 (mean): 0.16237653629671284\n",
            "158 stories passed, ROUGE F1 (mean): 0.1622694350572159\n",
            "159 stories passed, ROUGE F1 (mean): 0.16446340783113977\n",
            "160 stories passed, ROUGE F1 (mean): 0.16420081762809272\n",
            "161 stories passed, ROUGE F1 (mean): 0.16485317754492546\n",
            "162 stories passed, ROUGE F1 (mean): 0.16522445419595988\n",
            "163 stories passed, ROUGE F1 (mean): 0.164517555676046\n",
            "164 stories passed, ROUGE F1 (mean): 0.16435544271203284\n",
            "165 stories passed, ROUGE F1 (mean): 0.16483754569273001\n",
            "166 stories passed, ROUGE F1 (mean): 0.16453963644428696\n",
            "167 stories passed, ROUGE F1 (mean): 0.16433968374692567\n",
            "168 stories passed, ROUGE F1 (mean): 0.16479004274481301\n",
            "169 stories passed, ROUGE F1 (mean): 0.16623560997470824\n",
            "170 stories passed, ROUGE F1 (mean): 0.16811943863085324\n",
            "171 stories passed, ROUGE F1 (mean): 0.16938549676530895\n",
            "172 stories passed, ROUGE F1 (mean): 0.1687757911166904\n",
            "173 stories passed, ROUGE F1 (mean): 0.16942276197844394\n",
            "174 stories passed, ROUGE F1 (mean): 0.1686701112405212\n",
            "175 stories passed, ROUGE F1 (mean): 0.1686133114833839\n",
            "176 stories passed, ROUGE F1 (mean): 0.1689664700873228\n",
            "177 stories passed, ROUGE F1 (mean): 0.16843035482875593\n",
            "178 stories passed, ROUGE F1 (mean): 0.16957895593821548\n",
            "179 stories passed, ROUGE F1 (mean): 0.16949106316117146\n",
            "180 stories passed, ROUGE F1 (mean): 0.16920304088759675\n",
            "181 stories passed, ROUGE F1 (mean): 0.16911819618543425\n",
            "182 stories passed, ROUGE F1 (mean): 0.16928787640309673\n",
            "183 stories passed, ROUGE F1 (mean): 0.1698877774239725\n",
            "184 stories passed, ROUGE F1 (mean): 0.16966573653096906\n",
            "185 stories passed, ROUGE F1 (mean): 0.16989522556208006\n",
            "186 stories passed, ROUGE F1 (mean): 0.16984896261639085\n",
            "187 stories passed, ROUGE F1 (mean): 0.16928568540496142\n",
            "188 stories passed, ROUGE F1 (mean): 0.1695135339248264\n",
            "189 stories passed, ROUGE F1 (mean): 0.16939093035260266\n",
            "190 stories passed, ROUGE F1 (mean): 0.1687186973595645\n",
            "191 stories passed, ROUGE F1 (mean): 0.16866202738658653\n",
            "192 stories passed, ROUGE F1 (mean): 0.16868937641100853\n",
            "193 stories passed, ROUGE F1 (mean): 0.16893132063070196\n",
            "194 stories passed, ROUGE F1 (mean): 0.17008859535916307\n",
            "195 stories passed, ROUGE F1 (mean): 0.17007104698410186\n",
            "196 stories passed, ROUGE F1 (mean): 0.171904417970554\n",
            "197 stories passed, ROUGE F1 (mean): 0.17122335921994764\n",
            "198 stories passed, ROUGE F1 (mean): 0.1712982236012876\n",
            "199 stories passed, ROUGE F1 (mean): 0.1714843296580092\n",
            "200 stories passed, ROUGE F1 (mean): 0.17087081042625696\n",
            "201 stories passed, ROUGE F1 (mean): 0.17115787815434236\n",
            "202 stories passed, ROUGE F1 (mean): 0.17053558440417155\n",
            "203 stories passed, ROUGE F1 (mean): 0.1707325830618424\n",
            "204 stories passed, ROUGE F1 (mean): 0.17054925338529864\n",
            "205 stories passed, ROUGE F1 (mean): 0.1697173058078094\n",
            "206 stories passed, ROUGE F1 (mean): 0.1697923925771643\n",
            "207 stories passed, ROUGE F1 (mean): 0.16981229908093348\n",
            "208 stories passed, ROUGE F1 (mean): 0.16938835845632733\n",
            "209 stories passed, ROUGE F1 (mean): 0.16997828459927286\n",
            "210 stories passed, ROUGE F1 (mean): 0.1699410649510889\n",
            "211 stories passed, ROUGE F1 (mean): 0.16976756856967246\n",
            "212 stories passed, ROUGE F1 (mean): 0.1714978411759221\n",
            "213 stories passed, ROUGE F1 (mean): 0.17138821818679784\n",
            "214 stories passed, ROUGE F1 (mean): 0.17132906835303546\n",
            "215 stories passed, ROUGE F1 (mean): 0.17161385471523058\n",
            "216 stories passed, ROUGE F1 (mean): 0.17155420795146328\n",
            "217 stories passed, ROUGE F1 (mean): 0.1724564785776205\n",
            "218 stories passed, ROUGE F1 (mean): 0.17266883415756207\n",
            "219 stories passed, ROUGE F1 (mean): 0.17284169935278484\n",
            "220 stories passed, ROUGE F1 (mean): 0.17296514615385292\n",
            "221 stories passed, ROUGE F1 (mean): 0.17327471248503382\n",
            "222 stories passed, ROUGE F1 (mean): 0.17324494649038152\n",
            "223 stories passed, ROUGE F1 (mean): 0.1740778137918327\n",
            "224 stories passed, ROUGE F1 (mean): 0.17374710924400366\n",
            "225 stories passed, ROUGE F1 (mean): 0.17350823318670586\n",
            "226 stories passed, ROUGE F1 (mean): 0.17366232652902286\n",
            "227 stories passed, ROUGE F1 (mean): 0.17317699495203945\n",
            "228 stories passed, ROUGE F1 (mean): 0.1735139379365715\n",
            "229 stories passed, ROUGE F1 (mean): 0.17360829605426664\n",
            "230 stories passed, ROUGE F1 (mean): 0.17329562916088903\n",
            "231 stories passed, ROUGE F1 (mean): 0.17342590706716188\n",
            "232 stories passed, ROUGE F1 (mean): 0.17331695119009266\n",
            "233 stories passed, ROUGE F1 (mean): 0.17379934316526\n",
            "234 stories passed, ROUGE F1 (mean): 0.17374037159221187\n",
            "235 stories passed, ROUGE F1 (mean): 0.17365571532925006\n",
            "236 stories passed, ROUGE F1 (mean): 0.1733050940209492\n",
            "237 stories passed, ROUGE F1 (mean): 0.17341773073559502\n",
            "238 stories passed, ROUGE F1 (mean): 0.1729291808269306\n",
            "239 stories passed, ROUGE F1 (mean): 0.17369330789991053\n",
            "240 stories passed, ROUGE F1 (mean): 0.17318888400936494\n",
            "241 stories passed, ROUGE F1 (mean): 0.17286543673701024\n",
            "242 stories passed, ROUGE F1 (mean): 0.17266764565559586\n",
            "243 stories passed, ROUGE F1 (mean): 0.17382057791090313\n",
            "244 stories passed, ROUGE F1 (mean): 0.17467778340629606\n",
            "245 stories passed, ROUGE F1 (mean): 0.1747349322099399\n",
            "246 stories passed, ROUGE F1 (mean): 0.17540847121767295\n",
            "247 stories passed, ROUGE F1 (mean): 0.17517461932714012\n",
            "248 stories passed, ROUGE F1 (mean): 0.1744682700556597\n",
            "249 stories passed, ROUGE F1 (mean): 0.17399070893540047\n",
            "250 stories passed, ROUGE F1 (mean): 0.1737653343166831\n",
            "251 stories passed, ROUGE F1 (mean): 0.17332599855643066\n",
            "252 stories passed, ROUGE F1 (mean): 0.1734521977693432\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bHGjhvMaKw5"
      },
      "source": [
        "# III. Appendix\n",
        "\n",
        "## A. Exploring Learning Rates (Haven't gotten this to run here yet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sR7_T_A7_wcb"
      },
      "source": [
        "### SMALL TRAIN ###\n",
        "\n",
        "# collect train data from all_stories\n",
        "source_text_train = []\n",
        "target_text_train = []\n",
        "\n",
        "for i in range(250):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[train_files[i],'story']\n",
        "    highlights = all_stories.loc[train_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/single highlight pairs\n",
        "    source_text_train.append(story)\n",
        "    target_text_train.append('. '.join(highlights)+'.')\n",
        "\n",
        "# format as a train dataset\n",
        "train_df = pd.DataFrame(list(zip(source_text_train, target_text_train)),columns =['source', 'target'])\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "print(train_dataset)\n",
        "\n",
        "### SMALL VAL ###\n",
        "\n",
        "# collect val data from all_stories\n",
        "source_text_val = []\n",
        "target_text_val = []\n",
        "\n",
        "for i in range(25):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[valid_files[i],'story']\n",
        "    highlights = all_stories.loc[valid_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/single highlight pairs\n",
        "    source_text_val.append(story)\n",
        "    target_text_val.append('. '.join(highlights)+'.')\n",
        "\n",
        "# format as a val dataset\n",
        "val_df = pd.DataFrame(list(zip(source_text_val, target_text_val)),columns =['source', 'target'])\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "print(val_dataset)\n",
        "\n",
        "### SMALL TEST\n",
        "\n",
        "# collect val data from all_stories\n",
        "source_text_test = []\n",
        "target_text_test = []\n",
        "\n",
        "for i in range(25):\n",
        "    \n",
        "    # get formatted input and target\n",
        "    story = all_stories.loc[test_files[i],'story']\n",
        "    highlights = all_stories.loc[test_files[i],'highlights']\n",
        "    \n",
        "    # format data as story/single highlight pairs\n",
        "    source_text_test.append(story)\n",
        "    target_text_test.append('. '.join(highlights)+'.')\n",
        "\n",
        "# format as a val dataset\n",
        "test_df = pd.DataFrame(list(zip(source_text_test, target_text_test)),columns =['source', 'target'])\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "print(test_dataset)\n",
        "\n",
        "# tokenize\n",
        "train_tokenized = train_dataset.map(tokenize, batched=True, batch_size=512)\n",
        "val_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(val_dataset))\n",
        "test_tokenized = val_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n",
        "\n",
        "learn_rates = [0.001, 0.0005, 0.001, 0.005, 0.01, 0.05]\n",
        "\n",
        "output_dir = 'learning_rate_test'\n",
        "\n",
        "for rate in learn_rates:\n",
        "\n",
        "    # training arguments to feed to Trainer object\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir = output_dir, \n",
        "        num_train_epochs = 2, \n",
        "        per_device_train_batch_size = 8, \n",
        "        per_device_eval_batch_size = 8, \n",
        "        eval_accumulation_steps = 1,\n",
        "        prediction_loss_only = True,\n",
        "        learning_rate = rate, ### learning rate is the ONLY CHANGE\n",
        "        evaluation_strategy = 'steps',\n",
        "        save_steps = 10,\n",
        "        save_total_limit = 1,\n",
        "        remove_unused_columns = True,\n",
        "        run_name = 'run_name',\n",
        "        logging_steps = 500, \n",
        "        eval_steps = 500, \n",
        "        logging_first_step = False,\n",
        "        load_best_model_at_end = True,\n",
        "        metric_for_best_model = \"loss\", \n",
        "        greater_is_better = False\n",
        "    )\n",
        "\n",
        "    # create Trainer to feed the train/dev data\n",
        "    trainer = Trainer(\n",
        "        model = model,\n",
        "        args = training_args,\n",
        "        train_dataset = train_tokenized,\n",
        "        eval_dataset = val_tokenized\n",
        "    )\n",
        "\n",
        "    # train the model and save it to our directory\n",
        "    trainer.train()\n",
        "    trainer.save_model(output_dir + '/model')\n",
        "\n",
        "    faster_model = T5ForConditionalGeneration.from_pretrained('learning_rate_test/model')\n",
        "\n",
        "    eval_args = TrainingArguments(\n",
        "        per_device_eval_batch_size=8,\n",
        "        # remove_unused_columns=True,\n",
        "        eval_accumulation_steps=1,\n",
        "        output_dir = 'output'\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(model=faster_model, args=eval_args)\n",
        "\n",
        "    print('='*75)\n",
        "    print('learning rate =',rate)\n",
        "    print('='*60)\n",
        "    print(trainer.evaluate(test_tokenized))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}